\section{Background}

Grappa is built on many existing ideas in programming languages, systems and
architecture. In this section we discuss related frameworks and key enabling
technologies that Grappa builds upon.

\paragraph{Comparable frameworks} Distributed graph processing
frameworks like Pregel \cite{pregel:2010} and Distributed GraphLab
\cite{distgraphlab:vldb12} share similar goals as Grappa. Pregel adopts
a bulk-synchronous parallel (BSP) execution model, which makes it
inefficient on workloads that could prioritize vertices. GraphLab, on
the other hand, schedules vertex computations individually, allowing
prioritization, which gives faster convergence in a variety of iterative
algorithms.  GraphLab, however, imposes a rigid computation model where
programmers must express computation as transformations on a vertex and
its edge list only, with only information from adjacent vertexes. Pregel
is only slightly less restrictive, as the input data can be any vertex
in the graph.  Grappa also supports dynamic parallelism with
asynchronous execution, but parallelism is expressed as tasks or loop
iterations. By comparison, this is a far more general programming model
for irregular computation tasks.

\paragraph{Global memory} Grappa includes a custom implementation of a
software distributed shared memory (DSM) system. Many traditional
software DSM systems are page based~\cite{Treadmarks,munin} and aim to
hide the fact that they are built in software from applications by
exploiting the processor's paging mechanisms, therefore relying heavily
on locality. Instead, Grappa, like other partitioned global address
space (PGAS) models, implements its DSM at the language, rather than
system level. Languages such as Chapel \cite{Chamberlain:2007}, X10
\cite{X10:2005}, and UPC \cite{upc:2005} make accesses to shared
structures look like normal memory references. As we describe later,
Grappa chooses a middle ground, where global addresses are explicit in
the API and local accesses are emitted conventionally by the compiler. 
While Grappa's DSM system is conceptually similar to prior work, its
implementation is tuned for irregular computations.  Past DSM work,
being page-based, could exploit the RDMA capabilities of network
hardware to move large page-sized blocks of data from node to node.  In
our experience, when these networks move small blocks of data (a few
bytes), only a fraction of the available bandwidth is achieved.  In
addition, the DSM system in Grappa ends up being tightly coupled to the
task scheduler in order to overlap long latency memory operations with
useful computation.  For these two reasons it became necessary to build
a new DSM system specifically for Grappa. 


%% We could be stronger here on why Grappa's task library is different,
%% but we are running out of time.  -Mark

\paragraph{Multithreading} Grappa uses multithreading to tolerate memory
latency. This is a well known technique. Hardware implementations
include the Tera MTA \cite{tera:mta1}, Cray XMT \cite{feo:xmt},
Simultaneous multithreading \cite{tullsen:smt}, MIT Alewife
\cite{agarwal:alewife}, Cyclops \cite{almasi:cyclops}, and even GPUs
\cite{gpus}. As we describe in this paper, Grappa use a lightweight
user-mode task scheduler to multiplex \emph{thousands} of tasks on a
single processing core. The large number of tasks is required because of
the extremely high internode latency Grappa is mitigating.  Grappa's
task library employs several optimizations: an extremely fast task
switch, a small task size, and judicious use of hardware prefetching to
bring task state into the cache long before that task is actually
scheduled.

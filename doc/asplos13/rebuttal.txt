
We thank our reviewers for their thoughtful comments.

Some reviewers (1, 4, 5) did not see a novel contribution in our
work. We believe that our system is the first runtime designed
specifically to tolerate latency by switching between thousands of
concurrent tasks per core. We are able to keep cores busy while
performing remote memory, synchronization, and work-stealing
operations. While the individual ideas used to build Grappa are
generally well-known, we know of no other runtime that combines these
ideas to trade latency for throughput.

Reviewer 3 takes issue with our characterization of our performance as
"on par" with that of the XMT. We are able to provide a simple,
general programming model similar to that of the XMT on
already-installed commodity clusters with at most a 2.5x slowdown,
even though the XMT has a 10x faster network injection rate and 20x
faster context switches than we do. We believe this slowdown is within
the realm of optimization---improvements to our network layer should
allow us to double the number of cores per node we use, and
improvements to our scheduler should allow us to increase the rate at
which each core injects messages into the network by ~30 percent.

Reviewer 2 asks about algorithmic modifications such as Beamer's BFS
transformation. Both the XMT and Grappa can benefit from such
transformations. Beamer's transformation increases BFS performance in
two ways: first, by reducing the number of edges traversed, and
second, by reducing the number of synchronizing operations
executed. Both the XMT and Grappa would benefit from the reduction in
edges traversed, but since Grappa's remote references are relatively
more costly than the XMT's, we would expect Grappa to benefit
more. Atomic operations on both platforms are no more expensive than
normal memory operations, so we would expect no significant benefit
from the reduction in synchronizing operations. Many transformations
also benefit by exploiting locality: Grappa provides mechanisms to do
so easily, while the hashed addressing of the XMT makes exploiting
locality difficult.

Reviewer 1 suggests comparing Grappa against other benchmark
implementations on commodity clusters in order to evaluate the benefit
of the programming model. In this work, we focused on providing a
similar programming model to the XMT, which is known to be simple
relative to distributed memory machines. We will explore programmer
productivity and compare against other commodity-cluster-based
implementations in future work.

Our evaluation was written in haste; in a final version we would
discuss key metrics including scheduling overhead and aggregator
performance limitations <Reviwer 2>.

We thank Reviewer 4 for expanding our understanding of the related
work. We will incorporate the suggestions.

[scalability?]

[citation for current memory hierarchy != irregular apps?]



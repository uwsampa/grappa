\section{Introduction} \label{sec:intro}

Irregular applications are those whose parallel execution generates tasks with work, interdependences, or memory accesses that are highly sensitive to input.  Classic examples of irregular applications include branch and bound optimization, SPICE circuit simulation, contact algorithms in car crash analysis, and network flow.  Contemporary examples include real-time mining or querying large unstructured data sets in the business, national security, machine learning, data-driven science, and social network computing domains.  For these emerging applications, fast response -- given the sheer amount of data -- requires multinode systems.  The most broadly available multinode systems are those built from x86 commodity computing nodes interconnected via ethernet or infiniband.  Our goal is to facilitate scaling of irregular applications efficiently on these mass market systems.

What makes our goal challenging is that irregular applications do not readily exploit the features of commodity systems.  This is true even for single node systems.  It is not atypical for any given task's data references to be spread randomly across the entire memory of the system. Yet, for commodity processors' caches to be effective, computations must exploit spatial locality.  Furthermore, the addresses of references are often determined in the task only just before their values are used.  Unfortunately, commodity prefetching hardware is effective only when addresses are known many cycles before the resident data is consumed.  Because commodity processors' pipeline utilization relies on spatial and temporal locality -- or, at least the ability to predict in advance what is needed -- they stall often when executing irregular applications.

On multinode systems, the challenges presented by low locality are analogous, and exacerbated by the increased latency of going off-node.  Irregular applications also present a challenge to mass market network technology, which is designed to transfer large blocks of data, not the word-sized references emitted by irregular application tasks.  Injection rate into the networks is insufficient to utilize wire bandwidth when blocks are below about two-kilobytes, so any straightforward communication strategy severely under-utilizes the network.  While some irregular applications can be restructured to better exploit locality, aggregate requests to increase message size, and manage the additional challenges of load balance and synchronization across multinode systems, the work to do so is formidable and requires knowledge and skills pertaining to distributed systems far beyond those of most application programmers.

An idealized alternative to demanding this expertise of programmers was provided by the fully custom Tera MTA-2~\cite{mta-2} system.  The MTA was well-suited to extremely irregular applications: it had a large distributed shared memory with no caches, relying on multithreaded processors to tolerate memory latency.  On every clock cycle, each processor would execute a ready instruction chosen from one of its 128 hardware thread contexts.  Any application with sufficient thread parallelism would fully utilize the processors independent of the memory access pattern (modulo hot-spots).  The network was designed with an single word injection rate that matched the processor clock frequency and sufficient bandwidth to sustain a reference from every processor on every clock cycle.  Unfortunately, while an excellent match to extremely irregular applications, the MTA was not cost-effective on applications that could exploit locality and had very poor single threaded performance, making it a commercial failure.  The Cray XMT approximates the Tera MTA-2, substantially reducing its cost but not overcoming its narrow range of applicability.

In the twenty years that have elapsed since the Tera MTA, commodity microprocessors have become much faster and multicore has driven down the absolute price of computation; commodity network price-performance has improved as well.  This shift has afforded us the opportunity to attack the challenges posed by irregular applications by emulating with inexpensive mass market hardware components the approach taken by Tera. We exploit the increased aggregate instruction rate per socket relative to chip bandwidth.  That is, where locality friendly computations make use of instructions to get work done in cache, we use what would otherwise be wasted instructions to manage the multiplexing of as many as several thousand tasks per core, thus tolerating memory latency, reducing stalls, and making better use of available bandwidth.  Ultimately, the opportunity is to cover the spectrum of irregular to regular computation:  where tasks exhibit locality, multiplex fewer tasks and expend fewer instructions on context switching; where locality is lacking, multiplex more tasks at a higher rate to tolerate latency.  Thus we make best use of task parallelism -- either to scale to more cores or to tolerate latency -- and of caches -- either to exploit application locality or to house more task contexts.

In this paper we introduce Grappa, a software runtime system that allows a
commodity x86 distributed-memory HPC cluster to be programmed as if it were a
single large shared-memory machine and provides scalable performance for
irregular applications. Grappa is designed to smooth over some of the
performance discontinuities in commodity hardware, giving good performance
when there is little locality to be exploited while allowing the programmer to
exploit it when it is available. Grappa leverages as much freely available and
commodity infrastructure as possible. We use unmodified Linux for the
operating system. An off the shelf user-mode infiniband device driver
stack~\cite{Melonox?} is used for network interfacing. MPI~\cite{mpi} is used
for process setup and tear down. GASnet~\cite{gasnet} is used as the
underlying mechanism for remote memory reads and writes using active message
invocations. To this commodity hardware and software mix Grappa adds three
main software components: (1) a lightweight tasking layer that supports a
context switch in as few as \checkme{40ns} and distributed load balancing; (2)
a distributed shared memory layer that supports customary operations such as
read and write as well as remote operations such as
fetch-and-add~\cite{fetchandadd}; and (3) a message aggregation layer that
combines short messages to mitigate the aforementioned problem that commodity
networks are designed to achieve peak bandwidth only on large packet sizes,
yet irregular applications tend to fetch only a handful of bytes at a time.



Each of these components will be described in more detail in Section~\ref{sec:grappa}.

{\em Provide evidence we have succeeded -- will rewrite as necessary based on final results:}
Grappa achieves its design goal, which is to make irregular applications execute quickly and scalably on commodity clusters.  Our yardstick for comparison is the XMT hardware itself.  Using the same number of processors (in our case cores on a single multicore processor), Grappa is in the same ballpark as the XMT: on some graphs, such as the \checkme{WTF} Grappa is \checkme{2X} faster, while on others, such as the \checkme{WTF2} Grappa is \checkme{2X} slower.  In Section~\ref{sec:results} we explore the factors that underpin this performance.  But most importantly, application performance \emph{scales} with Grappa.  For significantly less real world cost, users can \emph{add} more processors to a commodity cluster and easily outperform the XMT hardware on graph processing, while still saving significant dollars.

\TODO{Add roadmap here. Luis: do we really want a roadmap here? We have enough about what we have done I think, so we might not need to sell things we plan to do.}





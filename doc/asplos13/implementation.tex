\section{Implementation} \label{sec:implementation}

We now discuss implementation details for the major system components discussed in Section~\ref{sec:grappa}.

\subsection{Tasking}

\subsubsection{Tasks and workers}

Grappa tasks are 32-byte entities: a 64-bit function pointer plus
three 64-bit arguments. We use three arguments because tasks are
usually generated as part of a parallel loop decomposition, and thus
each task needs three kinds of data:
\begin{description}
\item[Function pointer] This 64-bit value indicates what routine
  should run. We depend on the fact that all processes in the system
  load the same binary so that function pointers are consistent across
  processes.
\item[Private argument] This 64-bit value is used for task-specific
  data, and is often a loop index.
\item[Shared argument] This 64-bit value is used for data shared
  between all tasks that are part of a loop, and is usually a global
  pointer to a struct. Each task uses its private argument to 
\item[Synchronization argument] This 64-bit value is used to determine
  when all tasks that are part of a loop have finished. It is usually
  a global pointer to a synchronization object allocated at the core
  that spawned the task.
\end{description}
While these are the most common uses of the three task arguments, they
are treated as arbitray 64-bit values in the runtime, and can be used
to store whatever the user wants.

Tasks are not allocated any execution resources until the scheduler
decides to run them; when this occurs, tasks are matched with {\em
  workers}. Each worker is simply a collection of status bits and a
stack, allocated at a particular core. Workers 


\subsubsection{Context switching}

Context switching in Grappa is non-preemptive. Any Grappa routine that
does a potentially-long-latency operation calls a context-switch
routine that gives control of the core back to the scheduler. Users
may also explicity yield control to the scheduler.

As with other cooperative multithreading systems, we treat context
switches as function calls, saving and restoring only the callee-saved
state as specified in the x86-64 ABI \TODO{cite}. This involves saving
six general-purpose 64-bit registers and the stack pointer, as well as
the 16-bit x87 floating point control word and the SSE context/status
register. Thus, the minimum amount of state a cooperative context
switch routine must save according to the ABI is 62 bytes.

Since the compiler sees all calls to the context switch routine, we
can save even less state. Our context switch routine appears to the
compiler as inline assembly; we can declare all the registers we need
to save as ``clobbered'' by the inline assembly routine, and the
compiler will issue its own save and restore code. This allows the
compiler to avoid saving any registers that are not used, or are used
for temporary values that are not needed after the context switch. In
particular, only one of our applications does floating point
operations; all the other applications can safely ignore the x87 and
SSE2 registers as part of context switching.

\subsubsection{Scheduling}

Each core in the Grappa system has its own independent scheduler. Each
scheduler has three main tasks to perform.  First, it must ensure that
its communication resources are serviced reasonably often. Second, it
must ensure that if a running task was waiting on a long-latency
operation and that operation has completed, the task will be
rescheduled. Third, if there are tasks waiting to be run and spare
execution resources, the tasks must be matched with idle workers.

Each scheduler has three queues:
\begin{description}
\item{Ready worker queue} This is a FIFO queue of tasks that are
  matched with workers and are ready to execute.
\item{Private task queue} This is a FIFO queue of tasks that must run on this core.
\item{Public task queue} This is a LIFO queue of tasks that are
  waiting to be matched with workers. It is a local partition of a shared
  task pool; this is described in the next section.
\end{description}

Whenever a task yields or suspends, the scheduler makes a decision
about what to do next. First, it determines if
the communication resources should be serviced. This is done on a
periodic basis. Second, it determines if any workers with running
tasks are ready to execute; if so, one is scheduled. Finally, if there
are no workers ready to run, but there are tasks waiting to be matched
with workers, an idle worker is woken (or a new worker is spawned),
matched with a task, and scheduled.

The scheduler uses its core's timestamp counter to track time. The
timestamp counter is read once per scheduling decision. Timestamp
values are not shared between cores. We use timestamp values only to
ensure period execution of communication maintenance tasks; being
wrong will only change the rate of these events with a potential
performance cost, and no correctness violation. Nevertheless, we
assume Grappa will run on machines with modern timestamp counter
implementations that are monotonic and frequency-compensated.

\subsubsection{Work stealing} \label{subsec:implementation-worksteal}
When a \checkme{grappa-process} runs
out of work, it becomes a ``thief'' and asks another other
\checkme{grappa-process} (``victim'') for work. If the victim has tasks
to spare in its public queue, they will be transferred to the thief's public queue.

Most study of work stealing assumes that a processor has one worker
thread and so it only steals when utilization would go to zero. 
%that is,when there are no more local tasks available. 
Grappa, on the other hand, relies on having many worker threads per core. The
core would be fully-utilized if there is always something on the ready
queue. Even if there are many active tasks, if they are all suspended
for long-latency network requests, then the core is underutilized.
So, there is a choice: should the core use some of this idle time to
do perform work stealing or should it just wait that local tasks will
create more work. We choose to have a core inititiate a steal when all
of the following conditions hold: no workers are ready to run, 
%\checkme{there are idle worker threads --not required to mention since
%technically would happen if have right number}
the unstarted task queues are empty, and there are no outstanding steal requests. 
Having one outstanding steal request allowed throttles steals to
prevent flooding the network, but other scalable quieting mechanisms
would be possible, such as voting tree\cite{scalableWorkStealingOrCilk98}
or lifelines \cite{lifelines}. Termination detection is not
built into the Grappa task scheduler, rather, it is considered a programming error for
the program to exit without syncing all tasks.

\TODO{where to put the following related}
Fork join computations: implement with futures or continuation passing
style
feed-forward computations: implement with scalable task counting (we
focus on this style in our benchmarks)
(breifly describe global task joiner)

choices we've made
-random victims
-steal from FIFO side of queue
-steal when at the moment readyQ empty and task queues empty
-steal min(chunk\_size, half)

Recent work has explored scalable work stealing in terms of efficient
termination detection. 



\subsection{Communication}

\TODO{Jacob}

\subsection{Aggregation}

\subsection{Synchronization}

\subsection{Consistency}

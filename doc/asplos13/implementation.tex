\section{Implementation} \label{sec:implementation}

We now discuss implementation details for the major system components discussed in Section~\ref{sec:grappa}.

\subsection{Tasking}

\subsubsection{Tasks and workers}

Grappa tasks are 32-byte entities: a 64-bit function pointer plus
three 64-bit arguments. We use three arguments because tasks are
usually generated as part of a parallel loop decomposition, and thus
each task needs three kinds of data:
\begin{description}
\item[Function pointer] This 64-bit value indicates what routine
  should run. We depend on the fact that all processes in the system
  load the same binary so that function pointers are consistent across
  processes.
\item[Private argument] This 64-bit value is used for task-specific
  data, and is often a loop index.
\item[Shared argument] This 64-bit value is used for data shared
  between all tasks that are part of a loop, and is usually a global
  pointer to a struct. Each task uses its private argument to 
\item[Synchronization argument] This 64-bit value is used to determine
  when all tasks that are part of a loop have finished. It is usually
  a global pointer to a synchronization object allocated at the core
  that spawned the task.
\end{description}
While these are the most common uses of the three task arguments, they
are treated as arbitray 64-bit values in the runtime, and can be used
to store whatever the user wants.

Tasks are not allocated any execution resources until the scheduler
decides to run them; when this occurs, tasks are matched with {\em
  workers}. Each worker is simply a collection of status bits and a
stack, allocated at a particular core. Workers 


\subsubsection{Context switching}

Context switching in Grappa is non-preemptive. Any Grappa routine that
does a potentially-long-latency operation calls a context-switch
routine that gives control of the core back to the scheduler. Users
may also explicity yield control to the scheduler.

As with other cooperative multithreading systems, we treat context
switches as function calls, saving and restoring only the callee-saved
state as specified in the x86-64 ABI \TODO{cite}. This involves saving
six general-purpose 64-bit registers and the stack pointer, as well as
the 16-bit x87 floating point control word and the SSE context/status
register. Thus, the minimum amount of state a cooperative context
switch routine must save according to the ABI is 62 bytes.

Since the compiler sees all calls to the context switch routine, we
can save even less state. Our context switch routine appears to the
compiler as inline assembly; we can declare all the registers we need
to save as ``clobbered'' by the inline assembly routine, and the
compiler will issue its own save and restore code. This allows the
compiler to avoid saving any registers that are not used, or are used
for temporary values that are not needed after the context switch. In
particular, only one of our applications does floating point
operations; all the other applications can safely ignore the x87 and
SSE2 registers as part of context switching.

\subsubsection{Scheduling}

Each core in the Grappa system has its own independent scheduler. Each
scheduler has three main tasks to perform.  First, it must ensure that
its communication resources are serviced reasonably often. Second, it
must ensure that if a running task was waiting on a long-latency
operation and that operation has completed, the task will be
rescheduled. Third, if there are tasks waiting to be run and spare
execution resources, the tasks must be matched with idle workers.

Each scheduler has three queues:
\begin{description}
\item{\bf Ready worker queue} This is a FIFO queue of tasks that are
  matched with workers and are ready to execute.
\item{\bf Private task queue} This is a FIFO queue of tasks that must run on this core.
\item{\bf Public task queue} This is a LIFO queue of tasks that are
  waiting to be matched with workers. It is a local partition of a shared
  task pool; this is described in the next section.
\end{description}

Whenever a task yields or suspends, the scheduler makes a decision
about what to do next. First, it determines if
the communication resources should be serviced. This is done on a
periodic basis. Second, it determines if any workers with running
tasks are ready to execute; if so, one is scheduled. Finally, if there
are no workers ready to run, but there are tasks waiting to be matched
with workers, an idle worker is woken (or a new worker is spawned),
matched with a task, and scheduled.

The scheduler uses its core's timestamp counter to track time. The
timestamp counter is read once per scheduling decision. Timestamp
values are not shared between cores. We use timestamp values only to
ensure period execution of communication maintenance tasks; being
wrong will only change the rate of these events with a potential
performance cost, and no correctness violation. Nevertheless, we
assume Grappa will run on machines with modern timestamp counter
implementations that are monotonic and frequency-compensated.

\subsubsection{Work stealing} \label{subsec:implementation-worksteal}
When a \checkme{grappa-process} runs
out of work, it becomes a ``thief'' and asks another other
\checkme{grappa-process} (``victim'') for work. If the victim has tasks
to spare in its public queue, they will be transferred to the thief's public queue.

Most study of work stealing assumes that a processor has one worker
thread and so it only steals when utilization would go to zero. 
%that is,when there are no more local tasks available. 
Grappa, on the other hand, relies on having many worker threads per core. The
core would be fully-utilized if there is always something on the ready
queue. Even if there are many active tasks, if they are all suspended
for long-latency network requests, then the core is underutilized.
So, there is a choice: should the core use some of this idle time to
do perform work stealing or should it just wait that local tasks will
create more work. We choose to have a core inititiate a steal when all
of the following conditions hold: no workers are ready to run, 
%\checkme{there are idle worker threads --not required to mention since
%technically would happen if have right number}
the unstarted task queues are empty, and there are no outstanding steal requests. 
Having one outstanding steal request allowed throttles steals to
prevent flooding the network, but other scalable quieting mechanisms
would be possible, such as voting tree\cite{scalableWorkStealingOrCilk98}
or lifelines \cite{lifelines}. Termination detection is not
built into the Grappa task scheduler, rather, it is considered a programming error for
the program to exit without syncing all tasks.

\TODO{where to put the following related}
Fork join computations: implement with futures or continuation passing
style
feed-forward computations: implement with scalable task counting (we
focus on this style in our benchmarks)
(breifly describe global task joiner)

choices we've made
-random victims
-steal from FIFO side of queue
-steal when at the moment readyQ empty and task queues empty
-steal min(chunk\_size, half)

Recent work has explored scalable work stealing in terms of efficient
termination detection. 



\subsection{Communication}

In order to mitigate the low message injection rate limits of commodity
networks, Grappa's communication has two layers. 

At the upper layer, Grappa implements asynchronous active messages
\cite{vonEicken92}. Each message consists of a function pointer, an
optional argument payload, and an optional data payload. When a task
sends a message, the message is copied to a send queue associated with
the message's destination and the task continues execution.

Grappa's lower networking layer aggregates the upper layer's messages
to improve performance. In our applications we see that the bulk of
the messages sent are small. \TODO{include stats} Commodity networks,
even ones designed for high-performance computing, provide low
bandwidth utilization at small message sizes. \TODO{include stats?} To
make the best use of the network, we must convert our small messages
into large ones.

Grappa leverages its lightweight context switching to do this
conversion. When a task sends a message, the message is not
immediately sent. Instead, it is stored in a queue associated with its
destination, to be sent later. If the task requires a response, it
suspends itself and the scheduler finds other work to run while the
task is waiting. When the reply arrives, the suspended task is woken.

There are three situations in which a queue of aggregated messages is
sent. First, each queue has a message size threshold, chosen to give
reasonable network performance. If the size in bytes of a queue is
above the threshold, the contents of the queue are sent
immediately. Second, each queue has a wait time threshhold. If the
oldest message in a queue has been waiting longer than this threshold,
the contents of the queue are sent immediately, even if the queue size
is lower than the message size threshold.  Third, queues may be
explicitly flushed in situations where the programmer wants to
minimize the latency of a message at the cost of bandwidth
utilization.

The network layer is serviced by polling. Periodically when a context
switch occurs, the Grappa scheduler switches to the network polling
thread. This thread has three responsibilities. First, it polls the
lower-level network layer to ensure it makes progress. Second, it
deaggregates received messages and executes active message
handlers. Third, it checks to see if any aggregation queues have
messages that have been waiting longer than the threshold; if so, it
sends them.

Underneath the aggregation layer, Grappa uses the GASNet communication
library \cite{GASNet} to actually move data. All interprocess
communication, whether on or off a cluster node, is handled by the
GASNet library. GASNet is able to take advantage of many communication
mechanism, including Ethernet and Infiniband between nodes, as well as
shared memory within a node.

Some networks provide access to a remote machine's memory
directly. This would seem to be a good fit for a programming model
focused on global shared memory, but in fact we do not use it. RDMA
operations are subject to the same message rate limitations as all
other messages on these cards, and thus using raw RDMA operations for
our small messages would make inefficient use of bandwidth. Instead,
we implement remote memory operations with active messages as
described in the next section. This means that Grappa is not limited
to RDMA-capable networks.

\subsection{Memory}




remote access

heap

allocation

synchronization

consistency
>>>>>>> Stashed changes


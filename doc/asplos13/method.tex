\section{Metholody} \label{sec:method}

To explore the performance of the Grappa runtime we have implemented three algorithms: breadth first search, betweenness centrality, and unbalanced tree search.  These algorithms were implemented for the Cray XMT (our baseline) and for Grappa.  Performance results for Grappa were obtained on a 144 node cluster.  Individual nodes of this cluster contain the hardware depicted in Table~\ref{table:grappanode}.  Nodes are interconnected with both 10G Ethernet and Infiniband.  For all but startup and configuration, they are configured to communicate over the Infiniband network for these experiments.

The metric we use is algorithmic time, which means startup and loading of the graph (from disk) is not included in the measurement.  Data is collected on real systems, which means minor variations in runtime exist from run to run.  The average of multiple runs are used, and where appropriate, confidence in the result is reported (error bars, standard deviation, etc).

One question that must be answered when making a comparison between the Cray XMT and a typical HPC cluster is what is a fair comparison -- these systems are quite different.  Three options immediately come to mind: equal number of processing cores, equal number of network interfaces, and equal dollars.  We discounted the last option fairly quickly, because it isn't a lasting data-point and, as will be evident later, would be hugely unfair to the XMT.  The first option, cores, has some merit, but Grappa is designed for applications that have no locality in their computation.  This means almost all of their memory accesses are remote.  Hence, the factor that limits their performance is not processing, but communicating.  Hence, we have chosen the middle option, network interfaces as the way to normalize across the XMT and our cluster.  Each processor in the XMT system has its own network interface to access shared memory.  In the HPC cluster we use, each processor (which contains 12 cores), has a single Infiniband interface.  Hence, for our results we scale up XMT processors one for one with full system nodes.

\subsection{Applications}
%Traversal of an unbalanced, unpartitioned tree captures the foundations for implementing most forms of irregular parallelism. 
%Recursion is found in every irregular divide and conquer algorithm. Loops, even regular ones, must be dynamically scheduled to get good load balance when run on a large, non-uniform system. Nested parallelism involves fine-grain creation of dynamic amounts of work. Dataflow..

\paragraph{Unbalanced tree search (UTS)}, is a benchmark for evaluating the programmability and performance of systems for parallel applications that require dynamic load balancing \cite{Olivier:uts2006}. It involves traversing an unbalanced implicit tree: at each vertex, its number of children is sampled from some probability distribution, and this number of new nodes are added to a work queue to be visited. While this benchmark captures irregular, dynamic parallelism, we want to evaluate performance on algorithms that touch existing data. We augment UTS by using the traversal to create a large tree in memory, and then we traverse the in-memory tree. We call this benchmark UTS-mem, and the timed portion is this traversal of the in-memory tree. The assumption is that in general the size of each subtree is unknown until that subtree has actually been traversed.

%todo: say tree explore is same as a search except we are additionally needing to synchronize on all vertices visited

\comment{The Grappa version of tree search UTS-mem

\lstset{language=C++,
       basicstyle=\footnotesize,
       tabsize=2}
\begin{lstlisting}
// base of shared vertex_t[]
GlobalAddress<vertex_t> Vertices;        

// base of shared int64_t[]
GlobalAddress<int64_t> ChildrenPointers;  

void search_vertex( id ) {
  GlobalAddress<vertex_t> v_addr = Vertices + id;
  Incoherent<vertex_t>::RO v( v_add, 1 );

  // start index of my children pointers
  childIndex = v->childIndex;    

  // how many children I have
  numChildren = v->numChildren;  

  // parallel loop over the child list for this vertex
  parallel_for(fn=&search_children, start=childIndex, iters=numChildren);
}

void search_children( start, iters ) {
  // take advantage of spatial locality in the array of children
  GlobalAddress<int64_t> child_base_addr = ChildrenPointers + start;
  Incoherent<int64_t>::RO childIds( child_base_addr, iters );

  // spawn a task to visit each child
  for (i = 0..iters) {
    SoftXMT_publicTask( fn=&search_vertex, id=childIds[i] );
  }
}
\end{lstlisting}
}


\section{Flat Combining}

Flat combining allows threads to cooperate when accessing a synchronized shared data structure to get greater overall throughput than even each of them operating serially. Careful engineering must be employed to implement a mechanism for cooperation that does not introduce the same synchronization issues as the original contended data structure. However, the same delegation mechanism can be reused in any data structure protected by a single global lock. 

\subsection{Physically-shared memory}

The benefits of flat-combining can be broken down into three components: improved locality, reduced synchronization, and data structure-specific optimization. Having a single thread bound to a particular core do a string of accesses trivially results in a lower cache miss rate than having multiple threads on multiple cores bring the data structure into cache each in turn.

The story for reducing synchronization is more complicated. The problem with having many threads attempt to acquire a lock is that as contention increases, there will be more failed synchronization attempts (failed \emph{compare and swap}, or \emph{CAS} operations), which must be retried until they succeed.
Instead of having most of the threads repeated fail on acquiring the lock, when threads observe that the lock has already been taken, they can delegate their work to the core with the lock by adding their requests to the structure's publication list. However, in a naive implementation, this operation still needs to perform synchronization every time a request is added to ensure inserts do not collide.
There is no way to avoid all synchronization, however, the cost may be amortized over many operations on the same data structure. Each thread must pay the synchronization cost to insert a publication record into the list once, but after that, it may reuse the record for subsequent accesses without synchronizing.

The final piece of the flat combining story is how operations are semantically combined using knowledge of how data structure accesses compose. All of the threads that did not acquire the lock block on their request until it is satisfied. Whichever thread did obtain the lock becomes the combiner. It iterates over the publication list, merging each non-empty publication record into a combined operation using a combining function defined by the data structure. When the pass is complete, the combined operation is applied to the actual data structure. The combiner thread then takes the aggregated results of the combined operation and distributes them to the correct requests in the publication list. When each request is fulfilled, the thread blocking on it wakes and continues, and finally the combiner thread releases the lock. If any publication requests remain, which could have come from threads that published their request after the combining thread had passed, one of them acquires the lock and becomes the new combiner.

\TODO{Add example of doing flat-combining on a global stack, discuss how pushes/pops are flattened.}

\subsection{In Grappa}

The cost of performing synchronization remotely on a distributed data structure motivates applying the flat-combining paradigm to Grappa data structures. Instead of all of the workers on a core sending messages to do remote synchronization independently, flat-combining can allow all of the workers to share a single remote synchronization operation. In a way, the Grappa runtime is already reducing synchronization traffic when it aggregates small messages that are all going to the same destination. However, it doesn't know what the operations are doing, so it must still serialize, pack, transmit, perform the operation on the remote side, and send a response for each message. Flat combining is about ``teaching'' it how to more efficiently perform many operations remotely in certain special cases.

In a PGAS setting, and in Grappa in particular, a number of different choices are made when implementing flat combining. Because memory is physically distributed, locality must be made explicit. In addition, in Grappa there are orders of magnitude more concurrent threads (``workers'') accessing shared data structures. Typically around a thousand are needed per core to tolerate the latency of remote operations, so in a cluster of machines, there are easily millions of workers. Therefore, a different scheme for managing many threads is necessary.

Flat-combined global data structures have a local publication list on each core which serve as proxies to the global structure and are used to combine requests. Workers add their requests to the local publication list by issuing operations on their local proxy and then block waiting for their requests to be satisfied. Similar to the shared-memory case, one requesting worker is chosen to combine requests and perform the combined operation globally. This worker walks the local publication list, combining requests into a local request object as it goes. If it is possible to service a request locally, as it is on some data structures such as matching pushes and pops to a stack, the combiner can satisfy the requests and wake the correct workers even before synchronizing globally. The combiner then issues the combined data structure operation globally. In most cases, this means communicating with whichever core holds the ``lock-protected'' field that maintains sequential consistency. In the case of a global stack, this is the ``master'' \emph{top} pointer which must be incremented on pushes and decremented on pops. On the core where global synchronization is occurring, there must be synchronization between the multiple synchronization requests arriving from other cores.

At this point we have traded off one remote synchronization per operation for an additional local synchronization per operation and some number fewer remote synchronizations (depending on the amount of combining that occurs). This would likely result in a net benefit due to local synchronization being inherently cheaper. However, because of the way the Grappa runtime schedules computation, it is actually possible to get the correct mutual exclusion/atomicity without any additional synchronization overhead.

By design, each core in Grappa operates independently of the other cores. Workers are scheduled cooperatively, so each worker can assume atomicity until it performs an operation that yields to the scheduler, such as a remote call. Therefore workers within a core can cooperate without any explicit synchronization operations requiring memory fences. This allows them to publish requests to the local  proxy very inexpensively, eliminating the need for a complex synchronization-amortizing queue. This has the added benefit that there will be no unused publication records, which could have become a problem with thousands of workers. Further, remote operations are processed through the same cooperative-multithreading scheduler on the remote side, so combined operations are trivially serialized and atomic.

\paragraph{Correctness}

It may not be immediately obvious why this two-level version of flat combining preserves sequential consistency. The combining operations that are performed locally among requests on a single core must preserve the illusion of sequential order; this is unchanged from shared-memory flat-combining. When combined operations are applied globally, they are serialized at the ``master'' core for a given synchronization context. The global order is essentially the concatenation of each core's serialized updates. In the case where operations are satisfied locally (by matching up with other local operations), by definition these must be independent of other operations, so they can conceptually be placed anywhere in the global ordering (or even ignored completely in the global ordering).

In the spirit of the Data-Race-Free-0 Model (guaranteeing sequential consistency for data-race free programs), operations on the global data structure are considered to be made atomic by a conceptual global ``lock'' on the data structure. Atomicity and freedom from data races in the case of multiple data structures and other synchronization must be guaranteed externally, just as in any multithreaded shared-memory program.

\section{FC Framework}
\TODO{Describe how to implement FC data structure}.


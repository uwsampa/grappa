\section{Related Work}
Attempts to improve the scalability of synchronized data structures are as old as the data structures themselves. Though much attention has gone to lock-free data structures such as the Treiber Stack~\cite{treiber} and Michael-Scott Queue~\cite{msqueue}, a significant body of work has explored ways of scaling globally locked data structures.

Many techniques performed combining hierarchically to reduce synchronization and hot-spotting, differing mainly in the structures used to do combining: fixed trees~\cite{yew:combining-trees}, dynamic trees (or ``funnels'')~\cite{funnels,MAMA}, and randomized trees~\cite{edtrees}. In particular, MAMA~\cite{MAMA}, built for the MTA-2, leveraged funnels to better utilize hardware optimized for massive concurrency.
On the other hand, \emph{flat} combining~\cite{flatCombining} observed that in multicore systems, hierarchical schemes have too much overhead, and a cleverly implemented \emph{flat} publication list can perform better.
Follow-on work introduced parallel flat-combining~\cite{scalableFCQueues}, in which multiple publication lists are combined in parallel and their remainders serialized.
Flat combining was also applied in the NUMA (non-uniform memory access) domain with scalable hierarchical locks~\cite{fcNUMALocks}, which improves on prior work on hierarchical locking by leveraging flat-combining's publication mechanism which amortizes the cost of synchronization.

Our work extends the flat-combining paradigm further, to the PGAS domain, where only software provides the illusion of a physically distributed memory. The relatively higher cost of communication in PGAS makes flat combining even more compelling. Within-core combining is naturally done in a \emph{flat} way. In addition, the physical isolation per node combined with Grappa's engineered per-core isolation guarantees, create a novel situation for combining, which led to our \emph{combining proxy} design.

In the distributed memory and PGAS domains, implementing scalable and distributed synchronization is the name of the game.
Besides Grappa, other work has proposed leveraging latency tolerance in PGAS runtimes, including MADNESS~\cite{shet:async-upc} which proposes ways of using asynchronous operations in UPC, and Zhang et al.~\cite{zhang:barnes-hut} who use asynchronous messages in a UPC Barnes-Hut implementation to overlap computation with communication and allow for buffering.
Jose et al. introduced UPC Queues~\cite{jose:upc-queues}, a library which provides explicit queues as a replacement for locks to synchronize more efficiently and allow for buffering communication. This work demonstrates a similar use case for synchronized queues as in our work, and uses them with the Graph500 benchmark as well. In contrast, Grappa performs much of the same message aggregation invisibly to the programmer, and the FC framework can be used to implement many different linearizable data structures.


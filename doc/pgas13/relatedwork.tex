\section{Related Work}
Attempts to improve the scalability of synchronized data structures are as old as the data structures themselves. Though much attention has gone to lock-free data structures such as the Treiber Stack~\cite{treiber} and Michael-Scott Queue~\cite{msqueue}, a good body of work has explored ways of scaling globally-locked data structures.

Flat combining~\cite{flatCombining} is one of long trail of techniques for allowing a single thread to perform the updates of others. Previous techniques performed combining hierarchically using fixed trees~\cite{yew:combining-trees}, dynamic trees (or ``funnels'')~\cite{funnels}, and randomized trees~\cite{edtrees}. However, the observation of the flat combining work was that in practice, a cleverly-implemented \emph{flat} publication list can outperform more complicated schemes.
Follow-on work introduced parallel flat-combining~\cite{scalableFCQueues}, in which multiple publication lists are combined in parallel and their remainders serialized.
Flat combining was also applied in the NUMA (non-uniform memory access) domain with scalable hierarchical locks~\cite{fcNUMALocks}, which improves on prior work on hierarchical locking by leveraging flat-combining's publication mechanism which amortizes the cost of synchronization.

Our work extends the flat-combining paradigm even further to the PGAS domain, where only software provides the illusion of a physically-distributed memory. The relatively higher cost of communication in PGAS makes flat combining even more compelling. In addition, the physical isolation per node combined with Grappa's engineered per-core isolation guarantees introduce a novel opportunity to apply combining, which led to our ``combining proxy'' design.

In the distributed memory and PGAS domains, implementing scalable and distributed synchronization is the name of the game.
Besides Grappa, other work has proposed leveraging latency tolerance in PGAS runtimes, including MADNESS~\cite{shet:async-upc} which proposes ways of using asynchronous operations in UPC, and Zhang et al.~\cite{zhang:barnes-hut} who use asynchronous messages in a UPC Barnes Hut implementation to overlap computation with communication and allow for buffering.
Jose et al. introduced UPC Queues~\cite{jose:upc-queues}, a library which provides explicit queues as a replacement for locks to synchronize more efficiently and allow for buffering communication. This work demonstrates a similar use case for synchronized queues as in our work, and in fact uses them in the Graph500 benchmark as well. In contrast, Grappa performs much of the same message aggregation invisibly to the programmer and even data structure designer, and the flat combining framework described in this work can be used to implement many different data structures for use in synchronizing.


\section{Evaluation}
To evaluate the impact flat combining has on the global data structures, we ran a series of experiments to test the raw performance of the data structures themselves under different workloads, and further measured their impact on performance of two simple graph benchmarks.

Experiments were run on a cluster of AMD Interlagos processors. Nodes have 32 2.1-GHz cores in two sockets, 64GB of memory, and 40Gb Mellanox ConnectX-2 InfiniBand network cards, connected via a QLogic InfiniBand switch.

\subsection{Data Structure Throughput}
First we measured the raw performance of the global data structures on extremely simple throughput workloads. In each experiment, a Grappa parallel loop spawns an equal number of tasks on all cores. Each task randomly chooses an operation based on the predetermined ``operation mix'', selecting either a push or pop for the Stack and Queue, or an insert or lookup for the Set and Map.

\paragraph{Queue and Stack}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{data/plots/vector_perf.pdf}
  \caption{\emph{Queue and Stack performance.}
    Results are shown on a log scale for a throughput workload performing 256 million operations with 2048 workers per core and 16 cores per node. Flat combining improves throughput by at least an order of magnitude and allows performance to scale. Matching pushes and pops enables the stack to perform even better on a mixed workload.
  }
  \label{fig:vector}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{data/plots/stack_stats.pdf}
  \caption{\emph{GlobalStack Statistics.}
    Measured number of combined messages sent by the Stack with a fixed combining buffer of 1024 elements. Being above the buffer size indicates the amount of matched pushes and pops that occurred.
  }
  \label{fig:stack_stats}
\end{figure}

Figure~\ref{fig:vector} shows the results of the throughput experiments for the global Stack and Queue. Results are shown with flat combining completely disabled, only combining at the master core, only combining locally, and full two-level combining.

Despite Grappa's automatic aggregation, without combining, both the stack and queue completely fail to scale because all workers' updates must serialize on the master core.
Though combining on the master core alleviates some of the serialization, it does not provide significant benefit because all of the synchronization must still be performed on one core. However, with local flat combining, synchronization is able to be done mostly in parallel, with much less-frequent bulk synchronization at the master. Doing the additional level of combining at the master seems to not affect performance significantly compared to the impact of local combining.\TODO{Add data showing how many are combined in one pass on the master.}

On the mixed workload, the stack is able to do matching locally, allowing it to reduce the amount of communication drastically, greatly improving its performance. Figure~\ref{fig:stack_stats} corroborates this, showing that the amount of combining that occurs directly correlates with the observed throughput.

The queue also benefits from reduced synchronization and batching operations, and its all-push workload performs identically to the stack's.
However, the queue is unable to do matching locally, and in fact, the mixed workload performs worse because the current implementation serializes combined enqueue and dequeue operations. This restriction could be lifted with more careful synchronization at the master core allowing enqueues and dequeues to proceed in parallel as long as enough elements are in the queue to allow them to proceed without conflicting.

\paragraph{HashSet and HashMap}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{data/plots/hash_perf.pdf}
  \caption{\emph{GlobalHashSet and GlobalHashMap.}
    Results are shown for a throughput workload inserting and looking up 256 million random keys in the range 0-1024 into a global hash with 1024 cells, with 2048 workers per core and 16 cores per node.
    Performance without combining scales out to 32 nodes because synchronization happens at each hash cell, but drops off as the number of destinations increases. Due to eliminating duplicate inserts and lookups, the combining version is able to continue to scale.}
    \TODO{if time: add 'delete' operation, too (show that it doesn't affect correctness or performance)}
  \label{fig:hash_perf}
\end{figure}

Figure~\ref{fig:hash_perf} shows the throughput results for the GlobalQueue and GlobalStack.
Both data structures have the same synchronization strategy (serialization happens only at each hashed location) which allows them to scale fairly well even without combining because serialization only happens on conflicts. However, after 32 nodes, scaling drops off significantly due to increased number of concurrent accessors and more destinations.
The combining version is able to perform repeated inserts and repeated lookups with a single remote operation, enabling it to continue scaling further.

\subsection{Application Kernel Performance}
The original goal was to have scalable global data structures in the Grappa library that could be used in applications. Naive implementations of the data structures are insufficient for scaling, so HPC applications often implement their own highly customized versions. However, this forces each application to carefully reason about the ways in which the structures will be accessed. Additionally, many optimizations rely on relaxing global consistency in a way that does not affect the program. 
The Grappa data structures are synchronized to provide the most general use and match the expectations of programmers and algorithms.

\paragraph{Breadth-First Search}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{data/plots/bfs_perf.pdf}
  \caption{\emph{BFS} on a Graph500-spec graph of scale 26 (64 million vertices, 1 billion edges), with the direction-optimizing BFS algorithm. Performance is measured in millions of Traversed Edges Per Second (MTEPS).}
  \label{fig:bfs_perf}
\end{figure}
The first application kernel is the Graph500 Breadth-First-Search (BFS) benchmark. This benchmark does a search starting from a random vertex in a synthetic graph and builds a search tree of parent vertices for each vertex traversed during the search. While this is a relatively simple problem, it exercises the random-access throughput of a system as well as being a primitive in many other graph algorithms. The BFS algorithm contains a global queue which represents the frontier of vertices to be visited in each level.
Our implementation employs the direction-optimizing algorithm by Beamer et al.\cite{Beamer:Graph500} which performs particularly well for the scale-free RMAT graphs generated by the benchmark.
The frontier queue in BFS is amenable to further optimization to take advantage of the fact that the algorithm does pushes and pops in separate phases, allowing consistency to be relaxed.
We compare our implementation of BFS using the flat-combined global queue described above with a highly tuned Grappa implementation that uses a custom asynchronous queue.

Figure~\ref{fig:bfs_perf} shows the results of scaling the BFS kernel up to 64 nodes. The simple queue implementation without flat combining is completely unscalable. However, the flat combining queue tracks the highly tuned asynchronous version. This illustrates that providing a safe, synchronized data structure for initially developing algorithms for PGAS is possible, while further optimizations can be applied incrementally.

\paragraph{Connected Components}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{data/plots/cc_perf.pdf}
  \caption{\emph{Connected Components} on the same scale 26 Graph500 graph. Performance is measured in MTEPS.}
  \label{fig:cc_perf}
\end{figure}
Connected Components (CC) is another simple graph analysis kernel that illustrates another use of global data structures in irregular applications. We implement the three-phase CC algorithm~\cite{mtgl} which was designed for the massively-parallel MTA-2 machine. In first phase, the algorithm begins many traversals in parallel from random starting vertices, labeling vertices with the root vertex. Whenever two traversals encounter each other, their searches are pruned and an edge between the two roots is inserted in a set. After all edges have been traversed in this way, the set of edges forms a new, typically much smaller, graph. The second phase performs the classical Shiloach-Vishkin parallel algorithm\cite{shiloach1982n} on this reduced graph, and the final phase propagates the component labels out to the full graph.
The creation of the reduced-graph edge set dominates the runtime of this algorithm, so improving the implementation of the set operations has a significant impact on performance. As in the case of BFS, further optimizations involving relaxation of consistency can be applied, in this case, to the global set. Therefore, we compare our straightforward implementation using the generic GlobalHashSet with and without flat combining against a tuned asynchronous implementation.

The results in Figure~\ref{fig::cc_perf} show that none of these three implementations scale very well out to 64 nodes. However, performing combining does improves the performance of the algorithm overall and improves scaling. The tuned version outperforms the synchronous version because it is able to build up most of the set locally on each core before merging them at the end of the first phase. An implementation of the GlobalHashSet that did not provide synchronized semantics could potentially relax consistency in a more general way, but this is left for future work.

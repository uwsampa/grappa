\section{Evaluation}
To evaluate the impact flat combining has, we ran a series of experiments to test the raw performance of the data structures themselves under different workloads, and then measure the impact on performance of two graph benchmarks.
Experiments were run on a cluster of AMD Interlagos processors. Nodes have 32 2.1-GHz cores in two sockets, 64GB of memory, and 40Gb Mellanox ConnectX-2 InfiniBand network cards connected via a QLogic switch.

\subsection{Data Structure Throughput}
First we measured the raw performance of the global data structures on synthetic throughput workloads. In each experiment, a Grappa parallel loop spawns an equal number of tasks on all cores. Each task randomly chooses an operation based on the predetermined ``operation mix,'' selecting either a push or pop for the Stack and Queue, or an insert or lookup for the Set and Map.

\paragraph{Queue and Stack.}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{data/plots/vector_perf.pdf}
  \caption{\emph{Queue and Stack performance.}
    Results are shown on a log scale for a throughput workload performing 256 million operations with 2048 workers per core and 16 cores per node. Local flat combining improves throughput by at least an order of magnitude and allows performance to scale. Matching pushes and pops enables the stack to perform even better on a mixed workload.
  }
  \label{fig:vector}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{data/plots/stack_stats.pdf}
  \caption{\emph{GlobalStack Statistics.}
    Measured number of combined messages sent by the Stack with a fixed combining buffer of 1024 elements.
    Matched pushes and pops result in ops/message being greater than the buffer size.
  }
  \label{fig:stack_stats}
\end{figure}

Figure~\ref{fig:vector} shows the results of the throughput experiments for the global Stack and Queue. Results are shown with flat combining completely disabled, only combining at the master core (``centralized''), and combining locally (``distributed'').

Despite Grappa's automatic aggregation, without combining, both the stack and queue completely fail to scale because all workers' updates must serialize.
Though centralized combining alleviates some of the serialization, its benefit is limited 
because all operations involve synchronization through a single core.
However, with local flat combining, synchronization is done mostly in parallel, with less-frequent bulk synchronization at the master.

On the mixed workload, the stack is able to do matching locally, allowing it to reduce the amount of communication drastically, greatly improving its performance. Figure~\ref{fig:stack_stats} corroborates this, showing that the amount of combining that occurs directly correlates with the observed throughput.

% % left out for space
% We also experimented with performing both forms of combining together. Combining at the master did not affect performance significantly, which is somewhat surprising as all combined requests must still serialize.
% However, with both levels of combining enabled, relatively few requests are combined at the master---for the stack, a maximum of 6\% per pass for the all-push workload and 0.1\% for the mixed workload.

The queue benefits in the same way from reducing synchronization and batching, and its all-push workload performs identically to the stack's.
However, the queue is unable to do matching locally, and in fact, the mixed workload performs worse because the current implementation serializes combined pushes and combined pops. This restriction could be lifted with more careful synchronization at the master core.

\paragraph{HashSet and HashMap.}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{data/plots/hash_perf.pdf}
  \caption{\emph{GlobalHashSet and GlobalHashMap.}
    Results are shown for a throughput workload inserting and looking up 256 million random keys in a particular range into a global hash with the same number of cells, with 2048 workers per core and 16 cores per node.
  }
  \label{fig:hash_perf}
\end{figure}

% Without combining, performance scales out to 32 nodes because synchronization is distributed over hash cells, but drops off as the number of destinations increases.
Figure~\ref{fig:hash_perf} shows the throughput results for the Set and Map.
Both data structures synchronize at each hash cell, which allows them to scale fairly well even without combining. However, after 32 nodes, scaling drops off significantly due to the increased number of destinations.
Combining allows duplicate inserts and lookups to be eliminated, so performs better the smaller the key range. This reduction in message traffic allows scaling out to 64 nodes.

\subsection{Application Kernel Performance}
% The original goal was to have scalable global data structures in the Grappa library that could be used in applications. Generic implementations of data structures are often insufficient for scaling, so HPC applications often implement their own customized versions. However, this forces each application to carefully reason about the ways in which the structures will be accessed, and often relies on relaxing consistency.
The Grappa data structures are synchronized to provide the most general use and match the expectations of programmers and algorithms.
In these evaluations, we compare the flat-combining structures against custom, tuned versions that leverage relaxed consistency needs of the applications.

\paragraph{Breadth-First Search.}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.38\textwidth]{data/plots/bfs_perf.pdf}
  \caption{\emph{BFS} on a Graph500-spec graph of scale 26 (64 million vertices, 1 billion edges), with the direction-optimizing BFS algorithm. Performance is measured in millions of Traversed Edges Per Second (MTEPS).}
  \label{fig:bfs_perf}
\end{figure}
The first application kernel is the Graph500 Breadth-First-Search (BFS) benchmark~\cite{graph500list}. This benchmark does a search starting from a random vertex in a synthetic graph and builds a search tree of parent vertices for each vertex traversed during the search.
% While this is a relatively simple problem, it exercises the random-access throughput of a system as well as being a primitive in many other graph algorithms.
The BFS algorithm contains a global queue which represents the frontier of vertices to be visited in each level.
Our implementation employs the direction-optimizing algorithm by Beamer et al.~\cite{Beamer:Graph500}.
 % which performs particularly well for the scale-free RMAT graphs generated by the benchmark.
The frontier queue is write-only in one phase and read-only in the next, making it amenable to relaxed consistency.
We compare performance of BFS using the Grappa FC queue described above with a highly tuned Grappa implementation that uses a custom asynchronous queue.

Figure~\ref{fig:bfs_perf} shows the scaling results. Using the simple queue without flat combining is completely unscalable, but with FC, it tracks the tuned version. This illustrates that providing a safe, synchronized data structure for initially developing algorithms for PGAS is possible without sacrificing scaling.
% , while further optimizations can be applied incrementally.

\paragraph{Connected Components.}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.38\textwidth]{data/plots/cc_perf.pdf}
  \caption{\emph{Connected Components} on the same scale 26 Graph500 graph. Performance is measured in MTEPS.}
  \label{fig:cc_perf}
\end{figure}
Connected Components (CC) is another core graph analysis kernel that illustrates a different use of global data structures in irregular applications. We implement the three-phase CC algorithm~\cite{mtgl} which was designed for the massively parallel MTA-2 machine. In the first phase, parallel traversals attempt to claim vertices and label them.
Whenever two traversals encounter each other, an edge between the two roots is inserted in a set. 
% After all edges have been traversed in this way, the set of edges forms a new, typically much smaller, graph.
The second phase performs the classical Shiloach-Vishkin parallel algorithm~\cite{shiloach1982n} on the reduced graph formed by the edge set from the first phase, and the final phase propagates the component labels out to the full graph.
Creation of the reduced edge set dominates the runtime of this algorithm, but includes many repeated inserts at the boundary between traversals, so is a prime target for the flat-combining Set.
Similar to BFS, the Set is write-only first, then read-only later, so further optimizations involving relaxation of consistency can be applied. Therefore, we compare our straightforward implementation using the generic Set with and without flat combining against a tuned asynchronous implementation.

The results in Figure~\ref{fig:cc_perf} show that none of these three scale well out to 64 nodes. However, performing combining does improve the performance over the uncombined case. The tuned version outperforms the synchronous version because it is able to build up most of the set locally on each core before merging them at the end of the first phase. An implementation that did not provide synchronized semantics could potentially relax consistency in a more general way.

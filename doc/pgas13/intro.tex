\section{Introduction}
The goal of partitioned global address space (PGAS)~\cite{upc:2005} languages and runtimes is to provide the illusion of a single shared memory to a program actually executing on a distributed memory machine such as a cluster. This allows programmers to write their algorithms without needing to explicitly manage communication.
However, it does not alleviate the need for reasoning about consistency among concurrent threads.
Luckily, the PGAS community can leverage a large body of work solving these issues in shared memory and explore how differing costs lead to different design choices.

% Globally shared data structures are one of the cornerstones of shared-memory and PGAS abstractions. 
% In order for multiple concurrent threads to interact with one another correctly, they must observe shared data consistently.
It is commonly accepted that the easiest consistency model to reason about is \emph{sequential consistency} (SC), which enforces that all accesses are visible in program order and appear to happen in some global serializable order.
To preserve SC, operations on shared data structures should be \emph{linearizable}~\cite{herlihy1990linearizability}; that is, appear to happen atomically in some global total order.
In both physically shared memory and PGAS, maintaining linearizability presents performance challenges.
The simplest way is to have a single global lock to enforce atomicity and linearizability through simple mutual exclusion. Literally serializing accesses in this way is typically considered prohibitively expensive, even in physically shared memory.
% Much prior work has explored the scalability challenges of adding more concurrent accessors to shared data structures in shared memory systems.
However, even in fine-grained lock-free synchronization schemes, as the number of concurrent accessors increases, there is more contention, resulting in more failed synchronization operations.
With the massive amount of parallelism in a cluster of multiprocessors and with the increased cost of remote synchronization, the problem is magnified.

A new synchronization technique called \emph{flat combining}~\cite{flatCombining} coerces threads to \emph{cooperate} rather than \emph{contend}.
Threads delegate their work to a single thread, giving it the opportunity to combine multiple requests in data-structure specific ways and perform them free from contention.
This allows even a data structure with a single global lock to scale better than complicated concurrent data structures using fine-grained locking or lock-free mechanisms.

The goal of this work is to apply the flat-combining paradigm to a PGAS runtime to reduce the cost of maintaining sequentially consistent data structures.
We leverage Grappa, a PGAS runtime library optimized for fine-grained random access, which provides the ability to tolerate long latencies by efficiently switching between many lightweight threads as sketched out in prior work~\cite{Nelson:hotpar11-real}.
We leverage Grappa's latency tolerance mechanisms to allow many fine-grained synchronized operations to be combined to achieve higher, scalable throughput while maintaining sequential consistency.
In addition, we show how a generic flat-combining framework can be used to implement multiple global data structures.

The next section will describe in more detail the Grappa runtime system that is used to implement flat combining for distributed memory machines. We then explain the flat-combining paradigm in more depth and describe how it maps to a PGAS model. Next, we explain how several data structures are implemented in our framework and show how they perform on simple throughput workloads as well as in two graph analysis kernels.

\section{Introduction}
% - PGAS languages successfully bring shared memory problems into distributed memory systems
% - Cost functions are different, but many of the other considerations, such as reducing locking, still apply
% - With abundant parallelism, we can tolerate some serialization to reduce number of synchronizations and reduce overall communication

The goal of partitioned global address space (PGAS) languages and runtimes is to provide the illusion of a single shared memory to a program that actually executes on a number of machines each with their own memory. This allows programmers to write their algorithms without needing to explicitly manage communication. Once the algorithm is correct, many techniques exist to help improve performance improving locality (spatial and temporal) and coarsening communication where possible. However, with the shared memory abstraction come all of the difficult concurrency issues that arise in physically-shared memory. Luckily, PGAS can leverage the large body of work solving these issues in a shared memory domain, there are just a host of different costs and performance trade-offs to be made in a distributed setting.

When multiple threads of control concurrently access the same object, they have a choice about how to conduct themselves: contend or cooperate. Either they all fight to obtain the single global lock, or they attempt a series of potentially-contended atomic operations in the case of a more advanced synchronization strategy such as a lock-free queue. In both cases, as the number of concurrent accessors increases, the more contention and therefore failed synchronization operations there are. If, however, the threads cooperate, they can delegate one thread to do all the operations and avoid excessive synchronization overhead on highly contended locations. In addition, the particular insight of flat-combining is that some operations have semantics such that they can be \emph{combined} and applied more efficiently together than individually. With a sufficiently efficient mechanism for delegating and combining, the benefits of reduced synchronization can outweigh the cost of serialization.

To honor the shared-memory abstraction, a common goal is for distributed shared data structures to adhere to a sequentially consistent memory model. Again, the simplest way to maintain a consistent global view of the data structure is to have a single global lock that is used to implementation mutual exclusion and enforce a serializable order over insertions. However, in a system with physically distributed memory, the cost of synchronizing in this way is even greater. In addition, with the massive amount of parallelism in a cluster of multiprocessors, the amount of contention on this shared data structure is higher than any shared-memory multiprocessor.

In many cases, algorithms do not require operations to be immediately consistent. Many optimizations for message-passing or PGAS-style distributed memory algorithms leverage this observation to minimize the need for fine-grained communication. However, they typically must make this visible to programmers, who now must take care to insert their own memory barriers where appropriate, or otherwise express the way in which consistency relaxation can occur.

The goal of this work is to see whether the concept of flat-combining can be used to make the cost of enforcing fine-grained synchronization acceptable.
To evaluate this possibility, we leverage the Grappa runtime~\cite{Nelson:hotpar11-real}, a PGAS-style runtime library optimized for fine-grained random access, which provides the ability to tolerate long latencies by efficiently switching between many lightweight threads.
With enough concurrent threads, it is actually possible to allow each thread to maintain sequentiality by blocking on accesses, but still have an opportunity to effectively combine many fine-grained synchronous operations for better performance.

% lay out the upcoming sections of the paper...

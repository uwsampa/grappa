% Structured concurrency (scheduler, cooperative multithreading + communication) enables hierarchical flat-combining to be implemented efficiently for a software PGAS shared-memory solution.
The implementation of scalable synchronized data structures is notoriously difficult. Recent work in shared-memory multicores introduced a new synchronization paradigm called \emph{flat combining} that allows many concurrent accessors to cooperate efficiently to reduce contention on shared locks.
In this work we introduce this paradigm to a domain where reducing communication is paramount: distributed memory systems.
We implement a flat combining framework for Grappa, a latency-tolerant PGAS runtime, and show how it can be used to implement synchronized global data structures.
Even using simple locking schemes, we find that these flat-combining data structures scale out to 64 nodes with 2x-100x improvement in throughput. We also demonstrate that this translates to application performance via two simple graph analysis kernels.
The higher communication cost and structured concurrency of Grappa lead to a new form of distributed flat combining that drastically reduces the amount of communication necessary for maintaining global sequential consistency.

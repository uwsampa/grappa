\section{Conclusion}
In PGAS implementations there is typically a large discrepancy between the first, simple, description of an algorithm and the final, optimized one. At the core of this work is the desire to have a library of data structures which can be used without worrying about when they will be consistent or whether they will limit scalability.
Bringing findings from shared memory into the PGAS domain brings with it new challenges and opportunities.
We have shown that the additional concurrency that comes with a latency-tolerant runtime, rather than compounding the problem, provides a new opportunity for reducing communication by combining locally.
% perform some of the locality optimizations that finely-tuned implementations would.
This allows flat-combined sequentially-consistent global data structures to scale well out to a thousand cores with millions of concurrent threads.

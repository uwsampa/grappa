\input{header}
\usepackage{microtype}
\begin{document}

\title{Pomace: a Solid-State Grappa}
\authorinfo{Jacob Nelson$^{\dagger}$,
  Brandon Holt$^{\dagger}$,
  Brandon Myers$^{\dagger}$,
  Preston Briggs$^{\dagger}$,
  Luis Ceze$^{\dagger}$,
  Simon Kahan$^{{\dagger \ddagger}}$,
  Mark Oskin$^{\dagger}$
}{\textdagger University of Washington, \textdaggerdbl Pacific Northwest National Laboratory}{
  \{nelson, bholt, bdmyers, preston, luisceze, skahan, oskin\}@cs.washington.edu}


\maketitle
%\begin{abstract}
%\input{abstract}
%\end{abstract}

We would like to present our software system Grappa, designed to scale
up emerging irregular applications such as graph analytics on
commodity clusters, and ask this community: What would be required to
adapt Grappa to exploit SSD, and how would doing so benefit this
application space?

\section{What is Grappa?}
Grappa is a latency tolerant runtime system for off-the-shelf clusters
that targets irregular applications, such as graph analytics.  These
applications are notoriously difficult to scale-up due to
data-dependent execution and memory access behaviors. The resulting
irregularity in computational demands hinders efforts to balance load
and to maintain spatial and temporal locality.  Resulting
inefficiencies are exacerbated at scale, as wasted energy rises with
the number of idling nodes, undersized network packets, and cache
misses.  Consequently, few irregular applications today make good use
of high-performance computers.  Those that do use complex, application
specific techniques to overlap computation with communication and were
developed by expert programmers who are in short supply.

Yet there is increasing demand for irregular applications.  Evidence
is the growing number of publications, conferences, and companies
focused on graph benchmarks, semantic databases, machine learning, and
“big data” discovery.

Grappa aims to mitigate the inefficiencies plaguing development and
execution of these important applications.  Grappa facilitates
expression of fine-grained task parallelism at the application
programming level.  Programmers attend only to expressing the
parallelism in their program, not to scheduling it.  At execution
time, Grappa’s task scheduler dynamically maps these tasks to x86
processor cores using work stealing heuristics similar to those
described in the literature.  What distinguishes Grappa from other
such runtime systems is that tasks implicitly yield at every long
latency operation, resulting in a context switch to another task.  In
this way, processors remain well-utilized even in the presence of
frequent references to remote memory.  Grappa thus tolerates many
microseconds of latency by multiplexing thousands of tasks per core.
In addition, Grappa aggregates references into packets destined for
common remote node locations.  In this way, Grappa’s design provides
for up to approximately 100 million remote references per second per
node using today’s commodity network hardware.  While similar latency
tolerance and aggregation techniques can be manually coded into some
applications, Grappa relieves the programmer of this burden.  In
addition, Grappa provides an API for reader-writer semantics for the
purpose of manipulating a local copy of any contiguous portion of the
shared address space, enabling programmers to exploit spatial and
temporal locality while maintaining a consistent global view.  With
these features, we have demonstrated performance on commodity systems
for highly irregular applications approaching that of the Cray XMT, a
fully custom system.

\subsection{What is the potential we imagine?}

While Grappa’s current design scales to the limit of available network
bandwidth, it does so only for applications whose working set resides
in RAM.  Unfortunately, provisioning systems with petabytes of RAM is
costly, energy inefficient, and likely to increase the time to prepare
for and recover from faults.

Emerging non-volatile memory technologies offer hope for an
alternative.  We imagine placing the entire data set on SSD or PCM and
either using Grappa directly to tolerate the latency much in the same
way as it is used now to tolerate network latency or to stage blocks
of data into RAM, using Grappa to tolerate the latency incurred by the
transfers.  We imagine the energy per available byte being much lower.
We imagine recovery from faults with most of the data residing in
non-volatile memory being faster.

Prior work only partially explores this potentiality. GraphChi
represents a class of approaches that use windowing methods in the
irregular application space, structuring the data on secondary storage
to support filling RAM with multiple interrelated subblocks sufficient
for each of a series of phases of computation.  Such approaches are
algorithmically efficient only when each successive phase requires
access to the entire data set.  MAGT addresses this limitation
introducing support for asynchronous computation on graphs under the
assumption that RAM is sized to accommodate data structures
proportional to vertex-count, still leaving edges on non-volatile
storage.  In addition, both methods apply only to single node systems:
Grappa provides a global view of memory on multinode systems.

\section{Applying non-volatile memory to Grappa}
The natural point of integration for non-volatile memories in Grappa
is to store the global heap in nonvolatile memory, since the heap
usually contains data that is infrequently modified. All accesses to
this shared heap in Grappa programs are done through an API. If the
access is to local memory, Grappa issues a prefetch for the address
and context-switches to other work while the prefetch is executing. If
the access is to memory stored on another node, Grappa issues a
message to that node requesting the data and context-switches to other
work until the reply arrives. Non-volatile memory would be accessed in
the same way: if a request is to memory stored in non-volatile memory,
Grappa would initiate the request and context-switch to other work
until the request had completed.

\subsection{Challenges in using commodity SSDs}
Grappa’s goal is to provide good irregular application performance on
commodity hardware. Unfortunately, commodity non-volatile memories are
difficult to use in this way for three reasons:

\paragraph{SSDs assume locality.} In our benchmarks, we find the average request size to be
less than 64 bytes. Today’s SSDs are optimized for reads in the
kilobyte range.  

\paragraph{SSDs have a low request rate.} The fastest (cite) can
perform on the order of 1 million random reads per second, and many
are limited to less than 100K/s. In contrast, cores may be idle in
Grappa if each node cannot make 100 million random reads per second of
the global heap. Even the individual flash chips are slow: 50K/s (cite
micron).  

\paragraph{SSDs have high overhead.} Commodity SSDs are usually treated
as normal disks---accesses must occur through the kernel, incurring a
high context switch overhead. Grappa depends on user-mode access to
the network layer. This capability has been explored in NVM research
(cite Moneta).  

\subsection{Exploring the design space}

Previously we discussed the mechanism by which memory references in
Grappa could be issued to non-volatile memory devices. In this
section, we explore a number of different designs for interfacing the
non-volatile memory hardware with Grappa nodes. Each design has
tradeoffs, which we briefly discuss.

For simplicity, we omit discussion of write and erase operations. We
assume a non-volatile Grappa would store frequently-written temporary
data in DRAM and would use its standard latency-tolerance techniques
to cover the cost of infrequent NVM writes.

\subsubsection{Tolerating Flash-based SSD latency} 
A natural first step would be to add one or more SSDs to each Grappa
cluster node, and use Grappa's global memory API to expose concurrent
accesses and context-switch to tolerate the latency of SSD
access. Unfortunately this is unlikely to work well, for two
reasons. First, the SSD must accept requests at a rate sufficient to
keep the cores busy. Grappa's target is for each cluster node to
generate 100 million random 8-byte requests per second, but most SSDs
can accept requests at a rate two to four orders of magnitude slower.
Second, the SSD must be able to support many requests operating in
parallel. Little's Law suggests that if requests take 20 $\mu$s for
the SSD to service, and if the node generates requests at a rate of
100 million per second, the SSDs must support 2000 concurrent
requests. Again, existing SSDs support orders of magnitude less request concurrency.

We see three ways to address this disparity.
\paragraph{Scaling up the Flash array}
Existing Flash-based SSDs are designed for large block requests. An
SSD for Grappa could be designed to support high concurrency, small
requests. Where a traditional SSD might put multiple Flash chips on
the same bus, or use Flash chips in parallel to build a wider,
high-bandwidth bus, a Grappa SSD would try to maximize parallelism by
putting each chip on its own bus. Serial Flash might be used to
minimize pin count requirements, at the cost of some additional
latency. This design could scale to request rates, but the challenge
would be chip count: to support 100M reads/second, an array of
20$\mu$s flash would require 2000 chips. This is possible, but expensive.

\paragraph{Scaling down the processors}
Alternatively, rather than scaling up the flash array, processors
could be scaled down to match the flash request rate. A low-power
processor with few cores might generate only 1 million references per
second. This could be satisfied with only 20 20$\mu$s Flash chips.

There are two problems with this design. First, while each node
requires fewer chips, the overall cluster performance goal, and thus
chip requirement, is the same. Second, 


\paragraph{Buffering requests to increase locality.}
Grappa trades latency for better throughput by aggregating network
communications for better utilization and by migrating synchronization
operations for reduced contention. The same idea could be applied to
increase locality and amortize the cost of Flash accesses. Requests
to common flash blocks would be bucketed by flash block address for a
limited time. When a sufficient number of requests arrived for a block
or when requests timed out, the requests would be issued to the Flash
layer.

While similar techniques have provided benefit in prior work in
restricted situations (cite), they depend on a sufficient request
density to each block to get a benefit. As datasets scale up, the
request density may decrease and the benefit of this technique may
decrease.

\subsubsection{Using future memory technologies}

While Grappa's goal so far has been to leverage commodity hardware, it
is worth exploring the potential benefit of emerging non-volatile
memory technologies like Phase-Change Memory. The advantages of PCM
are twofold: first, read latency is much closer to that of DRAM, and
second, PCM supports finer-grained accesses.

\paragraph{PCM as Main Memory}
Much recent research has explored the use of PCM as a main memory
replacement. Grappa would easily be able to take advantage of such a
development. When a Grappa program references a low-locality program,
a prefetch request is issued for that data a few cycles before the
thread is context-switched in\cite{Nelson:hotpar2011}. A PCM-based
Grappa would do the same, but would issue the prefetches earlier to
cover the longer latency of PCM.

\paragraph{PCM as attached storage}

Grappa could also support a PCM array in a separate address space (for
instance, on the PCIe bus). PCM chips available today (cite) support a
64-bit read in 200 ns; this means that 20 chips acting independently
would be sufficient to reach our target request rate.

Just as in network communications, issuing individual PCIe bus
operations for each memory request has too high overhead to get a
reasonable request rate.  Instead, Grappa would delay threads
performing operations in the PCM array until it could send a buffer
with enough requests to amortize the overhead. The PCM array would
perform the operations and buffer the results. When a sufficient
amount have completed, the PCM array would write the results to DRAM,
and one core would deliver the results to the requesting threads,
waking them.

An ideal array would support limited atomic operations on 64-bit
words, including increment and compare-and-swap.

This system could be built today, with the only downside being
density---currently available PCM chips are significantly less dense
than Flash or even DRAM. PCM's technology scaling curve suggests that
this will be an ideal solution in the future.

\section{Conclusion}
Efficient execution of irregular applications on large datasets is of
growing importance.  Grappa promises to address this need on mass
market clusters.  Addition of non-volatile storage is a natural
extension of such systems.  We ask, what would developing a Pomace for
Grappa -- a Grappa system exploiting non-volatile memory -- entail and
what capability and efficiency improvements might it bring to Grappa’s
application space.  Answering this question is an opportunity for
collaboration between our group and the NVMW community.




%\subsection{References}
\bibliographystyle{abbrv}
\bibliography{paper}

\end{document}




%% move stuff to defs file
\newcommand{\todo}[1]{{\bfseries [[#1]]}}
%% To disable, just uncomment this line
 \renewcommand{\todo}[1]{\relax}

\newcommand{\mrps}[0]{Mref/s}

%\documentclass{acm_proc_article-sp}
\documentclass[10pt,nocopyrightspace,preprint]{sigplanconf}
\usepackage{url}
\usepackage{fancyvrb}
\usepackage{alltt}
\usepackage{graphicx}
\usepackage{comment}
\usepackage[labelfont=bf,font=small,belowskip=-7pt,aboveskip=-10pt]{caption}
\begin{document}

\title{Can We Crunch the Social Graph on the Cheap?}

%\numberofauthors{7}
%\author{
%Jacob Nelson, Brandon Myers, Andrew Hunter, Dan Grossman, Mark Oskin,
%Carl Ebeling, Luis Ceze\\
%University of Washington\\
%\email{\{nelson, bdmyers, ahh, djg, oskin, ebelin, luisceze\}@cs.washington.edu}\\ \\
%Simon Kahan \\ 
%Pacific Northwest National Lab\\
%\email{skahan@cs.washington.edu}\\ \\
%Preston Briggs\\ 
%Cray Inc.\\
%\email{preston.briggs@gmail.com}\\
%}

%todo format the UW names horizontally
\authorinfo{Jacob Nelson, Brandon Myers, A.H. Hunter, Dan Grossman, Mark Oskin,
Carl Ebeling, Luis Ceze}{University of Washington}{\{nelson, bdmyers, ahh, djg, oskin, ebelin, luisceze\}@cs.washington.edu}
\authorinfo{Simon Kahan}{Pacific Northwest National Lab}{skahan@cs.washington.edu}
\authorinfo{Preston Briggs}{}{preston.briggs@gmail.com}


\maketitle
\begin{abstract}
% Techniques from graph analytics have found important applications in
% areas such as social networks and bioinformatics. These irregular
% problems present a challenge for parallel machines: obtaining
% performance is not straightforward. Multithreading works well for
% such problems, but the only large-scale implementation of the technique
% is the Cray XMT, an expensive, non-commodity machine.

% Our goal is to build a system with XMT-like properties, implemented
% with commodity components. This paper presents a runtime for latency
% tolerance, along with our plans for scaling up to multiple
% nodes. Our analysis shows the feasibility of our approach.

  Crunching large graphs is the basis of many emerging applications,
  such as social network analysis and bioinformatics. Graph analytics
  algorithms exhibit little locality and therefore present significant
  performance challenges. Hardware multithreading systems (e.g, Cray
  XMT) showed that with enough concurrency, we can tolerate long
  latencies. Unfortunately, this solution is not available with
  commodity parts.

  Our goal is to develop a latency-tolerant system built out of
  commodity parts and mostly in software. The proposed system includes
  a runtime that supports a large number of lightweight contexts,
  full-empty synchronization and a memory manager that provides a
  high-latency but high-bandwidth global shared memory. This paper
  lays out the vision for our system, and justifies its feasibility
  with a performance analysis of the runtime for latency tolerance.

% Our analysis shows the feasibility of our
%   approach.

% To that end we developed a new runtime system that supports
%   a large number of lightweight contexts, together with full-empty
%   synchronization primitives and a large global address space.

% To that end, we propose a
%   new runtime system that is able to sustain a large number of active
%   threads 

%  XMT-like properties, implemented
%   with commodity components. This paper presents a runtime for latency
%   tolerance, along with our plans for scaling up to multiple
%   nodes. Our analysis shows the feasibility of our approach.


%   Large, sparse, irregular graphs map poorly to the memory systems of
%   commodity hardware. The scale of emerging real world datasets
%   presents a challenge for straightforward algorithms which will run
%   inefficiently due to many high-latency accesses. Pervasive
%   parallelism can tolerate that latency, but the only large-scale
%   implementation of the technique is the Cray XMT, an expensive,
%   non-commodity machine.

%   If we can adapt the XMT's programming model to commodity hardware,
%   we can dramatically improve performance without resorting to
%   expensive custom systems. This paper presents a runtime for latency
%   tolerance through the use of parallel lightweight contexts, along
%   with our plans for scaling up to multiple nodes. Our analysis shows
%   the feasibility of our approach.
\end{abstract}

\section{Introduction}

Many important emerging applications such as social network analysis,
bioinformatics and analysing vast sensor data from actual networks
(e.g, water, electrical grid) are based on crunching very large
graphs. Unfortunately, the computational cost of these applications
gets quickly out of hand. For example, in January 2011, Facebook had
500 million active users with an average of 130 friends each
\cite{Facebook:2011p91. Its friend suggestion algorithm in 2010 ran on
  a 40-node cluster with 72GB memory per node, and still, new
  suggestions could only be computed every two days
  \cite{Backstrom:2010p90}. Speeding up these applications at a low
  cost would have a large impact in how we deal and react to the fast
  growth of the data-set and would make the results even more
  valuable.

% Techniques from graph analytics have found important applications in
% areas such as social networks and bioinformatics. \todo{more!}


% In January 2011, Facebook had 500 million active users with an average
% of 130 friends each \cite{Facebook:2011p91}.
% Facebook's friend suggestion
% algorithm in 2010 ran on a 40-node cluster with 72GB memory per
% node. Even with that capacity, new suggestions could only be computed
% every two days \cite{Backstrom:2010p90}.


In this context, the most interesting computational challenge comes
from large, low-diameter, power law graphs: this combination makes
extracting performance difficult. They do not fit in a single
commodity machine's memory. Their low diameter means that they are
difficult to lay out with locality: every vertex needs to be near
every other vertex. Their power law distribution means that a few
vertices generate much more work than the rest, leading to load
imbalances. Together, these properties lead to a difficult
conclusion: each edge traversal is likely to lead to expensive
inter-node communication. %cache miss or remote access.

Multithreading is a technique that has been used successfully to
implement efficient computations for these graphs. The Cray XMT is an
example of such an approach: it solves the memory latency problem (or
communication problem) through concurrency rather than caching. Each
XMT processor supports 128 hardware contexts and 1024 outstanding
memory operations, and is able to switch contexts every cycle.
% \todo{cite XMT description}.  %% LUIS: there is enough information here.
This ability comes at a cost: the XMT is an expensive, non-commodity
machine. Moreover, arguably a lot of XMT's advantages for these type
of applications come at the cost of performance in general-purpose
applications.

% \todo{don't we want to also say XMT has poor performance on x
%   kind of apps?}

\begin{figure*}[htbp]
  \begin{center}
    \includegraphics[width=0.9\textwidth]{figures/system-overview.pdf}
	\end{center}
	\caption{Overview of our proposed system design. The base
          system is a cluster of commodity nodes and interconnect. We
          add a latency-tolerant runtime and a global memory manager
          to obtain efficient access to the global address space. Only
          the shaded components may require hardware design.}
	\label{fig:system-overview}
\end{figure*}

%We want the best of both worlds. 
We believe we can build a system mostly based on commodity parts that
can attain XMT-like performance with a familiar, also XMT-like,
programming model, but at a fraction of the cost. Therefore, our goal
is to build a system that has good performance on low-locality graph
codes but is implemented with cheap commodity processors and simple
additional support in an FPGA. This approach also has the added
benefit of not sacrificing general-purpose performance.

Figure~\ref{fig:system-overview} shows the structure of our
proposal. It is composed of multiple nodes built with commodity
processors, communicating over an Infiniband network. We add two
components: a runtime system, responsible for executing the user's
threads, and a global memory manager, responsible for facilitating
memory requests to the non-coherent global memory shared across the
nodes.  We are exploring a mix of hardware (FPGA) and software
implementations of the memory manager.

This paper describes the vision of our system and explores the
feasibility of our ideas using a single-node implementation of a
lightweight threading runtime. We use prefetch instructions together
with lightweight threads to tolerate the latency of DRAM access on the
local node node. We use adversarial pointer chasing benchmarks to
verify our runtime's overhead is acceptable. We also model the effects
of pointer chasing on a remote node by artificially increasing
latencys to the order of several micro-seconds.

The rest of the paper is organized as follows. We quickly describe our
programming model in section~\ref{sec:model}. We describe the
implementation of our latency-tolerant runtime and plans for extending
to multiple nodes in section~\ref{sec:approach}. We evaluate our
runtime in section~\ref{sec:evaluation}.  We present related work in
section~\ref{sec:related} and conclude in
section~\ref{sec:conclusion}.

\section{Programming model}
\label{sec:model}

Our goal is to preserve the XMT programming model: large shared global
address space, explicit concurrency with a large number of threads,
and full-empty-bits synchronization.  We partition the overall address
space into a global shared address space and per node private address
spaces. Locality may be exploited directly by the programmer in the
node private address spaces, just as in a conventional cache-coherent
multiprocessor node. In contrast, locality cannot be exploited in the
global space: its value is in providing high bandwidth random access
to any word in a shared data structure by any processor.

% Whereas in the PGAS model coherency of global copies is implictly
% maintained across the system, % \todo{cite pgas?}
% we completely forego
% caching of globally shared data.  

A node can exploit temporal locality of data stored in the global
space only by first copying it into the node's local space.  In this
way, a node can access data at random across the system while
incurring no hardware coherence overhead: consistency is enforced by
our mechanisms for explicit global synchronization.

The model promises efficiency subject to the presence of sufficient
concurrency.  Exactly how that concurrency is expressed by the
programmer is language dependent. The goal of our system is to support
that concurrency and consequently provide latency tolerant on a large
address space. We do so via software multithreading: computation that
is about to execute a long latency operation (e.g. a remote memory
reference) initiates the operation and then yields to the scheduler,
which quickly resumes execution of another computation.  Concurrency
is also used -- via aggregation of requests to a common node
destination -- to make more efficient use of network bandwidth and in
some cases reduce bandwidth demands.

Synchronization on locations in the node private address space is the
same as on conventional systems.  Synchronization on global addresses
works differently: we provide atomic operations such as {\tt
  int\_fetch\_add} as well as operations using full-empty-bits, as on
the Cray XMT.  As is true for other long latency operations,
synchronization latency is tolerated by yielding to the scheduler.
Even non-blocking global atomics yield: atomic operations executed as
x86 instructions sabbotage concurrency by forcing all outstanding
memory references to complete before new ones can be issued.  By
yielding, the core can switch to another computation while the
synchronization is accomplished outside of the pipeline.  In addition,
the implementation can exploit aggregation when concurrency is
abundant to increase the throughput of global synchronization.

%% I dont think we need the paragraph below.

% Thus, the programming model provides the programmer with options: express
% locality within the private space, and it will be exploited to reduce
% latency and bandwidth \todo{exploited by what? by the exisiting locality hardware like cache hierarchy?}; express concurrency, and it will be used toward
% tolerating latency and reducing the cost of bandwidth.

\begin {comment}
\todo{Change to discuss API:

context switches:
  yield()
  discuss coroutines

mem:
  alloc()
  read(p) 
  write(p, d)
  prefetch(p)

sync:
 {read, write} {EE, EF, FE, FF}
}

While the ideal programming model is one that facilitates easy
expression in the problem domain as well as efficient execution on the
available hardware, our programming model today is motivated by only
efficient execution: it exists as a rudimentary library for concept
demonstration, not as a language suitable for development.  The
programming model we use today is therefore only partially formed. It
exists as an augmentation of existing threaded programming models such
as pthreads or openmp that are readily available on most computer
systems.

Though not strictly necessary, for ease of exposition we imagine a
one-to-one binding between threads and cores.  Our model manifests
within any thread independently of the others as a ``fray'' \todo{for lack
of better term }.  Within the thread, a fray instance is the
instantiation of a set of coroutines and a scheduler.  Instantiation
is at user-level and the operating system sees the entire fray as as a
single user-level thread.

\todo{ describe calls that perform thread \& scheduler instantiation; and schedule activation }

Each coroutine executes non-preemptively \todo{seems like non-premptively isn't ``how" it executes, rather how it is scheduled, maybe say ``cooperatively"?}, utilizing the core to which
its ``parent'' thread has been assigned by the operating system until it
reaches a yield point.  Typically, the coroutine yields after
initiating a prefetch operation on data that the programmer or
compiler anticipates would probably not complete before the data is
referenced by a subsequent load operation.  In this way, the core
continues to execute instructions for other coroutines when otherwise
it would likely stall.  The programming model thus tolerates latency.

\todo{ describe calls to prefetch-and-yield }

When a thread frays, for the sake of expediency, a fixed portion of
its stack is divided amongst the coroutines.  Given a finite stack,
each coroutine is constrained to execute a call tree of depth known to
be finite at compilation time: recursion is not yet permitted.

\todo{ discussion of coroutines \& synchronization } Were a coroutine to
block on a synchronization variable shared with other threads, the
entire fray would suspend execution.  This can lead to deadlock when,
for example, one coroutine waits to consume from another thread that
is waiting to consume what only another coroutine in this first fray
can produce.  Instead, coroutines must yield on failed synchronization
events, spinning rather than blocking, where were they bona fide
threads, blocking might be more efficient.

\todo{ say something about ``synchronization'' within a fray }

Coroutines completing their work yield without adding themselves to
the scheduling queue.  The last coroutine to exit in this way returns
as the main thread, as in the common  fork-join model of parallelism.

\end{comment}

\section{Implementation}
\label{sec:approach}
The goal of our implementation is to use multithreading to tolerate
the latency of random accesses to memory in a global address space
spread across multiple nodes. This goal has three key challenges:
\begin{itemize}
\item {\bf Lightweight multithreading}
  
  We expect to need hundreds of
  threads to tolerate network latency, so our threading
  implementation must have low memory and context switching overheads.

\item {\bf Concurrency in global memory references}

  We will need to keep track of the hundreds of outstanding memory fetch operations required to utilize bandwidth well.

\item {\bf Low overhead synchronization}

  We must support global synchronization that can proceed without stalling the processing cores.

\end{itemize}
We discuss our approach to solving each of these challenges in
turn. For each one, we discuss both our current single-node
implementation and ideas for a multi-node implementation.

\subsection{Lightweight multithreading}

Our approach to supporting many lightweight threads uses a cooperative,
user-level multitasking system built using coroutines.

There are three main concerns in designing the coroutine
mechanism. First, the working set of each coroutine must be small, so
many coroutines can be active without trashing the cache. Second,
context switches must be fast, so that overhead does not swamp actual
work. Third, when a coroutine yields, we must choose a suitable
coroutine to run next.

We minimize context size by treating context switches as function
calls, as in~\cite{charm}. This allows us to save and restore the
minimum number of registers allowed by the ABI, and depend on the
compiler to save and restore any other registers in use. We minimize
context switch time both by keeping contexts small and by doing all
switching and scheduling in user space, as in \todo{cite other green
  threads packages, like qthreads}.

There are a number of possibilites for coroutine scheduling.
Ideally, a thread would be rescheduled when its request had
completed, but this may complicate the scheduling decision and slow
down context switches. A simple round-robin scheduler may be sufficient.

\subsubsection{Single-node implementation}
We implemented coroutines as described, with a round-robin
scheduler. If a prefetched data value is no longer in the cache when
the coroutine is rescheduled and the value is used, the coroutine pays
the full cost of the fetch.

\subsubsection{Multi-node ideas}
We will explore how the memory manager and coroutine scheduler may
interact, so that threads waiting on long-latency memory operations or
synchronization operations will be scheduled soon after their
operations have completed. 

\subsection{Concurrency in global memory references}

To enable concurrency in memory references, we break each read or
write into two operations: the {\em issue} operation and the {\em
  complete} operation. The read issue operation starts data moving
towards the processor and returns immediately, like a prefetch; the
read complete operation blocks until data is available. The write
issue operation takes data along with an address, and starts an
asynchronous write; the write complete operation blocks until the
write is committed.

A read operation in user code consists of three steps: a {\em read
  issue}, which causes the data to start moving; a {\em yield}, which
causes another thread to start executing, overlapping the read
latency; and, once the reading thread has been rescheduled, a {\em
  read complete} blocks until the data is available. A write operation
follow the same pattern.

\subsubsection{Single-node implementation}
We use prefetch instructions for the issue operation, and regular
blocking reads and writes for the complete operation. Since we expect
these global references to have low locality, we use the
``non-temporal'' form of prefetch, which tries to bring the data near
the core without disturbing the state of the cache
hierarchy \cite{intel:swdev}.

\subsubsection{Multi-node ideas}

We believe we will need hundreds of outstanding memory references per processor to
cover the latency of remote references in a multi-node
system\todo{ can we make a stronger statement? we know it will be 300$+$ according to our calculations}. Unfortunately, commodity processors have much smaller limits
on memory concurrency: section~\ref{subsubsec:evalsinglebase} finds a limit of
36 concurrent loads. We will have to manage memory concurrency on our
own to avoid this limit. We describe approaches in both software and
hardware.

One approach is for user code to delegate global references to a
special {\em global memory manager thread}, running on another core in
the chip. This thread translates the global address into a network
destination, makes the request through the network card to the remote
node, checks for completion, and delivers the returned data to the
requesting thread. On the remote node, the remote memory manager
thread would perform the memory operation and return the data to the
requesting node.

This approach has the benefit of enabling our entire system to be
build using commodity parts, but its potential for performance is not
clear. We have increased the overhead of a single memory operation:
what in the single node case was a simple read, is now multiple
queuing operations between threads and multiple reads and writes over
the processor's link to the network interface.

Another approach moves the global memory management to a hardware
device. We can add a coprocessor FPGA in a processor socket, listening
to coherence messages on the bus. Note that this accelerator is only a
memory proxy, not a compute coprocessor. When it detects a reference
to memory stored on a remote node, the accelerator would issue the
request through the network controller and track its progress,
delivering the data to the requesting thread when it was ready.

All global shared memory would be mapped into a segment of the local
address space on each node. The accelerator would do the
address-to-node translation. In addition, to support the issue and
complete operations for concurrency support, commands would be encoded
in the upper bits of the address: then each remote word would have multiple
addresses on the local node. Reading from the {\em issue} address would
immediately return a dummy value, but the accelerator would issue the
request over the network. Reading from the {\em complete} address
would block until the data was available.\todo{adjust with carl's suggestions}

With either approach, the question of request aggregation will be
important. Each memory request we make is small: 8 bytes is a likely
size. But most networks are optimized for bulk transfers of a few
thousand bytes. To improve network utilization, we will explore the
aggregation of multiple memory operations to different addresses on
the same remote node. 


\subsection{Low-overhead synchronization}

Just as with reads and writes, we enable concurrency around
synchronization operations by splitting them into {\em issue} and {\em
  complete} phases.

Implementing full-empty bit support efficiently on a platform not
designed for them is a challenge. It is possible to support full-empty
bit synchronization on arbitrary words by allocating additional
storage for the full-empty bits, and translating each atomic
full-empty operation into a sequence of memory operations that modify
the data and full-empty bit atomically, but this may have high
overhead. \todo{cite qthreads somehow}

One potential optimization is to limit full-empty synchronization to
pointers to aligned data types, and reuse wasted space in the
low-order bits of the pointer for full-empty bit storage. These bits
would be masked out when the pointer was returned to the user.  This
allows us to synchronize any data type through one level of
indirection.

\subsubsection{Single-node system}
Just as with reads and writes, we prefetch and yield before performing
a synchronization operation. We support only the synchronization
operations supported by our development platform; we have not
implemented full-empty bits.

\subsubsection{Multi-node ideas}

Synchronization operations can also be delegated to a memory manager
thread or accelerator. This may simplify the implementation of
synchronization operations: if only a single core (or single
accelerator) is accessing the data being synchronized, the use of
memory fences may be reduced or eliminated.

Note that we are not advocating using an FPGA as a memory controller
with attached DRAM and additional storage for full-empty bits. We
believe this unnecessarily complicates the design for such a
component. Our goal is to minimize the amount of hardware design our
proposal requires.
\todo{but are we eliminating the possibility of FE-only storage on the FPGA?}

% 2TB global memory => 68GB of distributed FE bits.  This seems reasonable, given that a 2TB system might have 64 nodes, so ~1GB per memory manager if one manager per node.

\section{Evaluation}
\label{sec:evaluation}

Our goal is to evaluate the feasibility of our proposed runtime. We want to determine two things: whether coroutines can generate memory concurrency without incurring minimal performance overhead, and whether the system can support the level of concurrency required to tolerate the latency that will be present in a multi-node system.

We focus the evaluation on one component of the runtime: lightweight context switching. We ran pointer-chasing benchmarks on a single-node implementation of our runtime. These pointer chasing experiments are intended to model a particular ``worst-case'' behavior of irregular applications, where each memory reference causes a cache miss.

We ran these experiments on a Dell PowerEdge R410 with two Xeon X5650 (Nehalem microarchitecture)
chips and 24GB of RAM, with hyperthreading disabled. These
chips use a NUMA memory architecture, where each chip has
its own integrated memory controller and DIMMs; references to other
chips' memory are carried over Intel's cache-coherent QuickPath
Interconnect (QPI) \cite{quickpath:website}.

Our evaulation consists of two parts. First, we demonstrate that the runtime system can achieve the same performance as when there is explicit memory concurrency available. Second, we look at the runtime within the multi-node picture and show that it can tolerate large latencies. In each part we begin by studying relevant aspects of our test machine's memory system so we can interpret our runtime results.


\subsection{Single-node}

We first characterize the performance of our test machine's memory
system. Then we use these results to evaluate the performance of our runtime within a single node.

\subsubsection{Base memory system performance}
\label{subsubsec:evalsinglebase}

We measured two parameters: the maximum random reference
rate that can be issued by the cores in one chip and the maximum random
reference rate that can be serviced by one chip's memory controller. 

To find the maximum random reference rate that one chip's cores
can issue, we ran pointer chasing code following the model of
Figure~\ref{fig:code}a. Each core issues $n$ list traversals
in a loop; we call $n * \textit{number of cores}$ the number of
{\em concurrent offered references}, since the memory system may not
be able to satisfy them all in parallel. Since our goal is to find a
baseline for evaluating our coroutine library, we depend on the
cores' exploitation of ILP for memory concurrency, rather than our coroutine
library.

\begin{figure*}[ht]
%list walk ILP
\begin{minipage}[b]{0.3\linewidth}
\centering
\begin{alltt}{\small
  while (count-- > 0) \{
    list1 = list1->next;
    list2 = list2->next;
    \ldots
    list\(n\) = list\(n\)->next;
  \}
  }\centering{\bf (a)}
\end{alltt}
%\caption{Pseudocode for pointer chasing without coroutines.}
\label{fig:pointernocoro}
\end{minipage}
%listwalk green threads
\begin{minipage}[b]{0.35\linewidth}
\centering
\begin{alltt}{\small
  while (count-- > 0) \{
     prefetch(&(list1->next));
     switch();
     list1 = read(&(list1->next));
 \}
 }\centering{\bf (b)}
\end{alltt}
  %%n lists
  % while (count-- > 0) \{
  % prefetch(&(list1->next));
  %   prefetch(&(list2->next));
   %  \ldots
   %  prefetch(&(list\(n\)->next));
   %  switch();
   %  list1 = read(&(list1->next));
   %  list2 = read(&(list2->next));
   %  \ldots
   %  list\(n\) = read(&(list\(n\)->next));
%\caption{Pseudocode for pointer chasing using coroutines.}
\label{fig:pointercoro}
\end{minipage}
%cache pressure
\begin{minipage}[b]{0.32\linewidth}
\centering
\begin{alltt}{\small
  while (count-- > 0) \{
    prefetch(&(list1->next));
    switch();
    list1 = read(&(list1->next));
    for( i in 1 to num_local_updates ) \{
      local->data++;
      local = local->next;
   \} \}
   }\centering{\bf (c)}
\end{alltt}
%\caption{Pseudocode for pointer chasing with local updates.}
\label{fig:pointerupdate}
\end{minipage}
\vspace{10pt}
\caption{Pseudocode for: (a) pointer chasing without coroutines, (b) pointer chasing using coroutines, (c) pointer chasing with local updates. Coroutine code, as in (b), might also include ILP like what (a) depends on for memory concurrency, e.g., by doing multiple prefetches to independent lists, switch, then multiple reads.}
\label{fig:code}
\end{figure*}

The lists were laid out randomly with pointers spaced at a cache line granularity, maximizing
the probability of each reference being a miss. We allocated the lists
on 1GB huge pages to minimize TLB refill overhead.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.47\textwidth]{figures/multi-listwalk-totalconc-edited.pdf}
	\end{center}
	\caption{Throughput of listwalk versus total number of
          concurrent references offered by the cores. Data points for
          running 1 core are shown as circles, and data points for
          running 6 cores are shown as diamonds. 
%Notice that the throughput for a single core levels out at 10
%concurrent references. For 4 to 6 cores, the throughput levels off at
%around 36 references, which seems to be the most memory concurrency a
%processor can handle.
        }
	\label{fig:listwalk-totalconc}
\end{figure}

Figure~\ref{fig:listwalk-totalconc} shows the result. Each point represents
the maximum rate pointers are traversed for a given number of
concurrent offered references. We see a maximum rate of 277 million references per second (\mrps), which agrees with the measurements found by \cite{Mandal:2010}. This rate is achieved when the number of offered references is 36. Note that a single core cannot support this level of
memory concurrency; the maximum reference rate for a single core is
107M. We believe this limit is due to the core having only enough {\em line fill
  buffers} \cite{nehalem:arch} to store 10 concurrent private cache misses.

The memory controller in the chip has more bandwidth than its
cores can saturate. To measure the memory controller's maximum random
reference rate, we extended the previous experiment so that cores in
both processors were traversing lists allocated in the first processor's
memory. With this configuration, we observed a maximum rate of 360
\mrps. We believe this difference is due to the chip having only 32 buffers in its {\em Global Queue} \cite{nehalem:perf} for tracking concurrently-executing read misses from all the cores' private caches.


\subsubsection{Coroutine performance}

To evaluate the performance of our runtime, we investigated two
effects: the maximum reference rate using coroutines
to obtain memory concurrency and the effects of cache pressure from the
coroutines' context storage. 

To find the maximum reference rate obtainable using our coroutine
library, we modified our pointer chasing benchmark as shown in
Figure~\ref{fig:code}b. Recall that the baseline code in Figure~\ref{fig:code}a relied solely on ILP to achieve memory concurrency, which may not be abundant in real applications. Using coroutines, there are two sources of offered
concurrent references: the ILP exploited by the processor, and the
memory concurrency enabled by prefetching and switching to a new
coroutine. As with our first experiment, we allocated the lists in the
same processor as the cores doing the traversal. 

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.47\textwidth]{figures/multi-green-edited.pdf}
  \end{center}
  \caption{Pointer chasing with coroutines, with one reference per
    coroutine. Number of coroutines per core is $\textit{total concurrent references} / \textit{number of cores}$.}
  \label{fig:multi-green}
\end{figure}

Figure~\ref{fig:multi-green} shows the result. For this
experiment, we limited memory concurrency to one concurrent miss per
coroutine, so the only source of memory concurrency is the use of
coroutines. We are able to obtain a rate of 275 \mrps with 48 concurrent misses, or 8 coroutines per core. These
results are very close to the ILP-only experiment, but more concurrent requests are required--about 48 versus 36. When the number of references per coroutine is larger than 1, fewer coroutines are required to reach full performance.

We observe a gradual decrease in reference rate once the number of
concurrent references per core exceeds 10; we believe this is due to
later prefetches squashing earlier ones in the line fill buffers. In the real runtime we expect this effect will not exist, since prefetch will not be implemented with built-in non-temporal prefetch instructions. We
also observe many points that do not follow the trend are below the max, but these results are repeatable. This effect is harder to explain, but we suspect it is
due to resource contention in the cores' pipelines once the memory
system is full of requests. 

%Such points are repeatably lower, and looks like the std dev of such points (the diamond ones than are low) is also higher than other "normal" points: around 35M versus 13M or less, which seems to suggest a weird effect at those numbers}


The stacks for the coroutines are stored in the data cache, where they
will compete for space with an application's data. To characterize the
effects of this cache pressure, we modified our list chasing benchmark
to include random updates to a per-coroutine local data structure that
is small enough to fit in cache. Figure~\ref{fig:code}c shows the
general idea.

% cache pressure figure
\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.42\textwidth]{figures/cache-pressure-edited.pdf}
  \end{center}
  \caption{Cache pressure with coroutines, on six cores.}
  \label{fig:cache-pressure}
\end{figure}

We varied the size of the local working set from 2KB
to 4MB, and ran both 0 and 32 local updates for each pointer
traversal. We measured the overall reference rate, including both the local updates and remote references.

Figure~\ref{fig:cache-pressure} shows the result. As we would expect,
when there are no references to the local structures, the reference rates grow with the number of coroutines just as in
Figure~\ref{fig:multi-green}. For 32 local updates per traversal, while performance
decreases for large working set sizes and large numbers of coroutines,
we see speedup as number coroutines increases for up to 128KB working set size with 8 coroutines per
core. \todo{preempt questions about drop at 16 which is much less than the about 50 needed per core by saying that the drop is likely the same drop effect seen earlier, which is likely to be prefetch squashing since it happens at number of line fill buffers}

\subsection{Performance with multiple nodes}

To evaluate the potential for our runtime to perform in the multi-node case, we simulated a network delay and saw whether the runtime could tolerate the larger latency. Since the network interface is accessed over QPI, estimated QPI bandwidth by measuring the QPI communication between the two chips of our test machine.

% take this figure out if we need the space
% maybe even take out whole QPI thing and just relate the 100M?
% the main point is room for remote references to local memory
%\begin{figure}[h]
 % \begin{center}
  %  \includegraphics[width=0.42\textwidth]{figures/qpi_bw-edited.pdf}
 % \end{center}
 % \caption{Pointer chasing in a remote processor's memory.}
 % \label{fig:listwalk-qpi}
%\end{figure}
%%%%%%%%%%

For this experiment, we allocated the lists in the
second chip's memory and ran the traversals on the first chip's cores. We found that use of the QPI link limits us to 175 \mrps.

To simulate the performance of pointer chasing in a
multi-node system, we modified the scheduler of our coroutine library
to include a delay before a coroutine can be rescheduled, imitating
the network transit delay even though we are referencing local
memory. Recently published measurements of Infiniband latencies
\todo{cite} suggest we might expect 1.1 $\mu$s source-to-destination
latencies between two computers connected back-to-back; we assume a
switch transit delay of 400 ns \todo{how many hops? 1or3?} in each direction and thus estimate a 3
$\mu$s round-trip delay. In this experiment, once a coroutine has
yielded, we use the core's timestamp counter to ensure it does not get
rescheduled for 3 $\mu$s.

%simulated delay plot
\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.47\textwidth]{figures/multi-green-delay7800-edited.pdf}
  \end{center}
  \caption{Pointer chasing with simulated network delay.}
  \label{fig:network-delay}
\end{figure}

We ran these experiments with two sources of memory concurrency: each
core ran up to 167 coroutines, and each coroutine made between 1 and 4
concurrent references. The results are shown in
Figure~\ref{fig:network-delay}. We are still able to reach a
near-maximum rate of 273 \mrps, but at least 2
concurrent references per coroutine are required. Running with a single
reference per coroutine achieves 236 \mrps. Note that the throughput keeps increasing up to around 850 total concurrent references offered, far more than the 36 we know the chip to support. This is because we have modeled the network latency by forcing coroutines to wait the extra time before using the data, so the physical miss buffers on our machine are not tied up any longer than usual.

We also observe that to achieve a throughput equal to the expected max outgoing network bandwidth (100 \mrps), \todo{reference earlier section calculation?} when there is only one reference per context available, we require 320 coroutines. This is close to the minimum of 300 concurrent references needed to tolerate $3 \mu s$ latency.

Any network interface would be connected through a QPI link,
which we measured to have a bandwidth limit of 175 \mrps, so our runtime appears to have the capacity to chase pointers
in a remote memory. 

\todo{also have results that remote cores over QPI plus delay 3us can make it up to the 175M, with sufficient number of coroutines}


%\todo{Begin eval insert}
%\input{evaluation}
%\todo{End eval insert}

\section{Related work}
\label{sec:related}
The concept of using independent parallel contexts to tolerate latency
is not novel.  The inspiration for our project is of course the Cray
XMT \cite{feo-xmt}, which implemented a hardware version of our
programming model (in what would today be considered exceptionally
slow silicon.)  The XMT can be seen as simultaneous multithreading
\cite{tullsen-smt} writ large; the technique and motivation is
essentially the same, but the XMT allows hundreds of contexts (modern
Intel SMT implementations are generally limited to two.) The Alewife
system \cite{agarwal-alewife} also switches contexts (in hardware) to
hide latency on remote-node accesses.  Cyclops \cite{almasi-cyclops}
allows hundreds of independently executing contexts to share FPUs,
caches, and paths to memory, but does not context switch or share
basic execution hardware.  

Modern programmable GPUs, such as nVidia's CUDA programming model, can also be interpreted in using an XMT-like
model under the hood;
% Do I need to include a citation here?
while they generate large numbers of available execution contexts via
a (semi-)explicit SIMD model, those SIMD instructions keep functional
units satisfied by overlapping execution with the loads required for
other contexts.

Similar techniques have also been used in software.  Mowry proposed a system \cite{mowry-scm} of \emph{software-controlled
  multithreading}.  As opposed to our system of explicitly marked
remote references, SCM assumes a high hit rate and traps to an
interrupt routine which performed a context switch upon a cache miss.  The 2000-era processors had
short enough latencies that using more than two contexts generally
produced slowdown.

Partitioned global address space models, such as that used in Unified Parallel C,
% definitely need a citation here
 share our approach of exposing all data (even remote data)
 transparently to any threads and have largely solved the problems of
 how to efficiently translate those remote reads into message
 passing.  However, their programming models emphasize locality more
 than our work does; while a program can examine any vertex in a UPC
 program at any time, it probably shouldn't (whereas our kernels rely on our
 latency hiding technique to make common remote references fast.

The value of the information in extremely large and sparse graphs is
rapidly growing.  The GraphCT toolkit \cite{ediger-graphct} used an XMT to perform
sophisticated analysis on data from Twitter, identifying important
actors and their networks of influence, and it can scale to the full
size of graphs such as Facebook's friend network.

\section{Conclusion}
\label{sec:conclusion}


\bibliographystyle{acm}
\bibliography{softxmt-hotpar2011}

\end{document}

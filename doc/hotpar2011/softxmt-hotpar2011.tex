%% move stuff to defs file
\newcommand{\todo}[1]{{\bfseries [[#1]]}}
%% To disable, just uncomment this line
%% \renewcommand{\todo}[1]{\relax}

\documentclass{acm_proc_article-sp}
\begin{document}

\title{Latency-tolerant runtimes for large graph computations}

\numberofauthors{7}
\author{
(authors)
}

\maketitle
\begin{abstract}

  Large irregular applications such as sparse graph problems often
  perform badly on commodity clusters. These application make many
  non-sequential accesses to global data structures with low degrees of
  locality, interacting badly with commodity processors' memory
  systems, which are optimized for local, sequential memory
  accesses. Special-purpose hardware optimized for these irregular
  applications has been built; the Cray XMT is one such machine. It
  uses multithreading to tolerate the latency of low-locality memory
  accesses, and while it exhibits excellent performance and energy
  efficiency for irregular applications, it performs badly on standard
  sequential applications. We believe we can have the best of both
  worlds: we argue that modern commodity processors have many of the
  features necessary \todo{Brandon: we will have to be able to list these as features in the paper or use a different phrase; my feeling is that these are aspects of the system which are not optimized for this but are sufficient to suggest potential for enough memory concurrency} to give good performance on irregular problems,
  and that a latency-tolerant runtime designed to exploit these
  features can enable speedup for irregular applications on commodity
  processors. In this paper, we describe the key features of such a
  runtime system, and what a cluster built using our runtime might
  look like. Using a preliminary single-node implementation of our
  runtime, we obtained speedups on simple list chasing benchmarks. Our results show promise for a multi-node implementation.

\end{abstract}

\section{Introduction}

sparse graph problems important (our specific area is sparse graphs with low diameter, like social networks) 

Important 
Social networks
Facebook: 500M active users, 130 friends each on average. for
computing friend suggestions, 40-node cluster, 72GB/node, run every 2
days.

- difficult to partition




Most commodity processors sold today have multiple cores, and their 


perform badly on commodity processors, since most accesses are misses

context-swtiching for latency tolerance has been successful with MTA, but single thread performance sucks

we want latency tolerance on commodity processors for best of both worlds

could just wait for wider multicores, but if context switching works, we can get enough concurrency on fewer cores and shut off the rest.

\section{Programming model for graph problems}

many many threads

prefetch and switch on 

full/empty bits 

\subsection{Opportunities}

characteristics:



little FP use ==> don't worry about FP registers

would like enough concurrency to keep pipeline filled, but commodity CPUs don't have enough external bandwidth. So, get enough to saturate bandwidth.



\section{Approach}

Describe general approach.

\subsection{Low-latency context switch}

switch only stack pointer and program counter; depend on compiler to save/restore anything else. in general case, hopefully this will be fast.

\subsection{Memory concurrency}

Commodity procs don't provide enough. use software? FPGA?

\subsection{Synchronization}

use existing atomic instructions? Use FPGA listening snoops on QPI?


parition remote memory on each node; have one thread access each
partition, making sync easier

\section{Current Implementation and Results}

current: just context switch on single node


linked lists:

there is some room for memory concurrency

thread contexts don't take up too much space in cache

sparse matmul:

speedup?






\section{Related work}


hardware:

XMT/MTA

cyclops

niagra

software:

qthreads

cilk? tbb?



\section{Conclusion}

\end{document}

%% move stuff to defs file
\newcommand{\todo}[1]{{\bfseries [[#1]]}}
%% To disable, just uncomment this line
%% \renewcommand{\todo}[1]{\relax}

\documentclass{acm_proc_article-sp}
\usepackage{url}
\begin{document}

\title{A latency-tolerant runtime for large graph computations}

\numberofauthors{7}
\author{
(authors)
}

\maketitle
\begin{abstract}

  Large irregular applications such as sparse, low diameter graph problems often
  perform badly on commodity clusters. These application make many
  non-sequential accesses to global data structures with low degrees of
  locality, interacting badly with commodity processors' memory
  systems, which are optimized for local, sequential memory
  accesses. Special-purpose hardware optimized for these irregular
  applications has been built; the Cray XMT is one such machine. It
  uses multithreading to tolerate the latency of low-locality memory
  accesses, and while it exhibits excellent performance and energy
  efficiency for irregular applications, it performs badly on standard
  sequential applications. We believe we can have the best of both
  worlds: we argue that modern commodity processors have many of the
  features necessary \todo{Brandon: we will have to be able to list these as features in the paper or use a different phrase; my feeling is that these are aspects of the system which are not optimized for this but are sufficient to suggest potential for enough memory concurrency} to give good performance on irregular problems,
  and that a latency-tolerant runtime designed to exploit these
  features can enable speedup for irregular applications on commodity
  processors. In this paper, we describe the key features of such a
  runtime system, and what a cluster built using our runtime might
  look like. Using a preliminary single-node implementation of our
  runtime, we obtained speedups on simple list chasing benchmarks. Our results show promise for a multi-node implementation.

\end{abstract}

\section{Introduction}


Graphs have proven to be a useful abstraction for solving real-world
problems in social and biological network analysis. But many graphs in
social and biological network analysis are sparse, with low diameter
and a degree distribution that follows a power law. These properties
make it difficult to take advantage of the locality assumptions built
into the memory hierarchy of modern processors, so that even simple
traversals are dominated by memory latency.

And these graphs can be large---too large for a single node \todo{may want to avoid saying 'node' since graph 'node' is also used in this paragraph}. In January 2011, Facebook had 500 million active users with an average of
130 friends each \cite{Facebook:2011p91}. A natural approach would be
to try to partition the graph across the nodes in a cluster. But with
low diameter graphs, a traversal starting at any node is likely to
quickly leave its partition and travel over the network, incurring
high latency costs. And power law graphs have a small number of nodes
that generate significantly more work than other nodes, leading to
computational imbalance.

% Facebook's friend suggestion
% algorithm in 2010 ran on a 40-node cluster with 72GB memory per
% node. Even with that capacity, new suggestions could only be computed
% every two days \cite{Backstrom:2010p90}.


Multithreading is a technique that has been used successfully to
implement efficient computations for these graphs. The Cray XMT is an
example of such a solution: it tries to solve the memory latency
problem through concurrency rather than caching. Each XMT processor
supports 128 hardware contexts and $~$1000 outstanding memory
operations and is able to switch contexts every cycle. But this
approach has a limitation: each context is significantly slower than a
modern commodity processor, yielding bad single-threaded
performance. While the XMT is fast at sparse, low-degree graph
problems, it is slow at other scientific codes that can take advantage
of commodity processors' memory hierarchies. Thus, few XMTs have been
built.

We want the best of both worlds---we want to build a system that has
good performance on both these low-locality graph codes as well as on
common scientific codes. Our approach is inspired by the XMT, but
implemented in software on commodity microprocessors. The core of our
approach is a lightweight coroutine library designed to overlap
long-latency memory prefetches with other contexts' computation. This
document describes our approach.

\todo{do we want to mention hardware at this point? If so, we should
  perhaps call it a hybrid approach.}

\section{Programming model}

many many threads

prefetch and switch on 

full/empty bits 

\subsection{Opportunities}

characteristics:



little FP use ==> don't worry about FP registers

would like enough concurrency to keep pipeline filled, but commodity CPUs don't have enough external bandwidth. So, get enough to saturate bandwidth.



\section{Approach}

Describe general approach, explaining 

\subsection{Low-latency context switch}

switch only stack pointer and program counter; depend on compiler to save/restore anything else. in general case, hopefully this will be fast.

\subsection{Memory concurrency}

Commodity procs don't provide enough. use software? FPGA?

\subsection{Synchronization}

use existing atomic instructions? Use FPGA listening snoops on QPI?


parition remote memory on each node; have one thread access each
partition, making sync easier

\subsection{Extending to a multi-node system}


\section{Current Implementation and Results}

current: just context switch on single node


linked lists:

there is some room for memory concurrency

thread contexts don't take up too much space in cache

sparse matmul:

speedup?






\section{Related work}


hardware:

XMT/MTA

cyclops

niagra

software:

qthreads

cilk? tbb?

why not wait for more cores?

hyperthreading?



\section{Conclusion}

Large irregular problems found in areas such as social networking and
bioinformatics tend to have poor performance on commodity clusters due
to frequent low-locality memory accesses. We present a programming
model and runtime system that uses lightweight context switching to
tolerate long-latency accesses and increase performance on these
applications. Our preliminary evaluation shows that the overhead of
context switching is not prohibitive for thread counts needed for a
large-scale implementation, but memory concurrency is limited by
hardware structures in commodity processors. We describe how we plan
to address the challenges of large-scale memory concurrency and
lightweight synchronization.


\bibliographystyle{acm}
\bibliography{softxmt-hotpar2011}

\end{document}

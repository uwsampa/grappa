%% move stuff to defs file

%command inserts todo/comment
\newcommand{\todo}[1]{{\bfseries [[#1]]}}
%% To disable, just uncomment this line
 \renewcommand{\todo}[1]{\relax}


%\documentclass{acm_proc_article-sp}
\documentclass[10pt,nocopyrightspace]{sigplanconf}
\usepackage{url}
\usepackage{fancyvrb}
\usepackage{fancyhdr}
\usepackage{alltt}
\usepackage{graphicx}
\usepackage{comment}
\usepackage[labelfont=bf,font=small,belowskip=-3pt,aboveskip=-5pt]{caption}
\usepackage[compact]{titlesec}
\usepackage{mdwlist}
\usepackage{units}

% Mref/s
\newcommand{\mrps}[1]{\unit[#1]{Mref/s}}

\begin{document}

%\titlespacing{\section}{0pt}{*0}{*0}
%\titlespacing{\subsection}{0pt}{*0}{*0}
%\titlespacing{\subsubsection}{0pt}{*0}{*0}

%\textheight 9in
%\textwidth 6.5in
%\columnsep 1pc
%\renewcommand{\baselinestretch}{0}

%\title{Can We Crunch the Social Graph on the Cheap?}
\title{Crunching Large Graphs with Commodity Processors}

%\numberofauthors{7}
%\author{
%Jacob Nelson, Brandon Myers, Andrew Hunter, Dan Grossman, Mark Oskin,
%Carl Ebeling, Luis Ceze\\
%University of Washington\\
%\email{\{nelson, bdmyers, ahh, djg, oskin, ebeling, luisceze\}@cs.washington.edu}\\ \\
%Simon Kahan \\ 
%Pacific Northwest National Lab\\
%\email{skahan@cs.washington.edu}\\ \\
%Preston Briggs\\ 
%Cray Inc.\\
%\email{preston.briggs@gmail.com}\\
%}

\authorinfo{Jacob Nelson$^{\dagger}$,
  Brandon Myers$^{\dagger}$,
  A. H. Hunter$^{\dagger}$,
  Preston Briggs$^{\dagger}$
  Luis Ceze$^{\dagger}$,
  Carl Ebeling$^{\dagger}$,\\
  Dan Grossman$^{\dagger}$,
  Simon Kahan$^{{\dagger \ddagger}}$,
  Mark Oskin$^{\dagger}$,
}{\textdagger University of Washington, \textdaggerdbl Pacific Northwest National Laboratory}{
  \{nelson, bdmyers, ahh, preston, luisceze, ebeling, djg, skahan, oskin\}@cs.washington.edu}

%\lfoot{USENIX HotPar 2011, Berkeley}


\maketitle

% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
\thispagestyle{empty}
\pagestyle{empty}


\begin{abstract}
   Crunching large graphs is the basis of many emerging applications,
   such as social network analysis and bioinformatics. Graph analytics
   algorithms exhibit little locality and therefore present significant
   performance challenges. Hardware multithreading systems (e.g., Cray
   {\small XMT}) show that with enough concurrency, we can tolerate long
   latencies. Unfortunately, this solution is not available with
   commodity parts.
 
   Our goal is to develop a latency-tolerant system built out of
   commodity parts and mostly in software. The proposed system includes
   a runtime that supports a large number of lightweight contexts,
   full-bit synchronization and a memory manager that provides a
   high-latency but high-bandwidth global shared memory. This paper
   lays out the vision for our system and justifies its feasibility
   with a performance analysis of the runtime for latency tolerance.
 
\end{abstract}

\renewcommand{\baselinestretch}{0.8}

\section{Introduction}

Many important emerging applications such as social network analysis,
bioinformatics, and sensor networks rely on crunching very large
graphs. Unfortunately, the computational cost of these applications
gets quickly out of hand. While tools to analyze social networks and
query semantic graphs with billions of vertices and edges exist
today~\cite{joslyn:hpsf, ediger:msda, brocheler:cosi, pegasus,
  pregel}, graphs of interest to defense applications are expected to
have trillions of entities~\cite{hartley:mssg, kolda:imkd}. Speeding
up these applications at a low cost would have a large impact in how
we analyze large data-sets and make them even more valuable.




%2002 snapshot of the web has 1.4 billion vertices and 6.6 billion
%edges \cite{pegasus}. Facebook has over 500M active users who interact
%with 900M groups, events, and communities; the average user has 30 friends \cite{Facebook:2011p91}. 
%For example, in January 2011, Facebook had
%500 million active users with an average of 130 friends each
%\cite{Facebook:2011p91}. Its friend suggestion algorithm in 2010 ran on
%  a 40-node cluster with 72GB memory per node, and, still, new
%  suggestions could only be computed every two days
%  \cite{Backstrom:2010p90}. Speeding up these applications at a low
%  cost would have a large impact in how we deal with large data-sets and make them even more valuable.



% A Yahoo snapshot of the World Wide Web graph
% from 2002 has more than 1.4 billion vertices and 6.6 billion edges
% \cite{pegasus, hadi}; the social network Facebook has over 500 million
% active users with 130 friends each on average
% \cite{Facebook:2011p91}. Applications that process these large
% datasets are slow and power-hungry. Speeding up such applications at a
% low cost would have a large impact in how we deal with large data-sets
% and make them even more valuable.


% In January 2011, Facebook had 500 million active users with an average
% of 130 friends each \cite{Facebook:2011p91}.
% Facebook's friend suggestion
% algorithm in 2010 ran on a 40-node cluster with 72GB memory per
% node. Even with that capacity, new suggestions could only be computed
% every two days \cite{Backstrom:2010p90}.


%  The most interesting computational challenge comes from large,
%  low-diameter, power law graphs: this combination makes extracting
% performance difficult. They do not fit in a single
% commodity machine's memory. Their low diameter means that they are
% difficult to lay out with locality: every vertex needs to be near every
% other vertex. Their power law distribution means that a few vertices
% generate much more work than the rest, leading to load
% imbalances. Together, these properties \todo{including power law distr?} lead to a
% difficult conclusion: each edge traversal is likely to lead to a very
% expensive inter-node communication.


The most interesting computational challenge comes from large,
low-diameter, power law graphs: this combination makes extracting
performance difficult. They do not fit in a single commodity machine's
memory. They are difficult to lay out with locality, since every vertex
needs to be near every other vertex. They are difficult to partition
in a balanced way~\cite{lang, leskovec}, leading to hotspots and load
imbalance. Consequently, high-latency inter-node communication becomes
an important limiting factor in scalability~\cite{yoo:parallelbfs}.


% Their low diameter means that they are
% difficult to lay out with locality: every vertex needs to be near every
% other vertex. Their power law distribution means that a few vertices
% generate much more work than the rest, leading to load
% imbalances. 
% Together, these properties \todo{including power law distr?} lead to a
% difficult conclusion: each edge traversal is likely to lead to a very
% expensive inter-node communication.

% They are difficult to partition effi

% Common techniques to 
% Often 



% The most interesting computational challenge comes from large,
% low-diameter, power law graphs: this combination makes extracting
% performance difficult. The Web graph mentioned eariler requires 120GB
% of storage \cite{pegasus}, difficult to fit in a single commodity machine's
% memory. It has an average diameter of 15.6 \cite{hadi}: this low diameter makes it
% difficult to lay out with locality, or to partition across a
% cluster. Graphs with power law distributions are also difficult to
% partition in a balanced way \cite{lang, leskovec}; their few
% high-degree vertices also generate much more work than the rest,
% leading to load imbalances. 

%Together, these properties \todo{including power law distr?} lead to a
%difficult conclusion: each edge traversal is likely to lead to a very
%expensive inter-node communication.

Multithreading is a technique that has been used successfully to
implement efficient computations for these
graphs~\cite{bader:bfsmta}. The Cray {\small XMT} is an example of such an
approach: it solves the memory latency problem through concurrency
rather than caching. Each {\small XMT} processor supports 128 hardware contexts
and 1024 outstanding memory operations, and is able to switch contexts
every cycle~\cite{tera, feo-xmt}. This ability comes at a cost: the
{\small XMT} is an expensive, non-commodity machine with low
single-threaded performance.

\begin{figure*}[htbp]
  \begin{center}
    \vspace{-0.25in}
    \includegraphics[width=0.7\textwidth]{figures/system-overview.pdf}
    \vspace{-0.1in}
	\end{center}
	\caption{Overview of our proposed system design. The base
          system is a cluster of commodity nodes and interconnect. We
          add a latency-tolerant runtime and a global memory manager
          to obtain efficient access to the global address space. Only
          the shaded components may require hardware design.}
	\label{fig:system-overview}
\end{figure*}


We believe we can build a system based mostly on commodity parts that
can attain {\small XMT}-like performance with a familiar, {\small
  XMT}-like, programming model, but at a fraction of the
cost. Therefore, our goal is to build a system that has good
performance on low-locality graph codes but is implemented using cheap
commodity processors with the possible additional support of an FPGA. This
approach has the added benefit of not sacrificing general-purpose
performance.

Figure~\ref{fig:system-overview} shows an overview of our proposal. It
is composed of multiple nodes built with commodity processors,
communicating over an Infiniband network. We add two components: a
runtime system, responsible for executing and managing user threads,
and a global memory manager, responsible for facilitating memory
requests to the global memory space shared across the nodes.
We are exploring a mix of hardware (FPGA) and software implementations
of the memory manager.
 
This paper describes the vision of our system and explores the
feasibility of our ideas using a single-node implementation of a
lightweight threading runtime. We use prefetch instructions together
with lightweight threads to tolerate the latency of DRAM access on the
local node. We use worst-case pointer chasing benchmarks to
verify our runtime's overhead is acceptable. We also model the effects
of pointer chasing with a remote node by artificially increasing
latencies to several microseconds.
 
The rest of the paper is organized as follows. We briefly describe our
programming model in section~\ref{sec:model}. We describe the
implementation of our latency-tolerant runtime and plans for extending
to multiple nodes in section~\ref{sec:approach}. We evaluate our
runtime in section~\ref{sec:evaluation}.  We present related work in
section~\ref{sec:related} and conclude in
section~\ref{sec:conclusion}.


\section{Programming model}
\label{sec:model}

Our goal is to preserve the {\small XMT} programming model: large shared global
address space, explicit concurrency with a large number of threads,
and full-bit synchronization.  We partition the overall address
space into a global shared address space and per node private address
spaces. Locality may be exploited directly by the programmer in the
node private address spaces, just as in a conventional cache-coherent
multiprocessor node. In contrast, locality cannot be exploited in the
global space: its value is in providing high bandwidth random access
to any word in a shared data structure by any processor.

% Whereas in the PGAS model coherency of global copies is implicitly
% maintained across the system, % \todo{cite pgas?}
% we completely forego
% caching of globally shared data.  

% A node can exploit temporal locality of data stored in the global
% space only by first copying it into the node's local space.  In this
% way, a node can access data at random across the system while
% incurring no hardware coherence overhead: consistency is enforced by
% our mechanisms for explicit global synchronization.

The model promises efficiency subject to the presence of sufficient
concurrency.  Exactly how that concurrency is expressed by the
programmer is language dependent. The goal of our system is to support
that concurrency and consequently provide latency tolerance on a large
address space. We do so via software multithreading: computation that
is about to execute a long latency operation (e.g., a remote memory
reference) initiates the operation and then yields to the scheduler,
which quickly resumes execution of another computation. Concurrency 
beyond that required for latency tolerance
can also be used to make more efficient use of network bandwidth: it
enables aggregation of short memory requests into coarser requests,
leading to large network messages and consequently better
bandwidth utilization.

% \todo{vague}---via aggregation of requests to a common node
% destination--- and in
% some cases reduce bandwidth demands.

Synchronization on locations in the private address space of a node
works the same way as on conventional systems.  Synchronization on
global addresses works differently: we provide atomic operations such
as {\tt int\_fetch\_add} as well as operations using full-bits, as on
the Cray {\small XMT}.  As is true for other long latency operations,
synchronization latency is tolerated by yielding to the scheduler.
Even non-blocking global atomics yield so the core can switch to
another computation while the synchronization is accomplished outside
of the pipeline.  In addition, the system can also exploit aggregation
when concurrency is abundant to increase the throughput of global
synchronization.

\section{Implementation}
\label{sec:approach}
Our goal is to use multithreading to tolerate
the latency of random accesses to memory in a global address space
spread across multiple nodes.  Our implementation must provide three features:
\begin{description}
\item[Concurrency in global memory references\quad] We must support many
  outstanding references to our long-latency global memory to fully
  utilize its bandwidth.
%A thread making a
%  long-latency global memory reference must be able to yield and allow
%  other threads to make progress.

\item[Lightweight multithreading\quad] We expect to need hundreds of
  threads to tolerate a cluster's network latency, so our threading
  implementation must have low overhead.

\item[Low-overhead synchronization\quad] 
%  We must support global synchronization between threads that allows
%  unrelated threads to proceed unblocked.
Long-latency synchronization operations should not block the processor's
pipeline, so that other threads can proceed.
  
%Similarly to a regular memory access, long-latency synchronization
%  should also cause a thread to yield and allow others to make
%  progress. %\todo{repetitive with the previous item}
\end{description}
We discuss our approach to solving each of these challenges in
turn. For each one, we discuss both our current single-node
implementation and ideas for a multi-node implementation.


\subsection{Concurrency in global memory references}

To enable concurrency in memory references, we break each read or
write into two operations: the {\em issue} operation and the {\em
  complete} operation. The read issue operation starts data moving
towards the processor and returns immediately, like a prefetch; the
read complete operation blocks until data is available. The write
issue operation takes data along with an address, and starts an
asynchronous write; the write complete operation blocks until the
write is committed.

A latency-tolerant read operation in threaded user code then consists of three steps: a {\em read
  issue}, which causes the data to start moving; a {\em yield}, which
causes another thread to start executing, overlapping the read
latency; and, once the reading thread has been rescheduled, a {\em
  read complete} blocks until the data is available. A write operation
follows the same pattern, blocking until the data has been committed.

%\paragraph{Single-node implementation.}
In our single-node implementation, we use prefetch instructions for
the issue operation, and regular blocking reads and writes for the
complete operation. 
Writes use the prefetch instruction to gain
cache line ownership before modification.
%  Since we expect
%these global references to have low locality, we use the
%``non-temporal'' form of prefetch to avoid disturbing the state of the
%cache hierarchy \cite{intel:swdev}.

%\paragraph{Multi-node ideas.}
As we develop our multi-node implementation, we believe we will need hundreds of outstanding memory references per
processor to cover the latency of remote references in a multi-node
system. Unfortunately, commodity processors have much smaller limits
on memory concurrency: section~\ref{subsubsec:evalsinglebase} finds a
limit of 36 concurrent loads. We will have to manage memory
concurrency on our own to bypass this limit. We describe approaches in
both software and hardware.

One approach is for user code to delegate global references to a
special {\em global memory manager thread}, running on another core in
the chip. This thread translates the global address into a network
destination, makes the request through the network card to the remote
node, checks for completion, and delivers the returned data to the
requesting thread. On the remote node, the remote memory manager
thread performs the memory operation and return the data to the
requesting node.

%This approach has the benefit of enabling our entire system to be
%build using commodity parts, but its potential for performance is not
%clear. We have increased the overhead of a single memory operation:
%what in the single node case was a simple read, is now multiple
%queuing operations between threads and multiple reads and writes over
%the processor's link to the network interface.

Another approach moves the global memory management to a hardware
device. We can add a coprocessor FPGA in a processor socket, listening
to coherence messages on the bus, similar to \cite{mogill}. Note that
this accelerator is only a memory proxy, not a compute
coprocessor. The accelerator maps all global shared memory into a
segment of each node's local address space; when it detects a
reference to memory on a remote node, it does the address-to-node
translation and issues the request through the network controller,
delivering the data to the requesting thread when ready. We 
encode commands in the upper bits of the address; a read from a
location's {\em issue} address starts the remote request and returns
immediately, and a read from a location's {\em complete} address 
blocks until the data is available.


%To support the issue and complete operations for concurrency support,
%commands would be encoded in the upper bits of the address: then each
%remote word would have multiple addresses on the local node. Reading
%from the {\em issue} address would immediately return a value
%indicating that the accelerator is issuing the request over the
%network. Reading from the {\em complete} address would block until the
%data was available.

With either approach, the question of request aggregation will be
important. Each memory request we make is small---perhaps 8
bytes---but most networks are optimized for bulk transfers of a few
thousand bytes. To improve network utilization, we will explore the
aggregation of multiple memory operations to different addresses on
the same remote node.

\subsection{Lightweight multithreading}

Our approach to supporting many lightweight threads uses a
cooperative, user-level multitasking system built using
coroutines. Much work has been done on similar user-level systems
\cite{ult,capriccio}, but we have more stringent requirements:
coroutines must use little space so that many can be active without
trashing the cache; and context switches must be fast (a small
fraction of memory latency) so that we can overlap memory requests and
achieve concurrency.

%We minimize context size by treating context switches as function
%calls, as in~\cite{charm}. This allows us 
%to save and restore the
%minimum number of registers allowed by the ABI, and 
%Simon:  not really true -- we could save fewer if we knew which of the
%callee save registers weren't going to be clobbered.
%\todo{decide whether to change these 2 paragraphs to be inclusive of
%  other lightweight implementations, like jump table. I support the
%  idea of making it more inclusive but I can't think of what to
%  actually change.}  
We treat context switches as function calls, as
in~\cite{charm}. This allows us to depend on the compiler to save and
restore caller-save registers while we explicitly save and restore all
the callee-save registers.  We minimize context switch time by doing
all switching and scheduling in user space.

%\paragraph{Single-node implementation.}  
In our single-node implementation, we implemented coroutines
as described, with a round-robin scheduler. 
%Coroutines do local work,
%issue a remote prefetch, and switch to the next in turn.  When the
%coroutine is reactivated, it assumes the requested data is available
%and issues a read.  If the data is not yet received and in cache, the core
%(and all its coroutines) can stall.

%\paragraph{Multi-node ideas.}
%As we develop our multi-node implementation, we will explore how the memory manager and coroutine scheduler may
%interact, so that threads waiting on long-latency memory operations or
%synchronization operations will be scheduled soon after their
%operations have completed.
As we develop our multi-node implementation, we will modify our
scheduler to reactivate threads only after their long-latency operations
have completed.

% will explore how the memory manager and coroutine scheduler may
%interact, so that threads waiting on long-latency memory operations or
%synchronization operations will be scheduled soon after their
%operations have completed.

\subsection{Low-overhead synchronization}

Just as with reads and writes, we enable concurrency around
synchronization operations by splitting them into {\em issue} and {\em
  complete} operations.

Implementing full-bit support efficiently on a platform not
designed for them is a challenge. Previous work \cite{qthreads} has
enabled support for full-bit synchronization on arbitrary words by
allocating additional storage for the full-bits and implementing
atomic full-bit operations as library routines.

One potential optimization is to limit full-bit synchronization to
pointers to aligned data types, and reuse wasted space in the
low-order bits of the pointer for full-bit storage. These bits
would be masked out when the pointer is returned to the user.  This
allows us to synchronize any data type through one level of
indirection.

%\paragraph{Single-node system.}
In our single-node implementation, we prefetch and yield before performing
a synchronization operation. We support only the synchronization
operations supported by our development platform; we have not yet
implemented full-bits.

%\paragraph{Multi-node ideas.}
In a multi-node implementation, synchronization operations can be delegated to a memory manager
thread or accelerator. Synchronization on remote data would be
performed by the remote memory manager. This may simplify the
implementation: if only a single core (or single accelerator) is
accessing the data being synchronized, the use of memory fences may be
reduced or eliminated.

% 2TB global memory => 68GB of distributed FE bits.  This seems reasonable, given that a 2TB system might have 64 nodes, so ~1GB per memory manager if one manager per node.

\section{Evaluation}
\label{sec:evaluation}

Our goal is to evaluate the feasibility of our proposed runtime. We
want to determine two things: whether coroutines can generate memory
concurrency while incurring minimal performance overhead, and whether the system can support the level of concurrency required to tolerate the latency that will be present in a multi-node system.

We focused the evaluation on one component of the runtime: lightweight context switching. We ran pointer-chasing benchmarks on a single-node implementation of our runtime. These pointer chasing experiments are intended to model a particular ``worst-case'' behavior of irregular applications, where each memory reference causes a cache miss.

We ran these experiments on a Dell PowerEdge R410 with two Xeon X5650 (Nehalem microarchitecture)
chips and 24GB of RAM, with hyperthreading disabled. These
chips use a NUMA memory architecture, where each chip has
its own integrated memory controller and DIMMs; references to other
chips' memory are carried over Intel's cache-coherent QuickPath
Interconnect (QPI) \cite{quickpath:website}.

Our evaluation consists of two parts. First, we demonstrate that the
runtime system can achieve the same performance as when 
explicit memory concurrency is available. Second, we look at the runtime
within the multi-node picture and show that it can tolerate large
latencies. In each part, we begin by studying relevant aspects of our
test machine's memory system so we can interpret our runtime results.


\subsection{Single-node performance}

% We first characterize the performance of our test machine's memory
% system. Then we use these results to evaluate the performance of our runtime within a single node.

\subsubsection{Base memory system performance}
\label{subsubsec:evalsinglebase}

We measured two parameters: the maximum random reference
rate that can be issued by the cores in one chip and the maximum random
reference rate that can be serviced by one chip's memory controller. 

To find the maximum random reference rate that one chip's cores
can issue, we ran pointer chasing code following the model of
Figure~\ref{fig:code}a. Each core issues $n$ list traversals
in a loop; we call $n * \textit{number of cores}$ the number of
{\em concurrent offered references} since the memory system may not
be able to satisfy them all in parallel. Since our goal is to find a
baseline for evaluating our coroutine library, we depend on the
cores' exploitation of ILP for memory concurrency, rather than our coroutine
library.

\begin{figure*}[ht]
\vspace{-0.2in}
%list walk ILP
\begin{minipage}[b]{0.3\linewidth}{\small
\centering
\begin{alltt}{\scriptsize
  while (count-- > 0) \{
    list1 = list1->next;
    list2 = list2->next;
    \ldots
    list\(n\) = list\(n\)->next;
  \}
  }\centering{\bf (a)}
\end{alltt}
%\caption{Pseudocode for pointer chasing without coroutines.}
\label{fig:pointernocoro}
}\end{minipage}
%listwalk green threads
\begin{minipage}[b]{0.35\linewidth}{\small
\centering
\begin{alltt}{\scriptsize
  while (count-- > 0) \{
     readIssue(&(list->next));
     yield();
     list = readComplete(&(list->next));
 \}
 }\centering{\bf (b)}
\end{alltt}
  %%n lists
  % while (count-- > 0) \{
  % prefetch(&(list1->next));
  %   prefetch(&(list2->next));
   %  \ldots
   %  prefetch(&(list\(n\)->next));
   %  switch();
   %  list1 = read(&(list1->next));
   %  list2 = read(&(list2->next));
   %  \ldots
   %  list\(n\) = read(&(list\(n\)->next));
%\caption{Pseudocode for pointer chasing using coroutines.}
\label{fig:pointercoro}
}\end{minipage}
%cache pressure
\begin{minipage}[b]{0.32\linewidth}{\small
\centering
\begin{alltt}{\scriptsize
  while (count-- > 0) \{
    readIssue(&(remoteList->next));
    yield();
    remoteList = readComplete(&(remoteList->next));
    for( i in 1 to num_local_updates ) \{
      localList->data++;
      localList = localList->next;
   \} \}
   }\centering{\bf (c)}
\end{alltt}
%\caption{Pseudocode for pointer chasing with local updates.}
\label{fig:pointerupdate}
}\end{minipage}
\vspace{5pt}
\caption{Pseudocode for: (a) pointer chasing without coroutines, (b) pointer chasing using coroutines, (c) pointer chasing with local updates.}
\label{fig:code}
\end{figure*}

The lists were laid out randomly with pointers spaced at a cache line granularity, maximizing
the probability of each reference being a miss. We allocated the lists
on 1~GB huge pages to minimize overhead due to TLB refills.

\begin{figure}[t]
  \vspace{-.1in}
	\begin{center}
		\includegraphics[width=0.47\textwidth]{figures/multi-listwalk-totalconc-edited.pdf}
	\end{center}
	\vspace{-9pt}
	\caption{Pointer chasing throughput versus total number of
          concurrent references offered by the cores. Total concurrent references offered is \emph{number of lists per core * number of cores}.
%Notice that the throughput for a single core levels out at 10
%concurrent references. For 4 to 6 cores, the throughput levels off at
%around 36 references, which seems to be the most memory concurrency a
%processor can handle.
        }
	\label{fig:listwalk-totalconc}
\end{figure}

Figure~\ref{fig:listwalk-totalconc} shows the result. Each point represents
the maximum rate pointers are traversed for a given number of
concurrent offered references. We see a maximum rate of 277 million references per second (\mrps{}), which agrees with the measurements in \cite{Mandal:2010}. This rate is achieved when the number of offered references is 36. Note that a single core cannot support this level of
memory concurrency; the maximum reference rate for a single core is
\mrps{107}. We believe this limit is due to the core having only enough {\em line fill
  buffers} \cite{nehalem:arch} to store 10 concurrent private cache misses.

The memory controller in the chip has more bandwidth than its
cores can saturate. To measure the memory controller's maximum random
reference rate, we extended the previous experiment so that cores in
both processors traversed lists allocated in the first processor's
memory. With this configuration, we observed a maximum rate of \mrps{360}. We believe this difference is due to the chip having only 32 buffers in its {\em Global Queue} \cite{nehalem:perf} for tracking concurrently-executing read misses from all the cores' private caches.


\subsubsection{Coroutine performance}

To evaluate the performance of our runtime, we investigated two
effects: the maximum reference rate using coroutines
to obtain memory concurrency and the effects of cache pressure from the
coroutines' context storage. 

To find the maximum reference rate obtainable using our coroutine
library, we modified our pointer chasing benchmark as shown in
Figure~\ref{fig:code}b. Recall that the baseline code in Figure~\ref{fig:code}a relied solely on ILP to achieve memory concurrency, which may not be abundant in real applications. Using coroutines, there are two sources of offered
concurrent references: the ILP exploited by the processor, and the
memory concurrency enabled by prefetching and switching to a new
coroutine. As with our first experiment, we allocated the lists in the
same processor as the cores doing the traversal. 

\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.47\textwidth]{figures/multi-green-edited.pdf}
  \end{center}
  \vspace{-9pt}
  \caption{Pointer chasing with coroutines, with one reference per
    coroutine. Number of coroutines per core is $\textit{total concurrent references} / \textit{number of cores}$.}
  \label{fig:multi-green}
\end{figure}

Figure~\ref{fig:multi-green} shows the result. For this experiment,
the only source of memory concurrency is the use of coroutines. We are
able to obtain a rate of \mrps{275} with 48 concurrent misses, or 8
coroutines per core. More concurrent requests are required to saturate
memory bandwidth than in the ILP-only experiment.

We observe a gradual decrease in reference rate once the number of
concurrent references per core exceeds 10; we believe this is due to
later prefetches squashing earlier ones in the line fill buffers. In
the multi-node runtime, \texttt{readIssue} will not be implemented
with a prefetch instruction. 
%The single-node coroutine library does
%depend on prefetches: further experiments show that without them, the
%overhead of context switches limits concurrency when there are more
%than two coroutines per core. 
Lastly, we observe repeatable data points that are off the trend and fall below the max. This effect is harder to explain, but we suspect it may be
due to resource contention in the cores' pipelines once the memory
system is full of requests. 

%Such points are repeatably lower, and looks like the std dev of such points (the diamond ones than are low) is also higher than other "normal" points: around 35M versus 13M or less, which seems to suggest a weird effect at those numbers}


The stacks for the coroutines are stored in the data cache, where they
will compete for space with an application's data. To characterize the
effects of this cache pressure, we modified our list chasing benchmark
to include random updates to a per-coroutine local data structure that
is small enough to fit in cache. Figure~\ref{fig:code}c shows the
general idea.

% cache pressure figure
\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.48\textwidth]{figures/cachepressurenewview-edited.pdf}
  \end{center}
  \vspace{-10pt} %space with fig
  \caption{Cache pressure with coroutines, on six cores. The vertical
    axis is remote reference rate, with (a) 8 coroutines per core, and
    (b) 48 coroutines per core, about the number required for the
    network rate in the simulated delay experiments. }
  \vspace{-1.5pt} % space with text
  \label{fig:cache-pressure}
\end{figure}

We varied the size of the per-coroutine local working set from 64~B
to 4~MB, and ran 0 to 32 local updates for each pointer
traversal. We measured the remote reference rate (i.e., accesses to \texttt{remoteList} from Figure~\ref{fig:code}c).

Figure~\ref{fig:cache-pressure}a shows the result for 8~coroutines per
core.  With 1~to 4~updates per remote reference for smaller working
sets, the runtime achieves near maximum throughput. 
%As working set
%size increases, cache misses in the local space become a
%bottleneck. 
%When there are several updates to large working sets, 
As the number of updates and the working set size increase, the data
of other coroutines is likely to be evicted from cache and performance
suffers.

%As we would expect, when there are no references to the local structures, the reference rates grow with the number of coroutines, just as in Figure~\ref{fig:multi-green}. For 32 local updates per traversal, while performance decreases for large working set sizes and large numbers of coroutines, we see speedup as the number of coroutines increases up to 128KB working set size with 8 coroutines per core.

\subsection{Performance with multiple nodes}

To evaluate the potential for the lightweight contexts to perform in
the multi-node case, we simulated a network delay to see whether the
runtime could tolerate the larger latency.

Any network communication must travel over the QPI link in our test
system. We estimated the bandwidth of this link by allocating lists in
the first chip's memory and traversing those lists on the second
chip's cores. We found that use of the QPI link limits throughput to
\mrps{175}.

% take this figure out if we need the space
% maybe even take out whole QPI thing and just relate the 100M?
% the main point is room for remote references to local memory
%\begin{figure}[h]
 % \begin{center}
  %  \includegraphics[width=0.42\textwidth]{figures/qpi_bw-edited.pdf}
 % \end{center}
 % \caption{Pointer chasing in a remote processor's memory.}
 % \label{fig:listwalk-qpi}
%\end{figure}
%%%%%%%%%%

To simulate the performance of pointer chasing in a multi-node system
we referenced the first chip's memory from the second chip's cores and also had our coroutine scheduler include a delay
before a coroutine can be rescheduled, imitating the network transit
delay. We assume 1.1 $\mu$s interface and 400 ns switch latencies in each direction
\cite{infiniband, mellanox:site}, and thus estimate a 3 $\mu$s round-trip
delay. In analyzing the results, we assume a network issue rate of \mrps{100}, based on modern network interfaces \cite{mellanox:press}.

%simulated delay plot
%TODO: replace with the thru QPI version
\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.47\textwidth]{figures/delay7800-remote-edited.pdf}
  \end{center}
  \vspace{-0.3in}
  \caption{Pointer chasing across QPI link with simulated network delay of $3\mu s$, six cores. Using six cores, 53 coroutines per core are required to reach the assumed network rate of \mrps{100}.}
  \label{fig:network-delay}
\end{figure}

The results of the experiment with six cores are shown in
Figure~\ref{fig:network-delay}. The runtime can still reach a
maximum rate of \mrps{173},\footnote{The throughput keeps increasing
  up to around 564 total concurrent references offered (at which point
  QPI is saturated), far more than the 36 we know the chip to
  support. This is because we have modeled the network latency by
  forcing coroutines to wait the extra time before using the data, so
  the physical miss buffers on our machine are not tied up any longer
  than usual.} saturating the QPI link.  Little's Law predicts that 300 concurrent references are required to achieve the estimated network rate of \mrps{100} when there is $3\mu s$ latency. The results agree: \mrps{100} is reached with about 53 coroutines per core. Although there will be other overheads in a full runtime, this experiment suggests there is potential to tolerate multi-node system latencies.

Figure~\ref{fig:cache-pressure}b shows the results of the cache pressure experiment for supporting 48 coroutines per core, about the number of contexts needed to tolerate latency in the network experiment. We observe that when performing up to 8 local updates per remote reference, smaller working sets still allow the desired throughput. When performing 1 to 2 updates, the runtime can maintain throughput for working sets of up to 32~KB.

%Any network interface would be connected through a QPI link,
%which we measured to have a bandwidth limit of 175 \mrps, so our runtime appears to have the capacity to chase pointers
%in a remote memory. 


\section{Related work}
\label{sec:related}


Much work exists on using multithreading to tolerate latency. Hardware
implementations include the Tera MTA~\cite{tera} and Cray
{\small XMT}~\cite{feo-xmt}, Simultaneous Multithreading \cite{tullsen-smt},
MIT Alewife \cite{agarwal-alewife}, Cyclops \cite{almasi-cyclops}, and
even GPUs \cite{gpus}. Software-only approaches exist as well; the
Software Controlled Multithreading project \cite{mowry-scm}, QThreads
\cite{qthreads}, MAESTRO \cite{maestro}, and Charm \cite{charm} all
use variants of this idea.
%In contrast to all of these, our goal is to tolerate
%latencies across a multi-node system using mostly software.

There has also been much work on user level threading, including the First-Class
User Level Threading project \cite{ult} and Capriccio \cite{capriccio}. We have a
different goal; we want many lightweight contexts to tolerate memory
latency.

Our goal of presenting a global view of distributed memory to the
programmer is shared by the PGAS community, and is used in languages
like Chapel \cite{chapel}, X10 \cite{X10}, and UPC \cite{upc}. They
obtain performance by minimizing references to remote nodes, whereas we
design for remote references.

The idea of processing large graphs on distributed machines has been
explored by projects such as Pregel \cite{pregel}, the Parallel Boost
Graph Library \cite{parallelbgl}, and CGMLib \cite{cgmlib}. Our goal
is to develop infrastructure to aid implementation of similar
libraries.

\section{Conclusion}
\label{sec:conclusion}

We presented our plans for building a system for large graph
computation using commodity parts. We leverage decades-old ideas on
using a large amount of concurrency to tolerate latencies, but we do
so mostly in software. We developed a runtime system for latency
tolerance; our results showed that it supports enough concurrency
to tolerate the long latencies of our large high-bandwidth global
memory system.

%\bibliographystyle{acm}
\bibliographystyle{plain}
\renewcommand{\bibfont}{\footnotesize}  
\bibliography{softxmt-hotpar2011}
\end{document}

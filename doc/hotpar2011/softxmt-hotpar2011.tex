\documentclass{acm_proc_article-sp}
\usepackage{url}
\begin{document}

\title{Latency-tolerant runtimes for large graph computations}

\numberofauthors{7}
\author{
(authors)
}

\maketitle
\begin{abstract}

  Large irregular applications such as sparse graph problems often
  perform badly on commodity clusters. These application make many
  non-sequential accesses to global data structures with low degrees of
  locality, interacting badly with commodity processors' memory
  systems, which are optimized for local, sequential memory
  accesses. Special-purpose hardware optimized for these irregular
  applications has been built; the Cray XMT is one such machine. It
  uses multithreading to tolerate the latency of low-locality memory
  accesses, and while it exhibits excellent performance and energy
  efficiency for irregular applications, it performs badly on standard
  sequential applications. We believe we can have the best of both
  worlds: we argue that modern commodity processors have many of the
  features necessary to give good performance on irregular problems,
  and that a latency-tolerant runtime designed to exploit these
  features can enable speedup for irregular applications on commodity
  processors. In this paper, we describe the key features of such a
  runtime system, and what a cluster built using our runtime might
  look like. Using a preliminary single-node implementation of our
  runtime, we obtained speedups on simple list chasing and sparse
  matrix multiply benchmarks. Our results show promise for a
  multi-node implementation.

\end{abstract}

\section{Introduction}


Many problems in bioinformatics and social networks 







sparse graph problems important

Many 


Social networks are an 

Important 
Social networks

Social networks are 

And social networkIn January 2011, Facebook had 500 million active users with an
average of 130 friends each \cite{Facebook:2011p91}. Facebook's
friend suggestion algorithm in 2010 ran on a 40-node cluster with 72GB
memory per node. Even with that capacity, new suggestions could only
be computed every two days \cite{Backstrom:2010p90}.

We


- difficult to partition




Most commodity processors sold today have multiple cores, and their 


perform badly on commodity processors, since most accesses are misses

context-swtiching for latency tolerance has been successful with MTA, but single thread performance sucks

we want latency tolerance on commodity processors for best of both worlds

could just wait for wider multicores, but if context switching works,
we can get enough concurrency on fewer cores and shut off the rest.


replicate or partition

\section{Programming model for graph problems}

many many threads

prefetch and switch on 

full/empty bits 

\subsection{Opportunities}

characteristics:



little FP use ==> don't worry about FP registers

would like enough concurrency to keep pipeline filled, but commodity CPUs don't have enough external bandwidth. So, get enough to saturate bandwidth.



\section{Approach}

Describe general approach.

\subsection{Low-latency context switch}

switch only stack pointer and program counter; depend on compiler to save/restore anything else. in general case, hopefully this will be fast.

\subsection{Memory concurrency}

Commodity procs don't provide enough. use software? FPGA?

\subsection{Synchronization}

use existing atomic instructions? Use FPGA listening snoops on QPI?


parition remote memory on each node; have one thread access each
partition, making sync easier

\section{Current Implementation and Results}

current: just context switch on single node


linked lists:

there is some room for memory concurrency

thread contexts don't take up too much space in cache

sparse matmul:

speedup?






\section{Related work}


hardware:

XMT/MTA

cyclops

niagra

software:

qthreads

cilk? tbb?



\section{Conclusion}

Large irregular problems found in areas such as social networking and
bioinformatics tend to have poor performance on commodity clusters due
to freqent low-locality memory accesses. We present a programming
model and runtime system that uses lightweight context switching to
tolerate long-latency accesses and increase performance on these
applications. Our preliminary evaluation shows that the overhead of
context switching is not prohibitive for thread counts needed for a
large-scale implementation, but memory concurrency is limited by
hardware structures in commodity processors. We describe how we plan
to address the challenges of large-scale memory concurrency and
lightweight synchronization.


\bibliographystyle{acm}
\bibliography{softxmt-hotpar2011}

\end{document}

%% move stuff to defs file
\newcommand{\todo}[1]{{\bfseries [[#1]]}}
%% To disable, just uncomment this line
 %%\renewcommand{\todo}[1]{\relax}

\documentclass{acm_proc_article-sp}
\usepackage{url}
\usepackage{fancyvrb}
\usepackage{alltt}
\begin{document}

\title{Can We Crunch the Social Graph on the Cheap?}

\numberofauthors{7}
\author{
Jacob Nelson, Brandon Myers, Andrew Hunter, Dan Grossman, Luis Ceze\\
University of Washington\\
\email{\{nelson, bdmyers, ahh, djg, luisceze\}@cs.washington.edu}\\ \\
Simon Kahan \\ 
Pacific Northwest National Lab\\
\email{\{skahan\}@cs.washington.edu}
}

\maketitle
\begin{abstract}

 Techniques from graph analytics have found important applications in
  areas such as social networks and bioinformatics. These irregular
  problems present a challenge for parallel machines: obtaining
  performance is not straightforward. Multithreading works well for
  such problems, but large-scale implementations of the technique like
  the Cray XMT are expensive, non-commodity machines.

  Our goal is to build a system with XMT-like properties, implemented
  with commodity processors. This paper presents a runtime for latency
  tolerance, along with our plans for scaling up to multiple
  nodes. Our analysis shows the feasibility of our approach.
\end{abstract}

\section{Introduction}


Techniques from graph analytics have found important applications in
areas such as social networks and bioinformatics. \todo{more!}


% In January 2011, Facebook had 500 million active users with an average
% of 130 friends each \cite{Facebook:2011p91}.
% Facebook's friend suggestion
% algorithm in 2010 ran on a 40-node cluster with 72GB memory per
% node. Even with that capacity, new suggestions could only be computed
% every two days \cite{Backstrom:2010p90}.

The most interesting computational challenge comes from large,
low-diameter, power law graphs: this combination makes extracting
performance difficult. Their size keeps them from fitting in a single
commodity machine's memory. Their low diameter means that they are
difficult to lay out with locality. Their power law distribution means
that a few vertices generate much more work than the rest, leading to
computational imbalance and even less locality. Together, these
properties lead to a difficult conclusion: each edge traversal is
likely to be a very expensive cache miss.

Multithreading is a technique that has been used successfully to
implement efficient computations for these graphs. The Cray XMT is an
example of such an approach: it solves the memory latency problem
through concurrency rather than caching. Each XMT processor supports
128 hardware contexts and 1024 outstanding memory operations, and is
able to switch contexts every cycle. This ability comes at a cost: the
XMT is an expensive, non-commodity machine.

We want the best of both worlds. Our goal is to build a system that
has good performance on these low-locality graph codes but is
implemented with commodity processors. The core of our approach is a
lightweight coroutine library designed to overlap long-latency memory
prefetches with other contexts' computation.

In this paper, we describe our single-node runtime for latency
tolerance, along with our plans for scaling to multiple nodes.  We
evaluate the performance of our runtime using a set of list traversal
benchmarks designed to model certain worst-case behaviors of these
large graph problems. 

The rest of the paper is organized as follows. We present related work
in section~\ref{sec:related}. We describe the programming model we are
implementing in section~\ref{sec:model}, and describe our approach for
implementing this model in section~\ref{sec:approach}. We evaluate our
latency-tolerant runtime in section~\ref{sec:evaluation} and conclude
in section~\ref{sec:conclusion}.

\section{Related work}
\label{sec:related}


\todo{actually write this}



Multithreading for latency tolerance has been explored in a number of directions.

Hardware approaches: The Tera MTA and Cray XMT, the MIT Alewife
project, ??? use hardware context switches to tolerate memory latency
for large systems. More recently, Simultaneous Multi-Threading,
Niagara, Cyclops, and even GPUs switch between multiple contexts to
tolerate memory latency, for memory that is hundreds of cycles of
away, rather than the thousands we must tolerate.


software:

Mowry's SCM

Cullers' TAM

qthreads

cilk? tbb?

why not wait for more cores?



\section{Programming model}
\label{sec:model}

\todo{Change to discuss API:

context switches:
  yield()
  discuss coroutines

mem:
  alloc()
  read(p) 
  write(p, d)
  prefetch(p)

sync:
 {read, write} {EE, EF, FE, FF}
}

While the ideal programming model is one that facilitates easy
expression in the problem domain as well as efficient execution on the
available hardware, our programming model today is motivated by only
efficient execution: it exists as a rudimentary library for concept
demonstration, not as a language suitable for development.  The
programming model we use today is therefore only partially formed. It
exists as an augmentation of existing threaded programming models such
as pthreads or openmp that are readily available on most computer
systems.

Though not strictly necessary, for ease of exposition we imagine a
one-to-one binding between threads and cores.  Our model manifests
within any thread independently of the others as a ``fray'' \todo{for lack
of better term }.  Within the thread, a fray instance is the
instantiation of a set of coroutines and a scheduler.  Instantiation
is at user-level and the operating system sees the entire fray as as a
single user-level thread.

\todo{ describe calls that perform thread \& scheduler instantiation; and schedule activation }

Each coroutine executes non-preemptively \todo{seems like non-premptively isn't ``how" it executes, rather how it is scheduled, maybe say ``cooperatively"?}, utilizing the core to which
its ``parent'' thread has been assigned by the operating system until it
reaches a yield point.  Typically, the coroutine yields after
initiating a prefetch operation on data that the programmer or
compiler anticipates would probably not complete before the data is
referenced by a subsequent load operation.  In this way, the core
continues to execute instructions for other coroutines when otherwise
it would likely stall.  The programming model thus tolerates latency.

\todo{ describe calls to prefetch-and-yield }

When a thread frays, for the sake of expediency, a fixed portion of
its stack is divided amongst the coroutines.  Given a finite stack,
each coroutine is constrained to execute a call tree of depth known to
be finite at compilation time: recursion is not yet permitted.

\todo{ discussion of coroutines \& synchronization } Were a coroutine to
block on a synchronization variable shared with other threads, the
entire fray would suspend execution.  This can lead to deadlock when,
for example, one coroutine waits to consume from another thread that
is waiting to consume what only another coroutine in this first fray
can produce.  Instead, coroutines must yield on failed synchronization
events, spinning rather than blocking, where were they bona fide
threads, blocking might be more efficient.

\todo{ say something about ``synchronization'' within a fray }

Coroutines completing their work yield without adding themselves to
the scheduling queue.  The last coroutine to exit in this way returns
as the main thread, as in the common  fork-join model of parallelism.


\section{Approach}
\label{sec:approach}

Our goal is to enable high throughput on graphs having two challenging
properties. First, the graphs' combination of sparseness and low
degree means there is little locality to be exploited. Second, the
graphs are larger than a single node's memory and do not partition
well because of low diameter \todo{enough explanation now?}. We must spread the graph across multiple nodes
and send memory operations over a network. If memory requests are
uniformly distributed over the working set, a given node will make as
many accesses to remote nodes' memory as there are remote requests to the local node's memory coming in.

We would like to run as fast as current hardware will let us. Pin
bandwidth is the tightest resource: Intel's QuickPath Interconnect
(QPI) allows approximately 100 million requests per second
\cite{quickpath:website} \todo{specifically 25.6GB/s two way, which we expect 6.4GB/s for one way for outgoing request}, and recent interconnect products allow similar request
rates, with a round-trip latency of approximately 3$\mu$s
\todo{cite} \todo{we calculate the 3us in the evaluation based on some numbers for sw-to-sw plus switches, should we pull that up here?}. Little's law tells us then that we must support on the
order of 300 concurrent memory operations--and thus, up to this many coroutines--to
cover this latency.

These requirements drive the design of our system. There are three key
components. First, the system needs lightweight context switches (coroutines) to be able to support many contexts--thereby increasing memory concurrency available--with minimal impact on performance. Second, a memory management subsystem must support many concurrent accesses to a global address space. Third, the system must provide synchronization at a fine-granularity \todo{this last one sounds incomplete because no reason given}.

\subsection{Lightweight context switch}
\todo{can we include in the title something that means lightweight and good scheduling?}

There are three main concerns in designing the coroutine
mechanism. First, contexts must be small, so many coroutines can be
active without monopolizing the cache. Second, context switches must
be fast, so that overhead does not swamp actual work. Third, when a
coroutine yields, we must choose a suitable coroutine to run next.

We minimize context size by treating context switches as function
calls, as in \cite{charm}. This allows us to save and restore the
minimum number of registers allowed by the ABI, and depend on the
compiler to save and restore any other registers in use. We minimize
context switch time both by keeping contexts small and by doing all
switching and scheduling in user space, as in \todo{cite other green
  threads packages}.

The choice of which coroutine to run next presents us with a
tradeoff. The simplest scheduler chooses coroutines to execute in
round-robin fashion. But this may lead to performance problems: a
coroutine might be scheduled before the event which caused it to yield (e.g., a long latency memory access)
has completed, stalling the pipeline and keeping other coroutines from
progressing. A coroutine might also be scheduled long after the event
which caused it to yield has completed; while this wouldn't block
other coroutines from proceeding, it would add unnecessary latency to
the yielding coroutine. A more intelligent scheduler might allow
coroutines to execute only when they were ready, but this could
increase the overhead of context switching. Our initial implementation
takes the simple approach, but more investigation is warranted.

\subsection{Accessing Global Memory}

% Our goal is to support many concurrent accesses to a global address
% space spread across nodes in a cluster. We can draw on hardware
% support developed for the Partitioned Global Address Space family of
% languages to accomplish this.

% We draw on libraries and network support 
% goal. Languages in the Partitioned Global Address Space family such as
% X10 and Chapel have similar goals, 


% It is also useful to access this low-locality data without disturbing
% the thread contexts and local storage in the cache. Current x86
% processors support non-temporal loads and stores. These memory
% operations are quite relevant for our problem: they are designed to
% move data without polluting the cache. Using these instructions works
% better than mapping the low-locality data as uncached, since that
% restricts the processors' ability to reorder operations.



Following the lead of the PGAS family of languages, we will use
the RDMA functionality provided by modern Infiniband networks to
access our global memory. \todo{rethink?}

These networks allow us to read and write memory directly from remote
nodes, but they do so through a low-level interface designed to be
hidden behind a library. Loads and stores to global memory must be
replaced with calls to a library. This library causes the network
interface to fetch the remote data and place it in a temporary
buffer. When the requesting coroutine is scheduled again, data is
returned from the buffer and execution proceeds.

\subsubsection{Software management}
In order to minimize memory access overhead, the memory operation
library calls do not interact with the network interface
directly. Instead, they queue their operations to another thread, the
{\em memory manager thread}, running on another core in the CPU. This thread
translates the global address into a network destination and issues
the network operation, checks for completion, and manages the returned
data buffers.

But we have to be careful---directing memory requests from multiple
compute threads to a single memory manager thread may reduce memory
concurrency. And we have increased the overhead of a single memory
operation: what in the single node case was a simple read, is now
multiple queuing operations between threads and multiple reads and
writes over QPI to the network interface. Adding more memory manager
threads may enable more memory concurrency, but the overhead is still high \todo{reword so overhead still high sounds supported as above}
\subsubsection{Hardware management}
\todo{might want to call it accelerator throughout after saying FPGA once}
Considering this overhead, we propose hardware acceleration with a {\em memory manager
  FPGA}, a coprocessor FPGA inserted into a Xeon socket so it can
participate in the node's coherence domain. This coprocessor does no
computation---it would only manage the global memory requests of the
main CPU, performing the function of the memory manager thread
described earlier.

By participating in the coherence domain, the memory manager FPGA can
act as a proxy for the rest of global memory. It advertises the
address space of the rest of the system, and translates local loads
and stores to addresses on remote nodes into the network operations
required.

Unfortunately, current processors do not provide sufficient hardware
resources for us to reach our concurrency target using this
approach. Each core in Intel's X5650 Xeon supports only 10 outstanding
misses from its private L2; the entire chip is limited to 32 L2 misses
to local memory and 12 L2 misses for memory reached over the QPI
bus. A simple translation of a local load to a network access would
tie up one of these slots for the entire multi-microsecond latency of
the request. To allow the amount of concurrency we need, we must bypass these structures; the CPU needs to direct the memory manager FPGA
to do work on its behalf without tying up its own resources.

To accomplish this, we encode commands in the high-order bits of
remote memory addresses. The memory manager FPGA advertises each
remote address twice \todo{this sounded like an active thing the FPGA is doing each time you make remote request, first time I read it. Consider just sticking to cmd as upper order address bits than saying advertise each address twice}. The first address is used for blocking reads and
writes---any access incurs the full latency of the remote
access. \todo{verify writes block?} The second address is used for
prefetches and asynchronous writes. These addresses differ by one
high-order bit, easily set with a mask in a macro. A read from the
prefetch address starts the network transaction, but immediately
returns a dummy value. When the value arrives, it is stored in a
temporary buffer; when the requesting coroutine is scheduled again, it
executes the blocking read, and the memory manager immediately returns
the value from the buffer \todo{is this necessarily restricted to the memory manager returning value from buffer rather than buffer in local memory?  maybe it is, since if buffer was in local mem, would have to also write a full bit or make two reads: valid bit and data}. \todo{this isn't fully baked---do we want
  to include this sort of detail?}

\subsection{Synchronization}

In systems with hundreds of threads per node, lightweight
synchronization is important. While we have not implemented any
synchronization primitives yet, full-empty bits are a natural choice
given our programming model \todo{say why natural choice unless programming model section makes this point already}. Two questions then arise: at what
granularity should full-empty bits work, and what component should do
the synchronization?

We expect most data accesses to be to 64-bit words. It is possible to
support full-empty bit synchronization on arbitrary words by
allocating additional storage for the full-empty bits, and translating
each atomic full-empty operation into a sequence of memory operations
that modify the data and full-empty bit atomically. 

One potential optimization is to limit full-empty synchronization to
pointers to aligned 64-bit words. This leaves the bottom three bits
free to store the full-empty bits. These bits would be masked out when
the pointer was returned to the user.

These techniques can be implemented in software; \cite{qthreads} is
one example. Alternatively, atomic operations could be delegated to a
memory manager FPGA as described in the previous section. The address
command encoding would be extended to support the various full-empty
operations.

It might seem natural to use the FPGA as a memory controller as
well, storing all full-empty-bit-capable memory in DRAMs attached to
the FPGA. While this is conceptually simple, it does make the
programming model and the design more complicated. Another possibility
would be to store only the full-empty bits in memories attached to the
FPGA, and leave data in memory attached to the CPU. We believe that
starting simple is best: both data and full-empty bits should be
stored in the CPU's memory. \todo{discuss QPI bandwidth differences}

% 2TB global memory => 68GB of distributed FE bits.  This seems reasonable, given that a 2TB system might have 64 nodes, so ~1GB per memory manager if one manager per node.

\section{Evaluation}
\label{sec:evaluation}

Our goal is to evaluate the feasibility of our proposed runtime. We want to determine two things: whether coroutines can generate memory concurrency without incurring minimal performance overhead, and whether the system can support the level of concurrency required to tolerate the latency that will be present in a multi-node system.

We focus the evaluation on one component of the runtime: lightweight context switching. We ran pointer-chasing benchmarks on a single-node implementation of our runtime. These pointer chasing experiments are intended to model a particular ``worst-case'' behavior of irregular applications, where each memory reference causes a cache miss.

We ran these experiments on a Dell PowerEdge R410 with two Xeon X5650 (Nehalem microarchitecture)
chips and 24GB of RAM, with hyperthreading disabled. These
chips use a NUMA memory architecture, where each chip has
its own integrated memory controller and DIMMs; references to other
chips' memory are carried over Intel's cache-coherent QuickPath
Interconnect (QPI).

Our evaulation consists of two parts. First, we demonstrate that the runtime system can achieve the same performance as when there is explicit memory concurrency available: we measure pointer chasing performance and
the effects of cache pressure due to the number of contexts required. Second, we look at the runtime within the multi-node picture and simulate the effects of fetching memory over a network to show that the runtime can reach the level of concurrency required to hide the latency. In each part we begin by studying relevant aspects of our test machine's memory system so we can interpret our runtime results.


\subsection{Single-node}

We first characterize the performance of our test machine's memory
system. Then we use these results to evaluate the performance of our runtime within a single node.

\subsubsection{Base memory system performance}
\todo{title might be improved}

We measured two parameters: the maximum random reference
rate that can be issued by the cores in one chip and the maximum random
reference rate that can be serviced by one chip's memory controller.\todo{The 360M  "serviced" number could perhaps be an "aside" kind of paragraph since not supporting a major point}

To find the maximum random reference rate that one chip's cores
can issue, we ran pointer chasing code following the model of
Figure~\ref{fig:code}a. Each core issues $n$ list traversals
in a loop; we call $n * \textit{number of cores}$ the number of
{\em concurrent offered references}, since the memory system may not
be able to satisfy them all in parallel. Since our goal is to find a
baseline for evaluating our coroutine library, we depend on the
cores' exploitation of ILP for memory concurrency, rather than our coroutine
library.

\begin{figure*}[ht]
%list walk ILP
\begin{minipage}[b]{0.3\linewidth}
\centering
\begin{alltt}
  while (count-- > 0) \{
    list1 = list1->next;
    list2 = list2->next;
    \ldots
    list\(n\) = list\(n\)->next;
  \}
  \center{{\bf (a)}}
\end{alltt}
%\caption{Pseudocode for pointer chasing without coroutines.}
\label{fig:pointernocoro}
\end{minipage}
%listwalk green threads
\begin{minipage}[b]{0.35\linewidth}
\centering
\begin{alltt}
  while (count-- > 0) \{
     prefetch(&(list1->next));
     switch();
     list1 = read(&(list1->next));
 \}
  \center{{\bf (b)}}
\end{alltt}
  %%n lists
  % while (count-- > 0) \{
  % prefetch(&(list1->next));
  %   prefetch(&(list2->next));
   %  \ldots
   %  prefetch(&(list\(n\)->next));
   %  switch();
   %  list1 = read(&(list1->next));
   %  list2 = read(&(list2->next));
   %  \ldots
   %  list\(n\) = read(&(list\(n\)->next));
%\caption{Pseudocode for pointer chasing using coroutines.}
\label{fig:pointercoro}
\end{minipage}
%cache pressure
\begin{minipage}[b]{0.32\linewidth}
\centering
\begin{alltt}
  while (count-- > 0) \{
    prefetch(&(list1->next));
    switch();
    list1 = read(&(list1->next));
    for( i in 1 to num_local_updates ) \{
      local->data++;
      local = local->next;
   \}
  \}
   \center{{\bf (c)}}
\end{alltt}
%\caption{Pseudocode for pointer chasing with local updates.}
\label{fig:pointerupdate}
\end{minipage}
\caption{Pseudocode for: (a) pointer chasing without coroutines, (b) pointer chasing using coroutines, (c) pointer chasing with local updates.}
\label{fig:code}
\end{figure*}


The lists were sized to exceed the last level cache and were laid out
randomly with pointers spaced at a cache line granularity, maximizing
the probability of each reference being a miss. We allocated the lists
on 1GB huge pages to minimize TLB refill overhead. The lists were
allocated in the memory attached to the processor containing the cores
doing the traversal. We ran 10 traversals, measuring average rate in each, and recorded the maximum rate.


\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.5\textwidth]{figures/multi-listwalk-totalconc-edited.pdf}
	\end{center}
	\caption{Throughput of listwalk versus total number of
          concurrent references offered by the cores. Data points for
          running 1 core are shown as circles, and data points for
          running 6 cores are shown as diamonds. 
%Notice that the throughput for a single core levels out at 10
%concurrent references. For 4 to 6 cores, the throughput levels off at
%around 36 references, which seems to be the most memory concurrency a
%processor can handle.
        }
	\label{fig:listwalk-totalconc}
\end{figure}

Figure~\ref{fig:listwalk-totalconc} shows the result. Each point represents
the maximum rate pointers are traversed for a given number of
concurrent offered references. We see a maximum rate of 277 million
references per second, which agrees with the measurements found by \cite{Mandal:2010} with a similar machine. This rate is achieved when the number of offered references is 36. Note that a single core cannot support this level of
memory concurrency; the maximum reference rate for a single core is
107M. We believe this limit is due to the core's limited number of {\em line fill
  buffers} \cite{nehalem:arch}, which limits the core to 10
concurrent private cache misses.

The memory controller in the chip has more bandwidth than its
cores can saturate. To measure the memory controller's maximum random
reference rate, we extended the previous experiment so that cores in
both processors were traversing lists allocated in the first processor's
memory. With this configuration, we observed a maximum rate of 360
million references per second. We believe this difference is due to
the configuration of the processor's {\em Global Queue} \cite{nehalem:perf}, which limits the number of concurrently-executing read misses from a chips cores to its local last-level cache and memory to 32.


\subsubsection{Runtime performance}

To evaluate the performance of our runtime, we investigated two
effects: the maximum reference rate using coroutines
to obtain memory concurrency and the effects of cache pressure from the
coroutines' context storage. 

To find the maximum reference rate obtainable using our coroutine
library, we modified our pointer chasing benchmark as shown in
figure~\ref{fig:code}b. Now there are two sources of offered
concurrent references: the ILP exploited by the processor, and the
memory concurrency enabled by prefetching and switching to a new
coroutine. As with our first experiment, we allocated the lists in the
same processor as the cores doing the traversal. 

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figures/multi-green-edited.pdf}
  \end{center}
  \caption{Pointer chasing with coroutines, with one reference per
    coroutine. Number of coroutines per core is $\textit{total concurrent references} / \textit{number of cores}$.}
  \label{fig:multi-green}
\end{figure}

Figure~\ref{fig:multi-green} shows the result. For this
experiment, we limited memory concurrency to one concurrent miss per
coroutine, so the only source of memory concurrency is the use of
coroutines. We are able to obtain a rate of 275 million references per
second with 48 concurrent misses, or 8 coroutines per core. These
results are very close to the ILP-only experiment, but more concurrent requests are required--about 48 versus 36. When the number of references per coroutine is larger than 1, fewer coroutines are required to reach full performance.

We observe a gradual decrease in reference rate once the number of
concurrent references per core exceeds 10; we believe this is due to
later prefetches squashing earlier ones in the line fill buffers. We
also observe some points that do not follow the trend and look like noise, but these results are actually repeatable. This effect is harder to explain, but we suspect it is
due to resource contention in the cores' pipelines once the memory
system is full of requests.

%Such points are repeatably lower, and looks like the std dev of such points (the diamond ones than are low) is also higher than other "normal" points: around 35M versus 13M or less, which seems to suggest a weird effect at those numbers}


The stacks for the coroutines are stored in the data cache, where they
will compete for space with an application's data. To characterize the
effects of this cache pressure, we modified our list chasing benchmark
to include random updates to a per-coroutine local data structure that
is small enough to fit in cache. Figure~\ref{fig:code}c shows the
general idea.


\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figures/cache-pressure-edited.pdf}
  \end{center}
  \caption{Cache pressure with coroutines.}
  \label{fig:cache-pressure}
\end{figure}

We ran this code on six cores with all data allocated in the same
processor's memory. We varied the size of the local working set from 2KB
to 4MB, and ran both 0 and 32 local updates for each pointer
traversal. We measured the overall reference rate, including both the local updates and remote references.

Figure~\ref{fig:cache-pressure} shows the result. On the left of the
figure, we see the list-traversal-only case, where we allocate
per-coroutine space but make no references to it. As we would expect,
the reference rates grow with the number of coroutines just as in
Figure~\ref{fig:multi-green}. On the right of the figure, we see the
case with 32 updates for each list traversal. While performance
decreases for large working set sizes and large number of coroutines,
we see speedup as number coroutines increases for up to 128KB working set size with 8 coroutines per
core.

\subsection{Multi-node}

To evaluate the potential for our runtime to perform in the multi-node case, we simulated a network delay and saw whether the runtime could tolerate the larger latency. First we measure the QPI bandwidth between the two chips of the test machine, since the chip would talk to the network interface over a QPI link.

% take this figure out if we need the space
% the main point is room for remote references to local memory
\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figures/qpi_bw-edited.pdf}
  \end{center}
  \caption{Pointer chasing in a remote processor's memory.}
  \label{fig:listwalk-qpi}
\end{figure}
%%%%%%%%%%

We measured the rate at which the cores can make requests
over the QPI link. For this experiment, we allocated the lists in the
second chip's memory and ran the traversals on the first chip's cores. Figure~\ref{fig:listwalk-qpi} shows the result. We found that use of the QPI link limits us to 175 million references per second.


To simulate the performance of pointer chasing in a
multi-node system, we modified the scheduler of our coroutine library
to include a delay before a coroutine can be rescheduled, imitating
the network transit delay even though we are referencing local
memory. Recently published measurements of Infiniband latencies
\todo{cite} suggest we might expect 1.1 $\mu$s source-to-destination
latencies between two computers connected back-to-back; we assume a
switch transit delay of 400 ns \todo{how many hops} in each direction and thus estimate a 3
$\mu$s round-trip delay. In this experiment, once a coroutine has
yielded, we use the core's timestamp counter to ensure it does not get
rescheduled for 3 $\mu$s.

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figures/multi-green-delay7800-edited.pdf}
  \end{center}
  \caption{Pointer chasing with simulated network delay.}
  \label{fig:network-delay}
\end{figure}

We ran these experiments with two sources of memory concurrency: each
core ran up to 167 coroutines, and each coroutine made between 1 and 4
concurrent references. The results are shown in
Figure~\ref{fig:network-delay}. We are still able to reach a
near-maximum rate of 273 million references per second, but at least 2
concurrent references per coroutine are required. Running with a single
reference per coroutine achieves 236 million references per
second. Any network interface would be connected through a QPI link,
which we measured to have a bandwidth limit of 175 million references
per second, so our runtime appears to have the capacity to chase pointers
in a remote memory. 

\todo{do we want to include that the memory system imposes max limit so really more throughput is still achievable if there are enough ILP memory concurrency?}\\
\todo{also have results that remote cores over QPI plus delay 3us can make it up to the 175M, with sufficient number of coroutines}


%\todo{Begin eval insert}
%\input{evaluation}
%\todo{End eval insert}


\section{Conclusion}
\label{sec:conclusion}



\bibliographystyle{acm}
\bibliography{softxmt-hotpar2011}

\end{document}

%% move stuff to defs file
\newcommand{\todo}[1]{{\bfseries [[#1]]}}
%% To disable, just uncomment this line
%% \renewcommand{\todo}[1]{\relax}

\documentclass{acm_proc_article-sp}
\usepackage{url}
\begin{document}

\title{A latency-tolerant runtime for large graph computations}

\numberofauthors{7}
\author{
(authors)
}

\maketitle
\begin{abstract}

  Large irregular applications such as sparse, low diameter graph problems often
  perform badly on commodity clusters. These application make many
  non-sequential accesses to global data structures with low degrees of
  locality, interacting badly with commodity processors' memory
  systems, which are optimized for local, sequential memory
  accesses. Special-purpose hardware optimized for these irregular
  applications has been built; the Cray XMT is one such machine. It
  uses multithreading to tolerate the latency of low-locality memory
  accesses, and while it exhibits excellent performance and energy
  efficiency for irregular applications, it performs badly on standard
  sequential applications \todo{Brandon: should this be more specific or worded differently? Its true sequential perform badly since switching between many threads, but also does XMT also perform  poorly (relative to alternatives) on highly parallel but regular apps with lots of temporal and spatial locality?}. We believe we can have the best of both
  worlds: we argue that modern commodity processors have many of the
  features necessary \todo{Brandon: we will have to be able to list these as features in the paper or use a different phrase; my feeling is that these are aspects of the system which are not optimized for this but are sufficient to suggest potential for enough memory concurrency} to give good performance on irregular problems,
  and that a latency-tolerant runtime designed to exploit these
  features can enable speedup for irregular applications on commodity
  processors. In this paper, we describe the key features of such a
  runtime system, and what a cluster built using our runtime might
  look like. Using a preliminary single-node implementation of our
  runtime, we obtained speedups on simple list chasing benchmarks. Our results show promise for a multi-node implementation.

\end{abstract}

\section{Introduction}


Graphs have proven to be a useful abstraction for solving real-world
problems in social and biological network analysis. But many graphs in
social and biological network analysis are sparse, with low diameter
and a degree distribution that follows a power law. These properties
make it difficult to take advantage of the locality assumptions built
into the memory hierarchy of modern processors, so that even simple
traversals are dominated by memory latency.

And these graphs can be large---too large for a single node \todo{may want to avoid saying 'node' since graph 'node' is also used in this paragraph}. In January 2011, Facebook had 500 million active users with an average of
130 friends each \cite{Facebook:2011p91}. A natural approach would be
to try to partition the graph across the nodes in a cluster. But with
low diameter graphs, a traversal starting at any node is likely to
quickly leave its partition and travel over the network, incurring
high latency costs. And power law graphs have a small number of nodes
that generate significantly more work than other nodes, leading to
computational imbalance.

% Facebook's friend suggestion
% algorithm in 2010 ran on a 40-node cluster with 72GB memory per
% node. Even with that capacity, new suggestions could only be computed
% every two days \cite{Backstrom:2010p90}.


Multithreading is a technique that has been used successfully to
implement efficient computations for these graphs. The Cray XMT is an
example of such a solution: it tries to solve the memory latency
problem through concurrency rather than caching. Each XMT processor
supports 128 hardware contexts and $~$1000 outstanding memory
operations and is able to switch contexts every cycle. But this
approach has a limitation: each context is significantly slower than a
modern commodity processor, yielding bad single-threaded
performance. While the XMT is fast at sparse, low-degree graph
problems, it is slow at other scientific codes that can take advantage
of commodity processors' memory hierarchies. Thus, few XMTs have been
built.

We want the best of both worlds---we want to build a system that has
good performance on both these low-locality graph codes as well as on
common scientific codes. Our approach is inspired by the XMT, but
implemented in software on commodity microprocessors. The core of our
approach is a lightweight coroutine library designed to overlap
long-latency memory prefetches with other contexts' computation. This
document describes our approach.

\todo{do we want to mention hardware at this point? If so, we should
  perhaps call it a hybrid approach.}

\section{Programming model}

While the ideal programming model is one that facilitates easy
expression in the problem domain as well as efficient execution on the
available hardware, our programming model today is motivated by only
efficient execution: it exists as a rudimentary library for concept
demonstration, not as a language suitable for development.  The
programming model we use today is therefore only partially formed. It
exists as an augmentation of existing threaded programming models such
as pthreads or openmp that are readily available on most computer
systems.

Though not strictly necessary, for ease of exposition we imagine a
one-to-one binding between threads and cores.  Our model manifests
within any thread independently of the others as a ``fray'' [[for lack
of better term ]].  Within the thread, a fray instance is the
instantiation of a set of coroutines and a scheduler.  Instantiation
is at user-level and the operating system sees the entire fray as as a
single user-level thread.

[[ describe calls that perform thread & scheduler instantiation; and schedule activation ]]

Each coroutine executes non-preemptively, utilizing the core to which
its ``parent'' thread has been assigned by the operating system until it
reaches a yield point.  Typically, the coroutine yields after
initiating a prefetch operation on data that the programmer or
compiler anticipates would probably not complete before the data is
referenced by a subsequent load operation.  In this way, the core
continues to execute instructions for other coroutines when otherwise
it would likely stall.  The programming model thus tolerates latency.

[[ describe calls to prefetch-and-yield ]]

When a thread frays, for the sake of expediency, a fixed portion of
its stack is divided amongst the coroutines.  Given a finite stack,
each coroutine is constrained to execute a call tree of depth known to
be finite at compilation time: recursion is not yet permitted.

[[ discussion of coroutines & synchronization ]] Were a coroutine to
block on a synchronization variable shared with other threads, the
entire fray would suspend execution.  This can lead to deadlock when,
for example, one coroutine waits to consume from another thread that
is waiting to consume what only another coroutine in this first fray
can produce.  Instead, coroutines must yield on failed synchronization
events, spinning rather than blocking, where were they bonafide
thereads, blocking might be more efficient.

[[ say something about ``synchronization'' within a fray ]]

Coroutines completing their work yield without adding themselves to
the scheduling queue.  The last coroutine to exit in this way returns
as the main thread, as in the common  fork-join model of parallelism.

many many threads

prefetch and switch on 

full/empty bits 

\subsection{Opportunities}

characteristics:



little FP use ==> don't worry about FP registers

would like enough concurrency to keep pipeline filled, but commodity CPUs don't have enough external bandwidth. So, get enough to saturate bandwidth.



\section{Approach}

Describe general approach, explaining 

\subsection{Low-latency context switch}

switch only stack pointer and program counter; depend on compiler to save/restore anything else. in general case, hopefully this will be fast.

\subsection{Memory concurrency}

Commodity procs don't provide enough. use software? FPGA?

\subsection{Synchronization}

use existing atomic instructions? Use FPGA listening snoops on QPI?


parition remote memory on each node; have one thread access each
partition, making sync easier

\subsection{Extending to a multi-node system}


\section{Current Implementation and Results}

\todo{Begin eval insert}
\input{evaluation}
\todo{End eval insert}

current: just context switch on single node


linked lists:

there is some room for memory concurrency

thread contexts don't take up too much space in cache

sparse matmul:

speedup?






\section{Related work}


hardware:

XMT/MTA

cyclops

niagra

software:

qthreads

cilk? tbb?

why not wait for more cores?

hyperthreading?



\section{Conclusion}

Large irregular problems found in areas such as social networking and
bioinformatics tend to have poor performance on commodity clusters due
to frequent low-locality memory accesses. We present a programming
model and runtime system that uses lightweight context switching to
tolerate long-latency accesses and increase performance on these
applications. Our preliminary evaluation shows that the overhead of
context switching is not prohibitive for thread counts needed for a
large-scale implementation, but memory concurrency is limited by
hardware structures in commodity processors. We describe how we plan
to address the challenges of large-scale memory concurrency and
lightweight synchronization.


\bibliographystyle{acm}
\bibliography{softxmt-hotpar2011}

\end{document}

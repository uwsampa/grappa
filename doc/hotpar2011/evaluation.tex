

To study the memory concurrency in the system, we did experiments that approximate large irregular applications. The experiments consist of multiple threads each walking a number of separate linked lists. Each memory reference of a node is highly likely to result in an access to main memory, and we control the number of concurrent requests offered by a thread by varying the number of lists each thread walks. \todo{Brandon: somewhere we might explicitly say that this experiment involves no synchronization, and relate to plan to build this as mentioned in the components of runtime section} \todo{Brandon: is it important to mention that for isolation we use huge pages to reduce TLB misses?} Our experiments were run on a Xeon X5660, which uses the Nehalem microarchitecture, with 2 sockets, 6 cores per socket, and 12GB of DRAM per socket. For the results listed, memory allocations were pinned to socket0's DRAM.
\subsection{Results}
 
\todo{crop the pdfs}
\todo{enlarge font of tableau plot since pdf is getting scaled down}
\begin{figure*}
	\begin{center}
 		\includegraphics[width=0.5\textwidth]{figures/multi-listwalk-maxrates.pdf}
	\end{center}
	\caption{The max rates for processing list nodes with varying numbers of cores. Each core walks one list, and each node access results in a reference to main memory. All memory is local to socket0. Cores 1-6 are on socket0, and cores 7-12 are on socket1.}
	\label{fig:listwalk-maxrates}
\end{figure*}

\begin{figure*}
	\begin{center}
 		\includegraphics[width=0.5\textwidth]{figures/multi-listwalk-outflats.pdf}
	\end{center}
	\caption{}
	\label{fig:listwalk-outflats}
\end{figure*}

\todo{choose best phrase to use for outstanding/concurrent miss/refs/reads and use consistently unless there is a place one makes most sense}
\todo{if mentioning the cores 7-12 then need to talk about QPI link probably}
	 Figure~\ref{fig:listwalk-outflats} shows the total number of outstanding misses the system can handle before performance plateaus. We found that a single core is bandwidth-limited by the size of the L1 cache line-fill buffer (keeps track of outstanding misses of the core). Bandwidth \todo{word:performance?} plateaus at about 10 concurrent misses, the size of the buffer. Running multiple cores, the system reaches a bandwidth plateau with fewer outstanding misses per core. In fact, the maximum number of total memory references in flight on a single socket is consistently 36 (see figure). When using all six cores on the socket, bandwidth peaks at about 275Mref/s. Since this is less than even a conservative bound on available pin bandwidth, there is some bottleneck to performance in the memory system. A likely place is the global read-tracker queue containing references to the L3 cache; it has a capacity of 32 references, which concurs with the observed limit of 36 in-flight references. In addition, we performed measurements using performance counters, which consistently showed an average queue occupancy of 27, also close to the capacity 32.
 \todo{Brandon: it is great to identify why the bandwidth is limited, but in paper need to connect to why it is important. in general, we get an idea of what might need to be changed, especially as latency goes up and we need more concurrency}  \todo{Brandon: may cite nehalem bandwidth numbers here as well to validate our bw numbers with a second source}
	Bandwidth of memory requests from cores on a single socket seems to peak at about 275Mreq/s; however adding requests to socket0's memory from socket1 cores increases the bandwidth to about	350Mreq/s, showing that there is additional room in the memory system that is simply reserved for remote requests.\todo{Currently a bit of a 
hanging statement, but it is intended to show more evidence of limit being queue.}

\begin{figure*}
	\begin{center}
 		\includegraphics[width=0.5\textwidth]{figures/multi-core-green-thread-1gb.pdf}
	\end{center}
	\caption{}
	\label{fig:listwalk-green-threads}
\end{figure*}

\subsection{Using green threads}
	If a core is to generate some number of concurrent memory requests to tolerate latency and keep busy, it may need multiple contexts \todo{Brandon: this idea likely goes elsewhere in describing overall approach}.  We ran our experiments with varying numbers of green threads per core to use multiple contexts to achieve concurrency. Our results, displayed in Figure~\ref{fig:listwalk-green-threads} \todo{decide whether max/avg rate as well as how much info/whether to make consistent with the other plots format}, show that using green threads gives little to no overhead due to context switching. \todo{Brandon: somehow also tie in prefetch-and-switch; also might elaborate on the results and yield-only if space}


\subsection{Considering the 'multi-node case'}
	\todo{needs introduction, which will depend on what the system description section looks like}
	We assume that the QPI link to the network interface will be the minimum bandwidth and not the network. On the Nehalem, the QPI link to the network interface provides a bandwidth 12.8GB/s each way. This as much as 200Mref/s for 64 byte references. Given that half of the bandwidth will go towards satisfying remote requests, leaving half to making requests \todo{n-1/n req vs. sum-over-n(1/(n-1))}.  So, the cores must put out a bandwidth of 100Mref/s for full utilization of the link.  Assuming a reasonable network latency of $1-2\mu s$, by Little's Law, the number of outstanding references is $100Mref/s*1-2\mu s = 100-200$. This is the amount of memory concurrency that needs to be generated for the latency of memory requests to be hidden.
We consider two issues: first, can we support 100-200 misses?, and second, can the caches support enough contexts? \todo{Brandon: might want to be more explicit about the interplay between cache pressure, number of contexts, amount of references that can be generated by X number of contexts, etc}
	Can we support 100-200 misses? If the remote requests are reads that go to the global queue, then the number of outstanding references is limited to about 36. If the remote requests do not need to be stored in the global queue, then the line-fill buffers of the cores can support a total of $6 cores * 10 ref/core = 60 references$, still not quite enough. If, however, prefetches are messages--memory stores--requesting a piece of remote memory then many more outstanding references could be supported, either with a software data structure or a memory management device implemented in an FPGA.
\todo{talk about the simulated delay experiments? mention that there are still the same memory system limits like queues so results don't show what perf would look like beyond a certain point, but do show that by adding contexts we can achieve the same performance when latency goes up.}
	Can the caches support enough contexts? To start exploring this question, we did cache pressure experiments measuring the effect of working set size and number of software contexts on performance. Our experiments suggest that with up to about 8 contexts/core, we can achieve almost the same max bandwidth, and for one update per remote reference (expected for basic graph comps?), working set size/thread of up to 64KiB scales well. \todo{Brandon: need to be more specific.  Also, are there important highlights regarding how the plot scales as local updates increases}.  Threads do not take up much space in the cache, shown by XXX.

\todo{should we mention the lack of synchronization in the experiments and how intend to explore it in next step, given that it is part of our proposed runtime?}

\todo{order seems okay, but rework organization of subsections}

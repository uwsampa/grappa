\section{Tasking System}

Below we discuss the implementation of our task management support and then
describe how applications expose parallelism to the \Grappa runtime.

\subsection{Task Support Implementation}

The basic unit of execution in \Grappa is a {\em task}. When tasks are ready to
execute, they are mapped to a {\em worker}, which is akin to a user-level
thread. Each hardware core has a single operating system thread pinned to it.

\paragraph{Tasks} 
Tasks are specified by a ``functor'' object that holds both code to execute and initial state. The functor can be specified with a function pointer and explicit arguments, a C++ struct that overloads the parentheses operator, or a C++11 lambda. These objects, typically very small (on the order of 64 bytes), hold read-only values such as an iteration index and pointers to common data or synchronization objects. Task functors can be serialized and transported around the system, and eventually executed by a worker, as described next.

\paragraph{Workers} Workers execute application and system (e.g.,
communication) tasks. A worker is simply a collection of status bits and a
stack, allocated at a particular core. When a task is ready to execute it
is assigned to a worker, that executes the task functor on its own stack. 
Once a task is mapped to a worker it stays with that worker until it finishes.

\paragraph{Scheduling} During execution, a worker yields control of its core
whenever performing a long-latency operation, allowing the processor to
remain busy while waiting for the operation to complete. In addition, a
programmer can direct scheduling explicitly via the \Grappa API calls shown in
Figure~\ref{fig:scheduling}. To minimize yield overhead, the \Grappa scheduler
operates entirely in user-space and does little more than store state of one
worker and load that of another.

\begin{figure}[htbp]
  \begin{center}
	\begin{tabular}{l}
    \texttt{\scriptsize yield() } \\
      Yields core to scheduler, enqueuing caller to be \\ scheduled again soon \\
    \texttt{\scriptsize wake( task * $t$ ) } \\
      Enqueues some other task $t$ to be scheduled again soon \\
    \texttt{\scriptsize suspend() }  \\
      Yields core to scheduler, enqueuing caller only once \\ another task calls wake \\
	\end{tabular}
    \begin{minipage}{0.95\columnwidth}
      \caption{\label{fig:scheduling} \Grappa scheduling API } 
    \end{minipage}
  \end{center}
\end{figure}

Each core in a \Grappa system has its own independent scheduler. The scheduler
has a single FIFO queue of active workers ready to execute, the {\it
ready worker queue}. Each scheduler also has three queues of tasks
waiting to be assigned a worker:

\begin{itemize}

\item {\it deadline task queue}, a priority queue of tasks that are executed according to task-specific deadline constraints;

\item {\it private task queue}, a FIFO queue of tasks that must run on
this core and therefore is not subject to stealing;

\item {\it public task queue},  a LIFO queue of tasks that are
  waiting to be matched with workers. It is a local partition of a shared
  task pool.

\end{itemize}


Whenever a task yields or suspends, the scheduler makes a decision about what
to do next. First, any task in the deadline task queue who's deadline
constraint is met is chosen for execution. This queue manages high priority
system tasks, such as periodically servicing communication requests. Second,
the scheduler determines if any workers with running tasks are ready to
execute; if so, one is scheduled. Finally, if there are no workers ready to
run, but there are tasks waiting to be matched with workers, an idle worker is
woken (or a new worker is spawned), matched with a task, and scheduled.

\paragraph{Context switching} 
\Grappa context switches between workers non-preemptively. As with other
cooperative multithreading systems, we treat context switches as function
calls, saving and restoring only the callee-saved state as specified in the
x86-64 ABI~\cite{amd64:abi:2012}. This involves saving six general-purpose
64-bit registers and the stack pointer, as well as the 16-bit x87 floating
point control word and the SSE context/status register. Thus, the minimum
amount of state a cooperative context switch routine must save, according to
the ABI, is 62~bytes.

Since \Grappa keeps a very large number of active workers, their context data
will not fit in cache. By oversubscribing on the number of workers
beyond what is required for latency tolerance, the scheduler can
ensure there is always some number of context pointers in the ready
queue. This allows the scheduler to prefetch contexts into
cache using software prefetch instructions. The reason this works is because the size of the L1 cache is
sufficient to hold enough contexts to tolerate the latency to main
memory. Empirically we find that prefetching the fourth worker in the scheduling order is
sufficient. This prefetching makes context switching effectively free of cache misses even to hundreds
of thousands of threads. We provide an analysis of our context switch
performance in Section~\ref{eval:basic}.


\paragraph{Work stealing} 
When the scheduler finds no work to assign to its workers, it commences to
steal tasks from other cores using an asynchronous \texttt{call\_on} active
message. It chooses a victim at random until it finds one with a non-zero
amount of work in its public task queue. The scheduler steals half of the
tasks it finds at the victim. Work stealing is particularly interesting in
\Grappa since performance depends on having many active worker threads on each
core. Even if there are many active threads, if they are all suspended on
long-latency operations, then the core is underutilized.

\subsection{Expressing Parallelism}

\TODO{addressing reviewer2: make it clear that yes Grappa is a low
    level programming model, but it provides the mechanisms for
    implementing a variety of execution models. Further, in some cases
    like the parallel loops, we have allowed for may parallelism to
    give flexibility to the implementation.}
\Grappa programmers focus on expressing as much parallelism as possible
without concern for where it will execute. \Grappa then chooses where and when
to exploit this parallelism, scheduling as much work as is necessary on each
core to keep it busy in the presence of system latencies and task dependences.

\Grappa provides four methods for expressing parallelism, shown in
Figure~\ref{fig:expressing-parallelism}. First, when the programmer identifies
work that can be done in parallel, the work may be wrapped up in a function
and queued with its arguments for later execution using a \texttt{spawn}.
Second,
the programmer can invoke a parallel for loop with \texttt{parallel\_for}, provided that the trip count is
known at loop entry. The programmer specifies a function pointer along with
start and end indices and an optional threshold to control parallel overhead.
\Grappa does {\em recursive decomposition} of iterations, similar to Cilk's
cilk\_for construct~\cite {cilkforimplementation}, and TBB's {\tt
parallel\_for}~\cite{intel_tbb}. It generates a logarithmically-deep tree of
tasks, stopping to execute the loop body when the number of iterations is
below the required threshold. Third, a programmer may want to execute an active message; that is, to run a
small piece of code on a particular core in the system without waiting for
execution resources to be available. Custom synchronization primitives, for example, execute this way as a function executed on the core where the data
lives. \Grappa provides the \texttt{call\_on} call for this purpose.

\begin{figure}[htbp]
  \begin{center}
	\begin{tabular}{l}
    \texttt{\scriptsize spawn( Functor f )} \\
      Creates a new stealable task \\
    \texttt{\scriptsize parallel\_for( start, end, Functor iteration )} \\
      Executes iterations of a loop as stealable tasks that \\
      take the iteration index as an argument  \\
    \texttt{\scriptsize call\_on( core, Functor f )} \\ 
      Runs a limited function on a specific core without \\
      consuming \Grappa execution resources 
	\end{tabular}
    \begin{minipage}{0.95\columnwidth}
      \caption{\label{fig:expressing-parallelism} \Grappa API: expressing parallelism
      } % \vspace{-4ex}}
    \end{minipage}
    %\vspace{-3ex}
  \end{center}
\end{figure}

Figure~\ref{fig:sample} shows sample code using \Grappa for a parallel tree
search. The important aspect to note is that the code looks very similar to
what would be written for single shared-memory system, without any concern about data locality or communication.

\begin{figure}[htbp]
\begin{center}
\begin{scriptsize}
\begin{verbatim}
class node_t {
  key_t   key
  int64_t numChildren;
  GlobalAddress<node_t *> children;
};

void search(GlobalAddress<node_t *> node, key_t key) {
  if (node->key == key) {
    result = node;
    return;
  }
  parallel_for(0, node->numChildren, [=](int index) {
    spawnTask([=]{
      search(node->children[index], key);
    });
  });
}
\end{verbatim}
\end{scriptsize}

    \begin{minipage}{0.95\columnwidth}
      \caption{\label{fig:sample} Sample \Grappa code illustrating a parallel tree search similar to the unbalanced tree search benchmark we describe later.}
    \end{minipage}

\end{center}
\end{figure}


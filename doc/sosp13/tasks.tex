\section{Tasking System}

Below we discuss the implementation of our task management support and then
describe how applications expose parallelism to the \Grappa runtime.

\subsection{Task Support Implementation}

The basic unit of execution in \Grappa is a {\em task}. Each hardware core has
a single operating system thread pinned to it. When tasks are ready to
execute, they are mapped to a {\em worker}, which is akin to a user-level
thread. % \Grappa keeps a large pool of workers per hardware core.

\paragraph{Tasks} Each task is represented by a 64-bit function pointer and
three 64-bit arguments. We use three arguments because tasks are often
generated via a parallel loop decomposition, in which each task needs three
kinds of data:

\begin{itemize}
\item {\it private argument}, often a loop index; 

\item {\it shared argument}, typically data shared amongst a group of
tasks, or the number of loop iterations to be performed by this task;

\item {\it synchronization argument}, often used to determine when all
tasks that are part of a loop have finished, in the form of a global pointer
to a synchronization object allocated at the core that spawned a group of
tasks.
\end{itemize}

While these are the most common uses of the three task arguments, they are
treated as arbitrary 64-bit values in the runtime, and can be used for any
purpose.

\paragraph{Workers} Workers execute application and system (e.g.,
communication) tasks. A worker is simply a collection of status bits and a
stack, allocated at a particular core. When a task is ready to be executed it
is assigned to a worker. Once a task is mapped to a worker it stays with that
worker until it completes.

\paragraph{Scheduling} During execution, a worker yields control of its core
whenever it performs a long-latency operation, allowing the processor to
remain busy while waiting for the operation to complete. In addition, a
programmer can direct scheduling explicitly via the \Grappa API calls shown in
Figure~\ref{fig:scheduling}. To minimize yield overhead, the \Grappa scheduler
operates entirely in user-space and does little more than store state of one
worker and load that of another.

\begin{figure}[htbp]
  \begin{center}
	\begin{tabular}{l}
    \texttt{\scriptsize yield() } \\
      Yields core to scheduler, enqueuing caller to be \\ scheduled again soon \\
    \texttt{\scriptsize suspend() }  \\
      Yields core to scheduler, enqueuing caller only once \\ another task calls wake \\
    \texttt{\scriptsize wake( task * $t$ ) } \\
      Enqueues some other task $t$ to be scheduled again soon \\
	\end{tabular}
    \begin{minipage}{0.95\columnwidth}
      \caption{\label{fig:scheduling} \Grappa scheduling API } 
    \end{minipage}
  \end{center}
\end{figure}

Each core in a \Grappa system has its own independent scheduler. The scheduler
has a single FIFO queue of active \emph{workers} ready to execute, the {\it
ready worker queue}. Each scheduler also has three queues of \emph{tasks}
waiting to be assigned a worker:

\begin{itemize}

\item {\it deadline task queue}, a priority queue of tasks that are executed according to task-specific deadline constraints;

\item {\it private task queue}, a FIFO queue of tasks that must run on
this core and therefore is not subject to stealing;

\item {\it public task queue},  a LIFO queue of tasks that are
  waiting to be matched with workers. It is a local partition of a shared
  task pool.

\end{itemize}


Whenever a task yields or suspends, the scheduler makes a decision about what
to do next. First, any task in the deadline task queue who's deadline
constraint is met is chosen for execution. This queue manages high priority
system tasks, such as periodically servicing communication requests. Second,
the scheduler determines if any workers with running tasks are ready to
execute; if so, one is scheduled. Finally, if there are no workers ready to
run, but there are tasks waiting to be matched with workers, an idle worker is
woken (or a new worker is spawned), matched with a task, and scheduled.

\paragraph{Context switching} 
\Grappa context switches between workers non-preemptively. As with other
cooperative multithreading systems, we treat context switches as function
calls, saving and restoring only the callee-saved state as specified in the
x86-64 ABI~\cite{amd64:abi:2012}. This involves saving six general-purpose
64-bit registers and the stack pointer, as well as the 16-bit x87 floating
point control word and the SSE context/status register. Thus, the minimum
amount of state a cooperative context switch routine must save, according to
the ABI, is 62~bytes.

Since \Grappa keeps a very large number of active workers, their context data
will not fit in cache. Therefore, the scheduler has to perform very careful
prefetching in order to reduce context switch cost to a minimum. We determined
empirically that prefetching the fourth worker in the scheduling order is
sufficient. We also prefetch three cache lines from the stack for each worker.
This makes context switching effectively free of cache misses even to hundreds
of thousands of threads. We provide an analysis of our context switch
performance in Section~\ref{eval:basic}.

\paragraph{Work stealing} 
When the scheduler finds no work to assign to its workers, it commences to
steal tasks from other cores using an asynchronous \texttt{call\_on} active
message. It chooses a victim at random until it finds one with a non-zero
amount of work in its public task queue. The scheduler steals half of the
tasks it finds at the victim. Work stealing is particularly interesting in
\Grappa since performance depends on having many active worker threads on each
core. Even if there are many active threads, if they are all suspended on
long-latency operations, then the core is underutilized.

\subsection{Expressing Parallelism}

\Grappa programmers focus on expressing as much parallelism as possible
without concern for where it will execute. \Grappa then chooses where and when
to exploit this parallelism, scheduling as much work as is necessary on each
core to keep it busy in the presence of system latencies and task dependences.

\Grappa provides four methods for expressing parallelism, shown in
Figure~\ref{fig:expressing-parallelism}. First, when the programmer identifies
work that can be done in parallel, the work may be wrapped up in a function
and queued with its arguments for later execution using a \texttt{spawn}.
Second, a programmer can use \texttt{spawn\_on} to spawn a task on a specific
core in the system or at the home core of a particular memory location. Third,
the programmer can invoke a parallel for loop with \texttt{parallel\_for}, provided that the trip count is
known at loop entry. The programmer specifies a function pointer along with
start and end indices and an optional threshold to control parallel overhead.
\Grappa does {\em recursive decomposition} of iterations, similar to Cilk's
cilk\_for construct~\cite {cilkforimplementation}, and TBB's {\tt
parallel\_for}~\cite{intel_tbb}. It generates a logarithmically-deep tree of
tasks, stopping to execute the loop body when the number of iterations is
below the required threshold. Fourth, a programmer may want to execute an active message; that is, to run a
small piece of code on a particular core in the system without waiting for
execution resources to be available. Custom synchronization primitives, for example, execute this way as a function executed on the core where the data
lives. \Grappa provides the \texttt{call\_on} call for this purpose.

\begin{figure}[htbp]
  \begin{center}
	\begin{tabular}{l}
    \texttt{\scriptsize spawn( Functor f )} \\
      Creates a new stealable task \\
    \texttt{\scriptsize parallel\_for( start, end, Functor iteration )} \\
      Executes iterations of a loop as stealable tasks that take the iteration index as an argument  \\
    \texttt{\scriptsize call\_on( core, Functor f )} \\ 
      Runs a limited function on a specific core without \\
      consuming \Grappa execution resources 
	\end{tabular}
    \begin{minipage}{0.95\columnwidth}
      \caption{\label{fig:expressing-parallelism} \Grappa API: expressing parallelism
      \TODO{update this with latest C++ code from ``api\_examples.cpp''}
      } % \vspace{-4ex}}
    \end{minipage}
    %\vspace{-3ex}
  \end{center}
\end{figure}


\begin{figure}[htbp]
\begin{scriptsize}
\begin{verbatim}
class node_t {
  key_t   key
  int64_t numChildren;
  GlobalAddress<node_t *> children;
};

void search(GlobalAddress<node_t *> node, key_t key) {
  if (node->key == key) {
    result = node;
    return;
  }

  parallel_for(0, node->numChildren, [=](int index) {
    spawnTask([=]{
      search(node->children[index], key);
    });
  });
}
\end{verbatim}
\end{scriptsize}
\end{figure}

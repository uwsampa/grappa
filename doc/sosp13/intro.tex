\section{Introduction} \label{sec:intro}

Irregular applications exhibit workloads, dependences, and memory accesses
that are highly sensitive to input. Classic examples of such applications
include branch and bound optimization, SPICE circuit simulation, and car crash
analysis. Important contemporary examples include processing large graphs in
the business, national security, and social network computing domains. For
these emerging applications, reasonable response time -- given the sheer
amount of data -- requires large multinode systems. The most broadly available
multinode systems are those built from x86 compute nodes interconnected via
ethernet or InfiniBand. However, scalable performance of irregular
applications on these systems is elusive for two reasons:

\vspace{0.5ex}
\noindent{\bf Poor data locality and frequent communication.} Data reference
patterns of irregular applications are unpredictable and tend to be spread
across the entire system. This results in frequent requests for remote data.
Caches are of little assistance because of low data re-use and spatial
locality. Prefetching is ineffective because request locations are not known
early enough. Data-parallel frameworks such as
MapReduce~\cite{mapreduce:osdi04} are also inappropriate because they rely on
being able to partition data and regular communication patterns. As a
consequence, irregular applications require frequent communication of
typically small pieces of data. In contrast, commodity networks are designed
for large packets and have just a fraction of their peak bandwidth when moving
small packets.

\vspace{0.5ex}
\noindent{\bf High network communication latency.} The performance challenges
of frequent communication is exacerbated by the network latency relative to
processor performance. Latency of commodity networks runs anywhere from one to
hundreds of microseconds, which is much more than superscalar execution with
sophisticated memory hierarchies can tolerate. Since irregular application
tasks encounter remote references dynamically during execution and must
resolve them before making further progress, stalls are frequent and lead
severely underutilized compute resources.

While some irregular applications can be restructured to better exploit
locality, aggregate requests to increase network message size, and manage the
additional challenges of load balance and synchronization, the effort required
to do so is formidable and involves knowledge and skills pertaining to
distributed systems far beyond those of most application programmers. Luckily,
many of the important irregular applications (e.g., graph processing, our
focus in this paper) naturally offer large amounts of concurrency. This
immediately suggests taking advantage of concurrency to tolerate the latency
of data movement by overlapping computation with communication.

The fully custom Tera MTA-2~\cite{tera:mta1} system is a classic example of
supporting irregular applications by using concurrency to hide latencies. It
had a large distributed shared memory with no caches. On every clock cycle,
each processor would execute a ready instruction chosen from one of its 128
hardware thread contexts, a sufficient number to fully hide memory access
latency. The network was designed with a single-word injection rate that
matched the processor clock frequency and sufficient bandwidth to sustain a
reference from every processor on every clock cycle. Unfortunately, the MTA
was not general enough nor cost-effective. The Cray XMT approximates the Tera
MTA-2, reducing its cost but not overcoming its narrow range of applicability.

We believe we can support irregular applications with good performance and
cost-effectiveness with commodity hardware for two main reasons. First,
commodity multicore processors have become extremely fast and cheap. So with
careful software engineering we can multiplex hundred of thousands of
workers into a single core. Second, commodity networks offer high
bandwidth as long as messages are large enough. So with enough data requests
in flight simultaneously, the system can aggregate small messages into large
enough ones.

In this paper we introduce \Grappa, a software runtime system that allows a
commodity cluster of x86-based nodes connected via an InfiniBand network to be
programmed as if it were a single, large, shared-memory NUMA (non-uniform
memory access) machine with scalable performance for irregular applications.
\Grappa is designed to smooth over some of the performance discontinuities in
commodity hardware, giving good performance when there is little locality to
be exploited while allowing the programmer to exploit it when it is available.

\Grappa leverages as much freely available and commodity infrastructure as
possible. We use unmodified Linux for the operating system and an
off-the-shelf user-mode InfiniBand device driver stack~\cite{OFED}. MPI is
used for process setup and tear down. GASNet~\cite{gasnet} is used as the
underlying mechanism for remote memory reads and writes using active message
invocations. To this commodity hardware and software mix \Grappa adds three
main software components: (1) a \emph{lightweight tasking\/} layer that
supports a context switch in as few as 38ns and distributed global load
balancing; (2) a \emph{distributed shared memory\/} layer that supports normal
access operations such as \emph{read\/} and \emph{write\/} as well as
synchronizing operations such as \emph{fetch-and-add\/}~\cite{fetchandadd};
and (3) a \emph{communication\/} layer that combines short messages to
achieve peak bandwidth on commodity networks. As we will show later, \Grappa
can tolerate latencies way beyond that of the network. Therefore, \Grappa can
afford to \emph{trade latency for throughput}. By {\em increasing\/} latency
in key components of the system we are able to: increase effective random
access memory bandwidth by delaying and aggregating messages; increase
synchronization rate by delegating atomic operations to gatekeeper cores, even
when referencing node-local global data; and improve load balance by
tolerating the delays incurred when work-stealing.

Our evaluation of Grappa shows that it runs several irregular application
kernels (e.g., graph traversal and sort) very efficiently on a commodity
cluster. Our yardstick for comparison is the XMT hardware itself. Using the
same number of network interfaces, \Grappa is in the same ballpark as the XMT:
For unbalanced tree search, Grappa is over \TODO{3X} faster and shows greatly
improved scalability; conversely, for breadth first search and betweenness
centrality Grappa is 2.5X slower. In Section~\ref{sec:evaluation} we explore
the factors that underpin this performance. Most importantly, however, for
significantly less real world cost, users can \emph{add\/} significantly more
processors to a commodity cluster than an XMT machine and use Grappa to
achieve scalable performance. \TODO{revise paragraph after eval, including MPI
and possibly programming model}



\section{Introduction} \label{sec:intro}

Irregular applications exhibit workloads, dependences, and memory accesses
that are highly sensitive to input. Classic examples of such applications
include branch and bound optimization, SPICE circuit simulation, and car crash
analysis. Important contemporary examples include processing large graphs in
the business, national security, and social network computing domains. For
these emerging applications, reasonable response time -- given the sheer
amount of data -- requires large multinode systems. The most broadly available
multinode systems are those built from x86 compute nodes interconnected via
ethernet or InfiniBand. However, scalable performance of irregular
applications on these systems is elusive for two reasons:

\vspace{0.5ex}
\noindent{\bf Poor data locality and frequent communication.} Data reference
patterns of irregular applications are unpredictable and tend to be spread
across the entire system. This results in frequent requests for remote data.
Caches are of little assistance because of low data re-use and spatial
locality. Prefetching is ineffective because request locations are not known
early enough. Data-parallel frameworks such as
MapReduce~\cite{mapreduce:osdi04} are also inappropriate because they rely on
being able to partition data and regular communication patterns. As a
consequence, irregular applications require frequent communication of
typically small pieces of data. In contrast, commodity networks are designed
for large packets and have just a fraction of their peak bandwidth when moving
small packets.

\vspace{0.5ex} \noindent{\bf High network communication latency.} The
performance challenges of frequent communication is exacerbated by the network
latency relative to processor performance. Latency of commodity networks runs
anywhere from a few to hundreds of microseconds, which is much more than
superscalar execution (even with sophisticated memory hierarchies) can
tolerate. Since irregular application tasks encounter remote references
dynamically during execution and must resolve them before making further
progress, stalls are frequent and lead to severely underutilized compute
resources.

While some irregular applications can be manually restructured to better
exploit locality, aggregate requests to increase network message size, and
manage the additional challenges of load balance and synchronization, the
effort required to do so is formidable and involves knowledge and skills
pertaining to distributed systems far beyond those of most application
programmers. Luckily, many of the important irregular applications (e.g.,
graph processing, our focus in this paper) naturally offer large amounts of
concurrency. This immediately suggests taking advantage of concurrency to
tolerate the latency of data movement by overlapping computation with
communication.

The fully custom Tera MTA-2~\cite{tera:mta1} system is a classic example of
supporting irregular applications by using concurrency to hide latencies. It
had a large distributed shared memory with no caches. On every clock cycle,
each processor would execute a ready instruction chosen from one of its 128
hardware thread contexts, a sufficient number to fully hide memory access
latency. The network was designed with a single-word injection rate that
matched the processor clock frequency and sufficient bandwidth to sustain a
reference from every processor on every clock cycle. Unfortunately, the MTA
was not general enough nor cost-effective. The Cray XMT approximates the Tera
MTA-2, reducing its cost but not overcoming its narrow range of applicability.

We believe we can support irregular applications with good performance and
cost-effectiveness with commodity hardware for two main reasons. First,
commodity multicore processors have become extremely fast and cheap. So with
careful software engineering we can multiplex hundred of thousands of
workers into a single core. Second, commodity networks offer high
bandwidth as long as messages are large enough. So with enough data requests
in flight simultaneously, the system can aggregate small messages into large
enough ones.

In this paper we introduce \Grappa, a software runtime system that allows a
commodity cluster of x86-based nodes connected via an InfiniBand network to be
programmed as if it were a single, large, shared-memory NUMA (non-uniform
memory access) machine with scalable performance for irregular applications.
\Grappa is designed to smooth over some of the performance discontinuities in
commodity hardware, giving good performance when there is little locality to
be exploited while allowing the programmer to exploit it when it is available.

\Grappa leverages as much freely available and commodity infrastructure as
possible. We use unmodified Linux for the operating system and an
off-the-shelf user-mode InfiniBand device driver stack~\cite{OFED}. MPI is
used for process setup and tear down. GASNet~\cite{gasnet} is used as the
underlying mechanism for remote memory reads and writes using active message
invocations. \Grappa adds three main software components: (1) a
\emph{lightweight tasking\/} layer that supports a context switch in as little
as 38ns and distributed global load balancing; (2) a \emph{distributed shared
memory\/} layer that supports normal access operations such as \emph{read\/}
and \emph{write\/} as well as synchronizing operations such as
\emph{fetch-and-add\/}~\cite{fetchandadd}; and (3) a \emph{communication\/}
layer that combines short messages to achieve peak bandwidth on commodity
networks. As we will show later, \Grappa can tolerate latencies way beyond
that of the network. Therefore, \Grappa can afford to \emph{trade latency for
throughput}. By {\em increasing\/} latency in key components of the system we
are able to: increase effective random access memory bandwidth by delaying and
aggregating messages; increase synchronization rate by delegating atomic
operations to gatekeeper cores, even when referencing node-local global data;
and improve load balance by tolerating the delays incurred when work-stealing.

Our evaluation of Grappa shows that it runs several irregular application
kernels (e.g., graph traversal and sort) very efficiently on a commodity
cluster. Our yardstick for comparison is the XMT hardware itself. Using the
same number of network interfaces, \Grappa is in the same ballpark as the XMT:
For unbalanced tree search, Grappa is over \TODO{3X} faster and shows greatly
improved scalability; conversely, for breadth first search and betweenness
centrality Grappa is 2.5X slower. In Section~\ref{sec:evaluation} we explore
the factors that underpin this performance. Most importantly, however, for
significantly less real world cost, users can \emph{add\/} significantly more
processors to a commodity cluster than an XMT machine and use Grappa to
achieve scalable performance. \TODO{revise paragraph after eval, including MPI
and possibly programming model}



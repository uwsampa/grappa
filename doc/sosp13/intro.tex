\section{Introduction} \label{sec:intro}

Irregular applications exhibit workloads, dependences, and memory accesses
that are highly sensitive to input. Classic examples of such applications
include branch and bound optimization, SPICE circuit simulation, contact
algorithms in car crash analysis, and network flow. Important contemporary
examples include processing large graphs in the business, national security,
machine learning, and social network computing domains. For these emerging
applications, fast response -- given the sheer amount of data -- requires
multinode systems. The most broadly available multinode systems are those
built from x86 compute nodes interconnected via ethernet or InfiniBand.
Scalable performance of irregular applications on these systems is elusive
for two reasons:

\noindent{\bf Low Injection Rate} Reference patterns of irregular
applications are unpredictable, with odds increasing as data is
spread out more broadly across a larger system that a given reference
will refer to memory located remotely.  This behavior contrasts
sharply with that of data-parallel applications for which efficient
frameworks such as MapReduce~\cite{mapreduce:osdi04} may exploit the
structure of computation and data affinity to limit internode
communication to less infrequent, larger messages.  Mass
market network technology is designed to transfer these larger blocks
of data, not the word-sized references emitted by irregular
application tasks. Injection rate into the network is insufficient to
utilize wire bandwidth when blocks are below about two-kilobytes, so
any naive communication strategy severely under-utilizes the network.

\noindent{\bf High Latency} To tolerate latency, data parallel applications make explicit
their regular, predictable communications so as to minimize interference with computation.
In contrast, irregular application tasks encounter remote references dynamically during
execution and must resolve them before making further progress.  Latency of
commodity networks runs anywhere from one to hundreds of microseconds,
far more than can be tolerated via the hyperthrading available in x86 processors.
Consequently, even with RDMA, processors' prefetching and caching hardware alone is inadequate
to prevent diminishing pipeline utilization as irregular applications are scaled up using
commodity networks.

While some irregular applications can be restructured to better exploit
locality, aggregate requests to increase message size, and manage the
additional challenges of load balance and synchronization across multinode
systems, the effort required to do so is formidable and involves knowledge and
skills pertaining to distributed systems far beyond those of most application
programmers. Luckily, many of the important irregular applications (e.g.,
graph processing, our focus in this paper) naturally offer large amounts of
concurrency. This immediately suggests taking advantage of concurrency to
tolerate the latency of data movement.

The fully custom Tera MTA-2~\cite{tera:mta1} system is a classic example of
supporting irregular applications by using concurrency to hide latencies. It
had a large distributed shared memory with no caches. On every clock cycle,
each processor would execute a ready instruction chosen from one of its 128
hardware thread contexts, a sufficient number to fully hide memory access
latency. The network was designed with a single-word injection rate that
matched the processor clock frequency and sufficient bandwidth to sustain a
reference from every processor on every clock cycle. Unfortunately, the MTA
was not general enough nor cost-effective. The Cray XMT approximates the Tera
MTA-2, substantially reducing its cost but not overcoming its narrow range of
applicability.

We believe we can support irregular applications with good performance and
cost-effectiveness with commodity hardware for two main reasons. First,
commodity multicore processors have become extremely fast and cheap. So with
careful software engineering we can multiplex hundred of thousands of
workers into a single core. Second, commodity networks offer high
bandwidth as long as messages are large enough. So with enough data requests
in flight simultaneously, the system can aggregate small messages into large
enough ones.

In this paper we introduce \Grappa, a software runtime system that allows a
commodity cluster of x86-based nodes connected via an InfiniBand network to be
programmed as if it were a single, large, shared-memory NUMA (non-uniform
memory access) machine with scalable performance for irregular applications.
\Grappa is designed to smooth over some of the performance discontinuities in
commodity hardware, giving good performance when there is little locality to
be exploited while allowing the programmer to exploit it when it is available.

\Grappa leverages as much freely available and commodity infrastructure as
possible. We use unmodified Linux for the operating system and an
off-the-shelf user-mode InfiniBand device driver stack~\cite{OFED}. MPI is
used for process setup and tear down. GASNet~\cite{gasnet} is used as the
underlying mechanism for remote memory reads and writes using active message
invocations. To this commodity hardware and software mix \Grappa adds three
main software components: (1) a \emph{lightweight tasking\/} layer that
supports a context switch in as few as 38ns and distributed global load
balancing; (2) a \emph{distributed shared memory\/} layer that supports normal
access operations such as \emph{read\/} and \emph{write\/} as well as
synchronizing operations such as \emph{fetch-and-add\/}~\cite{fetchandadd};
and (3) a \emph{communication\/} layer that combines short messages to
achieve peak bandwidth on commodity networks. As we will show later, \Grappa
can tolerate latencies way beyond that of the network. Therefore, \Grappa can
afford to \emph{trade latency for throughput}. By {\em increasing\/} latency
in key components of the system we are able to: increase effective random
access memory bandwidth by delaying and aggregating messages; increase
synchronization rate by delegating atomic operations to gatekeeper cores, even
when referencing node-local global data; and improve load balance by
tolerating the delays incurred when work-stealing.

Our evaluation of Grappa shows that it runs several irregular application
kernels (e.g., graph traversal and sort) very efficiently on a commodity
cluster. Our yardstick for comparison is the XMT hardware itself. Using the
same number of network interfaces, \Grappa is in the same ballpark as the XMT:
For unbalanced tree search, Grappa is over \TODO{3X} faster and shows greatly
improved scalability; conversely, for breadth first search and betweenness
centrality Grappa is 2.5X slower. In Section~\ref{sec:evaluation} we explore
the factors that underpin this performance. Most importantly, however, for
significantly less real world cost, users can \emph{add\/} significantly more
processors to a commodity cluster than an XMT machine and use Grappa to
achieve scalable performance. \TODO{revise paragraph after eval, including MPI
and possibly programming model}



\section{Introduction} \label{sec:intro}

Irregular applications exhibit workloads, dependences, and memory accesses
that are highly sensitive to input. Classic examples of such applications
include branch and bound optimization, SPICE circuit simulation, and car crash
analysis. Important contemporary examples include processing large graphs in
the business, national security, and social network computing domains. For
these emerging applications, reasonable response time -- given the sheer
amount of data -- requires large multinode systems. The most broadly available
multinode systems are those built from x86 compute nodes interconnected via
ethernet or InfiniBand. However, scalable performance of irregular
applications on these systems is elusive for two reasons:

\vspace{0.5ex}
\noindent{\bf Poor data locality and frequent communication.} Data reference patterns of irregular applications are unpredictable and tend to be spread across the entire system. This results in frequent requests for small pieces of remote data. Caches are of little assistance because of low data re-use and spatial locality. Prefetching is of limited value because request locations are not known early enough. Data-parallel frameworks such as MapReduce~\cite{mapreduce:osdi04} are ineffective because they rely on being able to partition data and regular communication patterns. Consequently, commodity networks, which are designed for large packets, achieve just a fraction of their peak bandwidth on small messages, starving application performance.

\vspace{0.5ex} \noindent{\bf High network communication latency.} The performance challenges of frequent communication is exacerbated by the network latency relative to processor performance. Latency of commodity networks runs anywhere from a few to hundreds of microseconds -- ten's of thousands of processor clock cycles.  Since irregular application tasks encounter remote references dynamically during execution and must resolve them before making further progress, stalls are frequent and lead to severely underutilized compute resources.

While some irregular applications can be manually restructured to better exploit locality, aggregate requests to increase network message size, and manage the additional challenges of load balance and synchronization, the effort required to do so is formidable and involves knowledge and skills pertaining to distributed systems far beyond those of most application programmers. Luckily, many of the important irregular applications naturally offer large amounts of concurrency. This immediately suggests taking advantage of concurrency to tolerate the latency of data movement by overlapping computation with communication.

The fully custom Tera MTA-2~\cite{tera:mta1} system is a classic example of
supporting irregular applications by using concurrency to hide latencies. It
had a large distributed shared memory with no caches. On every clock cycle,
each processor would execute a ready instruction chosen from one of its 128
hardware thread contexts, a sufficient number to fully hide memory access
latency. The network was designed with a single-word injection rate that
matched the processor clock frequency and sufficient bandwidth to sustain a
reference from every processor on every clock cycle. Unfortunately, the MTA
was not general enough nor cost-effective. The Cray XMT approximates the Tera
MTA-2, reducing its cost but not overcoming its narrow range of applicability.

We believe we can support irregular applications with good performance and cost-effectiveness with commodity hardware for two main reasons. First, commodity multicore processors have become extremely fast with high clock rates, large caches and robust DRAM bandwidth. Second, commodity networks offer high bandwidth as long as messages are large enough. We build on these two observations and develop \Grappa, a software runtime system that allows a commodity cluster of x86-based nodes connected via an InfiniBand network to be programmed as if it were a single, large, shared-memory NUMA (non-uniform memory access) machine with scalable performance for irregular applications. \Grappa exploits fast processors and the memory hierarchy to provide a lightweight user-level tasking layer that supports a context switch in as little as 38ns and can sustain a large number of active workers. It bridges the commodity network bandwidth gap with a communication layer that combines short messages originating from a large number of active workers into larger ones.

Our evaluation of \Grappa shows that its core components provide scalable performance.  It can multiplex thousands of workers on a multicore CPU and is limited only by DRAM bandwidth, not latency, to fetch worker state.  The communication layer provides over 10X the small-message performance compared to the raw underlying message injection rate.  Combining these two components, \Grappa achieves 1.2 GUPS with 64 system nodes, far above any reported cluster performance~\cite{gups} for comprable systems.  When we compare \Grappa to native MPI implementations of more complex application kernels, the story is more muddled.  When the MPI implementation is carefully tuned to aggregate messages inside the application -- essentially duplicating the work \Grappa is doing for the programmer, without the generality and context switch overhead -- then the MPI implementation is between 4-10X faster.  We see this in our comparisons for breadth first search and integer sorting.  On the other hand, if the MPI version is implemented in a straightforward manner, as we see with PageRank, GUPS, and unbalanced tree search, \Grappa is 2-5X faster.  When we compare \Grappa to the custom Cray XMT, we find Grappa to be between 2X faster and 4X slower depending on the benchmark.  Overall we find this encouraging because \Grappa is scalable and achieves this performance from a mass-market cluster, hence adding more system nodes an inexpensive and attractive option.
 
\comment{
Our evaluation of \Grappa shows that it runs several irregular application
kernels (e.g., graph traversal and sort) very efficiently on a commodity
cluster. Our yardstick for comparison is the XMT hardware itself. Using the
same number of network interfaces, \Grappa is in the same ballpark as the XMT:
For unbalanced tree search, \Grappa is over \TODO{3X} faster and shows greatly
improved scalability; conversely, for breadth first search and betweenness
centrality \Grappa is 2.5X slower. In Section~\ref{sec:evaluation} we explore
the factors that underpin this performance. Most importantly, however, for
significantly less real world cost, users can \emph{add\/} significantly more
processors to a commodity cluster than an XMT machine and use \Grappa to
achieve scalable performance. \TODO{revise paragraph after eval, including MPI
and possibly programming model}
}

%%% bit bucket
%\Grappa is designed to smooth over some of the performance discontinuities in commodity hardware, giving good performance when there is little locality to be exploited while allowing the programmer to exploit it when it is available.



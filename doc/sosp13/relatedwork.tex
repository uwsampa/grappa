\section{Background}


% Grappa is built on many existing ideas in programming languages, systems and
% architecture. In this section we discuss related frameworks and key enabling
% technologies that Grappa builds upon.

\paragraph{Distributed graph processing} While \Grappa is a general runtime
system for any large-scale concurrent application, one of its killer
applications is graph analysis. Therefore, distributed graph processing
frameworks like Pregel~\cite{pregel:2010} and Distributed
GraphLab~\cite{distgraphlab:vldb12} are related. Pregel adopts a
bulk-synchronous parallel (BSP) execution model, which makes it inefficient on
workloads that could prioritize vertices. GraphLab, on the other hand,
schedules vertex computations individually, allowing prioritization, which
gives faster convergence in a variety of iterative algorithms. GraphLab,
however, imposes a rigid computation model where programmers must express
computation as transformations on a vertex and its edge list only, with
information only from adjacent vertexes. Pregel is only slightly less
restrictive, as the input data can be any vertex in the graph. Grappa also
supports dynamic parallelism with asynchronous execution, but parallelism is
expressed as tasks or loop iterations, which is a far more general programming
model for irregular computation tasks.

\paragraph{Global memory} Grappa supports a global memory namespace, presenting a programming abstraction similar to that of previous software distributed shared memory (DSM) systems but with differences attuned to irregular application demands.

Many traditional software DSM systems are page based~\cite{Treadmarks,munin}
and aim to hide the fact that they are built in software from applications by
exploiting the processor's paging mechanisms, therefore relying heavily on
locality. Instead, Grappa, like partitioned global address space (PGAS)
models, implements its DSM at the language, rather than the system level.
Languages such as Chapel~\cite{Chamberlain:2007}, X10~\cite{X10:2005}, and
UPC~\cite{upc:2005} make accesses to shared structures look like normal
references. As we describe later, Grappa chooses a middle ground, where global
addresses are explicit in the API and local accesses are emitted
conventionally by the compiler. Also, since past DSM work was mostly
page-based, it could exploit network's RDMA support to move large page-sized
blocks of data from node to node. However, when commodity networks move small
blocks of data (a few bytes), only a fraction of the available bandwidth is
achieved. 

In summary, while Grappa's DSM system is conceptually similar to prior work,
it builds latency tolerance from the ground up (message aggregation, constant
overlap of long latency memory operations with computation, etc.), so it can
better support irregular computations with little locality.

% Importantly, global memory support in Grappa
% ends up being tightly coupled to the task scheduler in order to overlap long
% latency memory operations with useful computation. For these two reasons it
% became necessary to build a new DSM system specifically for Grappa.


%% We could be stronger here on why Grappa's task library is different,
%% but we are running out of time.  -Mark

\paragraph{Multithreading} Grappa uses multithreading to tolerate memory
latency. This is a well known technique. Hardware implementations include the
Tera MTA~\cite{tera:mta1}, Cray XMT~\cite{feo:xmt}, Simultaneous
multithreading~\cite{tullsen:smt}, MIT Alewife~\cite{agarwal:alewife},
Cyclops~\cite{almasi:cyclops}, and even GPUs~\cite{gpus}. As we describe in
this paper, Grappa implements its own software-based multithreading with a
lightweight user-mode task scheduler to multiplex \emph{thousands\/} of tasks
on a single processing core. The large number of tasks is required because of
the to tolerate very high inter-node communication latency. Grappa's task
library employs several optimizations: an extremely fast task switch, a small
task size, and judicious use of hardware prefetching to bring task state into
the cache long before that task is actually scheduled. The main difference between Grappa's support for lightweight threads and prior work such as QThread~\cite{Wheeler08qthreads:an} and Capriccio~\cite{Behren03capriccio:scalable} is the pervasive use of prefetching (with a large number of active tasks, not even their contexts fit in cache!).


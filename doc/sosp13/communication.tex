\section{Communication Support}
\label{sec:communication}

\Grappa's communication support has two layers: user-level messaging interface
based on active messages; and network-level transport that supports request
aggregation for better communication bandwidth.

% In order to mitigate the low message injection rate limits of commodity
% networks, Grappa's communication stack has two layers:

\paragraph{Active messages interface} At the upper (user-level) layer, \Grappa
implements asynchronous active messages~\cite{vonEicken92}. Each message
consists of a function pointer, an optional argument payload, and an optional
data payload. 

% When a task sends a message, the message is linked to a send queue
% associated with the message's destination node and the task continues
% execution. Note that the message contents stay in the worker's stack until
% it is sent to the network interface buffer, to avoid multiple copies of the
% payload. 

\paragraph{Message aggregation} In our experiments the vast majority of upper
layer message requests are smaller than 44 bytes. Our measurements confirm
manufacturers' published data [15]; with 44-byte packets, the available
bisection bandwidth is only a small fraction (3\%) of the peak bisection
bandwidth. As mentioned earlier, commodity networks including InfiniBand
achieves their peak bisection bandwidth only when the packet sizes are
relatively large --- on the order of multiple kilobytes. The reason for this
discrepancy is the combination of overheads associated with handling each
packet (in terms of bytes that form the actual packet, processing time at the
card and processing on the CPU within the driver stack). Consequently, to make
the best use of the network, we must convert small messages into large ones.

\paragraph{Message processing mechanics} Since communication is very frequent
in \Grappa, aggregating and sending messages efficiently is very important. To
achieve that, \Grappa makes careful use of caches, prefetching and lock-free
synchronization operations. \TODO{Does the explanation below need a figure?}

Outgoing messages are organized as an array of linked lists, with one linked
list per destination node in the system. Each core in a given node is
responsible for aggregating and sending the resulting message to a set of
destination nodes. The outgoing message lists are located in a region of
memory shared across all cores in a \Grappa node (thus enabling cores to peak
at each other's message lists). When a task sends a message, it allocates a buffer (typically on its stack),  determines the destination system
node, and links the buffer into the corresponding linked list.

Each core has a system worker per destination node it is responsible for; this
system worker periodically checks whether messages to the corresponding
destination node is either large enough or has waited past a time-out period,
at which point the message is sent out. Aggregating and sending a message
involves manipulating a shared data-structure (the message list). This is done
using CAS (compare-and-swap) operations to avoid high synchronization costs. 

% Threads that dispatch a message allocate a message buffer (typically on their
% stack) and dispatch a send request. \Grappa examines the destination system
% node for this field, and adds the message to a linked list of messages
% destined for that node. Periodically, or when this linked list grows large,
% the list is transferred using a CAS (compare-and-swap} operation to the
% processor on the originating system node that is responsible for managing all
% outbound communication requests to the specified destination node. This
% two-step process is used because it was determined empirically that we could
% only infrequently use processor coherent access requests because they remain a
% bottleneck on modern multicore hardware.

Each node has a region of memory with send buffers where the final aggregated
messages are built. These buffers are visible to the network card, and
messages can be sent with user-mode operations only. When the worker
responsible for outbound messages to a given system node has received a
sufficient number of message send requests, or a timeout is reached, the
linked list of messages is walked and messages are copied to a send buffer.
This process requires careful prefetching because most of the outbound
messages are \emph{not} in the processor cache at this time (recall that a
core can be aggregating messages originating from other cores in the same
node). Once the send buffer has been formed it is handed off to GASnet for
transfer to the remote system node. RDMA is used if the underlying network
supports it. 

There are two useful consequences of forming the send buffer at the time of
message transmission instead of along the way, as individual upper layer
message send requests are received. First, as previously mentioned most of the
messages are not in the cache and prefetching is used to run-ahead in the
linked list of messages in order to avoid cache misses. But once the send
buffer is formed it is in the cache (for the most part). Hence, when it is
handed off to GASnet for transfer across the physical wire, the network card
can pull the message buffer from the processor cache instead of main memory,
which we have found speeds performance. The second consequence of this
decision is that we do not need to pre-allocate buffers for all destination
nodes in the system, as the buffer can be allocated on the fly. Nevertheless
we have found it efficient to build a flow-control like protocol of
outstanding message buffers between pairs of system nodes.

Once the remote system node has received the message buffer it is unpacked.
Upper level \Grappa messages are delivered to each core at the destination
system node by examining the message buffer at each processor core and then
handing the message buffer to the next core for delivery of messages destined
to that core.



%There are three situations in which a queue of aggregated messages is sent:
%(1) Each queue has a message size threshold of 4096 bytes, chosen to give
%reasonable network performance. If the size in bytes of a queue is above the
%threshold, the contents of the queue are sent immediately. (2) Each queue
%has a wait time threshold ($\approx${1ms}). If the oldest message in a queue
%has been waiting longer than this threshold, the contents of the queue are
%sent immediately, even if the queue size is lower than the message size
%threshold. (3) Queues may be explicitly flushed in situations where the
%programmer wants to minimize the latency of a message at the cost of bandwidth
%utilization.

%The network layer is serviced by polling. Periodically when a context
%switch occurs, the Grappa scheduler switches to the network polling
%thread. This thread has three responsibilities. First, it polls the
%lower-level network layer to ensure it makes progress. Second, it
%deaggregates received messages and executes active message
%handlers. Third, it checks to see if any aggregation queues have
%messages that have been waiting longer than the threshold; if so, it
%sends them.

%Underneath the aggregation layer, Grappa uses the \gasnet~communication
%library~\cite{gasnet} to actually move data. All interprocess
%communication, whether on or off a cluster node, is handled by the
%\gasnet~library. \gasnet~is able to take advantage of many communication
%mechanisms, including ethernet and infiniband between nodes, as well as
%shared memory within a node.

%Some networks provide access to a remote machine's memory directly. This
%would seem to be a good fit for a programming model focused on global
%shared memory, but in fact we do not use it. In our experiments, we
%found that RDMA operations are subject to the same message rate
%limitations as all other messages on these cards, and thus using raw
%RDMA operations for our small messages would make inefficient use of
%bandwidth. Instead, we implement remote memory operations with active
%messages. A byproduct of this design decision is that Grappa is not
%limited to RDMA-capable networks.

%\TODO{We should expand on the zero-copy aggregation stuff?}

% there is a message pointer list in each core. this list has as many entries
% as the number of cores in the system. each entry is another list of message
% with that destination.


% each core in a node is responsible for a set of nodes in the system

% cores in a node share a region of memory

% network buffers also reside in this shared region of memory

% messages ptrs also contain prefetching information to speed up assembling message in the buffer.

% there are as many sending workers in a core as the number of nodes it is responsible for.





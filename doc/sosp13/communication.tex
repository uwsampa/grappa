\section{Communication Support}
\label{sec:communication}

In order to mitigate the low message injection rate limits of commodity networks, Grappa's communication stack has two layers: one for user-level messages and one for network-level messages.

At the upper (user-level) layer, Grappa implements asynchronous active messages~\cite{vonEicken92}. Each message consists of a function pointer, an optional argument payload, and an optional data payload. When a task sends a message, the message is copied to a send queue associated with the message's destination and the task continues execution.  In our experiments the vast majority of upper layer message requests are smaller than 44 bytes. Our measurements confirm manufacturers' published data [15]; with 44-byte packets, the available bisection bandwidth is only a small fraction (3\%) of the peak bisection bandwidth.

To make the best use of the network, we must convert our small messages into large ones, and this is the responsibility of Grappaâ€™s lower networking layer. As mentioned earlier, commodity networks including InfiniBand achieves their peak bisection bandwidth only when the packet sizes are relatively large --- on the order of multiple kilobytes. The reason for this discrepancy is the combination of overheads associated with handling each packet (in terms of bytes that form the actual packet, processing time at the card and processing on the CPU within the driver stack).

Grappa's lower network layer makes careful use of processor cache and coherent access behaviors.  Threads that dispatch a message allocate a message buffer (typically on their stack) and dispatch a send request.  Grappa examines the destination system node for this field, and adds the message to a linked list of messages destined for that node.  Periodically, or when this linked list grows large, the list is transfered using a coherent compare-and-swap operation to the processor on the originating system node that is responsible for managing all outbound communication requests to the specified destination node.  This two-step process is used because it was determined empirically that we could only infrequently use processor coherent access requests because they remain a bottleneck on modern multicore hardware.

Once the processor core responsible for outbound messages to a given system node has received a sufficient number of message send requests, or a timeout is reached, the linked list of messages is walked to form a send buffer.  This involves making a copy of the messages in memory.  Careful prefetching is used because most of the outbound messages are \emph{not} in the processor cache at this time.  Once the send buffer has been formed it is handed off to GASnet for transfer to the remote system node.  RDMA is used if the underlying network supports it.  There are two useful consequences of forming the message buffer at the time of message dispatch instead of along the way, as individual upper layer message send requests are received.  First, as previously mentioned most of the messages are not in the cache and prefetching is used to run-ahead in the linked list of messages in order to avoid cache misses.  But once the message buffer is formed it is in the cache (for the most part).  Hence, when it is handed off to GASnet for transfer across the physical wire, the network card can pull the message buffer from the processor cache instead of main memory, which we have found speeds performance.  The second consequence of this decision we do not need to pre-allocate buffers for all destination nodes in the system, as the buffer can be allocated on the fly.  Nevertheless we have found it efficient to build a flow-control like protocol of outstanding message buffers between pairs of system nodes.

Once the remote system node has received the message buffer it is unpacked.  Upper level Grappa messages are delivered to each processor at the system node by examining the message buffer at each processor core and then handing the message buffer to the next processor for delivery of messages destined to that processor.



%There are three situations in which a queue of aggregated messages is sent:
%(1) Each queue has a message size threshold of 4096 bytes, chosen to give
%reasonable network performance. If the size in bytes of a queue is above the
%threshold, the contents of the queue are sent immediately. (2) Each queue
%has a wait time threshold ($\approx${1ms}). If the oldest message in a queue
%has been waiting longer than this threshold, the contents of the queue are
%sent immediately, even if the queue size is lower than the message size
%threshold. (3) Queues may be explicitly flushed in situations where the
%programmer wants to minimize the latency of a message at the cost of bandwidth
%utilization.

%The network layer is serviced by polling. Periodically when a context
%switch occurs, the Grappa scheduler switches to the network polling
%thread. This thread has three responsibilities. First, it polls the
%lower-level network layer to ensure it makes progress. Second, it
%deaggregates received messages and executes active message
%handlers. Third, it checks to see if any aggregation queues have
%messages that have been waiting longer than the threshold; if so, it
%sends them.

%Underneath the aggregation layer, Grappa uses the \gasnet~communication
%library~\cite{gasnet} to actually move data. All interprocess
%communication, whether on or off a cluster node, is handled by the
%\gasnet~library. \gasnet~is able to take advantage of many communication
%mechanisms, including ethernet and infiniband between nodes, as well as
%shared memory within a node.

%Some networks provide access to a remote machine's memory directly. This
%would seem to be a good fit for a programming model focused on global
%shared memory, but in fact we do not use it. In our experiments, we
%found that RDMA operations are subject to the same message rate
%limitations as all other messages on these cards, and thus using raw
%RDMA operations for our small messages would make inefficient use of
%bandwidth. Instead, we implement remote memory operations with active
%messages. A byproduct of this design decision is that Grappa is not
%limited to RDMA-capable networks.

%\TODO{We should expand on the zero-copy aggregation stuff?}

% there is a message pointer list in each core. this list has as many entries
% as the number of cores in the system. each entry is another list of message
% with that destination.


% each core in a node is responsible for a set of nodes in the system

% cores in a node share a region of memory

% network buffers also reside in this shared region of memory

% messages ptrs also contain prefetching information to speed up assembling message in the buffer.

% there are as many sending workers in a core as the number of nodes it is responsible for.





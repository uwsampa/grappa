\section{Communication Support}
\label{sec:communication}

\Grappa's communication support has two layers: user-level messaging interface
based on active messages; and network-level transport that supports request
aggregation for better communication bandwidth.

% In order to mitigate the low message injection rate limits of commodity
% networks, Grappa's communication stack has two layers:

\paragraph{Active messages interface} At the upper (user-level) layer, \Grappa
implements asynchronous active messages~\cite{vonEicken92}. Each message
consists of a function pointer, an optional argument payload, and an optional
data payload. 

% When a task sends a message, the message is linked to a send queue
% associated with the message's destination node and the task continues
% execution. Note that the message contents stay in the worker's stack until
% it is sent to the network interface buffer, to avoid multiple copies of the
% payload. 

\paragraph{Message aggregation} In our experiments the vast majority of upper
layer message requests are smaller than 44 bytes. Our measurements confirm
manufacturers' published data [15]; with 44-byte packets, the available
bisection bandwidth is only a small fraction (3\%) of the peak bisection
bandwidth. As mentioned earlier, commodity networks including InfiniBand
achieves their peak bisection bandwidth only when the packet sizes are
relatively large --- on the order of multiple kilobytes. The reason for this
discrepancy is the combination of overheads associated with handling each
packet (in terms of bytes that form the actual packet, processing time at the
card, multiple roundtrips on the PCI Express bus and processing on the CPU
within the driver stack). Consequently, to make the best use of the network,
we must convert small messages into large ones.

\paragraph{Message processing mechanics} Since communication is very frequent
in \Grappa, aggregating and sending messages efficiently is very important. To
achieve that, \Grappa makes careful use of caches, prefetching and lock-free
synchronization operations. \TODO{Does the explanation below need a figure?}

Outgoing messages are organized as an array of linked lists, with one linked
list per destination node in the system. Each core in a given node is
responsible for aggregating and sending the resulting messages to a set of
destination nodes. The outgoing message lists are located in a region of
memory shared across all cores in a \Grappa node (thus enabling cores to peek
at each other's message lists). When a task sends a message, it allocates a buffer (typically on its stack),  determines the destination system
node, and links the buffer into the corresponding linked list.

Each core has a system worker per destination node it is responsible for; this
system worker periodically checks whether messages to the corresponding
destination node is either large enough or has waited past a time-out period,
at which point the message is sent out. Aggregating and sending a message
involves manipulating a shared data-structure (the message list). This is done
using CAS (compare-and-swap) operations to avoid high synchronization costs. 

% Threads that dispatch a message allocate a message buffer (typically on their
% stack) and dispatch a send request. \Grappa examines the destination system
% node for this field, and adds the message to a linked list of messages
% destined for that node. Periodically, or when this linked list grows large,
% the list is transferred using a CAS (compare-and-swap} operation to the
% processor on the originating system node that is responsible for managing all
% outbound communication requests to the specified destination node. This
% two-step process is used because it was determined empirically that we could
% only infrequently use processor coherent access requests because they remain a
% bottleneck on modern multicore hardware.

Each node has a region of memory with send buffers where the final aggregated
messages are built. These buffers are visible to the network card, and
messages can be sent with user-mode operations only. When the worker
responsible for outbound messages to a given system node has received a
sufficient number of message send requests, or a timeout is reached, the
linked list of messages is walked and messages are copied to a send buffer.
This process requires careful prefetching because most of the outbound
messages are \emph{not} in the processor cache at this time (recall that a
core can be aggregating messages originating from other cores in the same
node). Once the send buffer has been formed it is handed off to GASNet for
transfer to the remote system node. RDMA is used if the underlying network
supports it. 

There are two useful consequences of forming the send buffer at the time of
message transmission instead of along the way, as individual upper layer
message send requests are received. First, as previously mentioned most of the
messages are not in the cache and prefetching is used to run-ahead in the
linked list of messages in order to avoid cache misses. But once the send
buffer is formed it is in the cache (for the most part). Hence, when it is
handed off to GASNet for transfer across the physical wire, the network card
can pull the message buffer from the processor cache instead of main memory,
which we have found speeds performance. The second consequence of this
decision is that we do not need to pre-allocate buffers for all destination
nodes in the system, as the buffer can be allocated on the fly. Nevertheless
we have found it efficient to build a flow-control like protocol of
outstanding message buffers between pairs of system nodes.

Once the remote system node has received the message buffer, it is unpacked
and put in the corresponding incoming message lists. This process is done in
parallel by all system worker responsible for communication. Upper level
\Grappa messages are delivered to each core at the destination system node by
examining the message buffer at each processor core and then handing the
message buffer to the next core for delivery of messages destined to that
core.


%%%% Comments from Jacob. 

% Last paragraph:

% When a buffer is received, all cores deaggregate their messages in parallel.
% This is how it works:
%
%	- the sending core does an RDMA put into a buffer owned by the receiving
%	core. an active message is sent to enqueue this buffer to be processed by a
%	receive worker.
%
%	- the receive worker computes the offsets/lengths of the messages in the
%	buffer for each core on the node. It sends messages through the CAS lists
%	to each core with their offset and length.
%
%	- The cores all deserialize and execute their received active messages.
%	This is done in parallel. Since messages to the same core are stored next
%	to each other in the buffer, this deserialization performs well with the
%	cache and the hardware prefetcher.
%
%  - When the cores are done, they send a message back to the receive worker.
%
%	- Once the receive worker knows all the cores on the node are done with the
%	buffer, it's done. (we then return the buffer pointer to the sending node
%	so it can send again.)
% 
% We should make sure we cite the Threaded Abstract Machine paper too.
% 
% If we haven't already, we should cite Myrinet, U-Net, EMP, and I think
% there's one more. Here are some links:
% 
% http://dl.acm.org/citation.cfm?id=623898&CFID=128100904&CFTOKEN=36373202
% http://dl.acm.org/citation.cfm?id=224061&CFID=128100904&CFTOKEN=36373202
% http://dl.acm.org/citation.cfm?id=582091&CFID=128100904&CFTOKEN=36373202

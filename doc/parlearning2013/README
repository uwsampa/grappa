Source of data suggested by Yucheng:
http://www.kddcup2012.org/c/kddcup2012-track2

Parallelized Stochastic Gradient Descent paper:
http://www.research.rutgers.edu/~lihong/pub/Zinkevich11Parallelized.pdf

Yucheng's claim (validated by the paper above) is that
state-of-the-art implementations of parallel stochastic gradient rely
on distributed calculation of weight vectors that are averaged
periodically into one weight vector and redistributed.  That is, each
processor maintains its own weight vector, updating it based on a
subset of the training data; from time to time, all processors combine
their partial results; the process repeats until the weight vector
converges.

The research question is whether or not grappa can accelerate
convergence of parallel stocahstic gradient by virtue of higher
throughput updates to a shared vector of weights.

Prerequisite to answering this question affirmatively is to determine
that increasing the rate at which the distributed weights are combined
does in fact increase the rate of convergence as a function of the
amount of data processed -- even on a sequential system.  To determine
this, we can maintain P copies of the weight vector, initialized to
zero, updating each once for each datum examined during the learning
process, in round-robin fashion.  After every K updates, we replace
every weight vector with the average.  We then study the convergence
rate as a function of K.  If the rate is sensitive to K, dropping
significantly as K increases, then we have evidence a parallel
implementation might benefit from more frequent communication amongst
the processors.

Given this evidence, we would then implement parallel stochastic
gradient in grappa.  There are many ways to do this...

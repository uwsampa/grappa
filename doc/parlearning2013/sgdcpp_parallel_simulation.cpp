#include <vector>
#include <iostream>
#include <stdint.h>
#include <cmath>
#include <cassert>
#include <cstdlib>

int num_parallel = 8;
int iters_between_ag = 2;
std::vector<std::vector<double> > all_weights;
std::vector<std::vector<size_t> > all_firsttime;
size_t timestep;
double regularization; // lambda
double stepsize; // eta

struct feature {
  size_t id;
  double value;
  feature(size_t id, double value): id(id), value(value) { }
};

/**
 * Takes a logistic gradient step using the datapoint (x,y)
 * changes the global variable weights, firsttime and timestep
 * Also returns the predicted value for the datapoint
 */
double logistic_sgd_step(const std::vector<feature>& x, double y, std::vector<double> &weights, std::vector<size_t> &firsttime) {
  // compute predicted value of y
  // remember to factor in regularization
  double linear_predictor = 0;
   
  // since we are doing lazy regularization, we need to compute
  for (size_t i = 0; i < x.size(); ++i) {
    // get the age of the weight  
    double age;
    size_t weight_first_access = firsttime[i];
    if (weight_first_access < timestep) {
      age = timestep - weight_first_access - 1;
    } else {
      age = 0;
      firsttime[i] = std::min(firsttime[i], timestep); // atomic
    }
    // compute the actual regularized weight based on the age of the  
    // parameter: essentially scaling it by (1 - eta * lambda)^(age)
    double regularized_weight = weights[x[i].id]  
        * std::pow(1.0 - stepsize * regularization, age);

    linear_predictor += x[i].value * regularized_weight;
  }  

  // probability that y is 0
  double py0 = 1.0 / (1 + std::exp(linear_predictor));
  double py1 = 1.0 - py0;
  // note that there is a change that we get NaNs here. If we get NaNs,
  // push down the step size or push up the regularization
   
  // ok compute the gradient
  // the gradient update is easy
  for (size_t i = 0; i < x.size(); ++i) {
    weights[x[i].id] += stepsize / sqrt(1.0 + timestep) * (double(y) - py1) * x[i].value; // atomic
  }
  timestep++; // atomic
  return py1;
}



/**
 * Generates a simple synthetic binary classification dataset in X,Y with numdata  
 * datapointsand where each datapoint has numweights weights and average  
 * sparsity "sparsity".
 * Every weight value will be between -1 and 1.
 * Also returns the true weight vector used to generate the data.
 * (the dataset generated by this procedure is actually quite hard to learn)
 */
static double rand_m1_p1() { //return unif random in [-1.0,1.0]
  return (double(std::rand()) / RAND_MAX)*2.0 - 1.0;
}
std::vector<double> generate_dataset(std::vector<std::vector<feature> >& X,
                                     std::vector<double>& Y,
                                     int numweights,
                                     int numdata,
                                     double sparsity) {
  assert(0 < sparsity  && sparsity <= 1.0);
  // generate a random small weight vector between -1 and 1
  std::vector<double> w(numweights, 0);
  for (size_t i = 0; i < numweights; ++i) {
    w[i] = rand_m1_p1();
  }

  for (size_t i = 0; i < numdata; ++i) {
    std::vector<feature> x;   
    // use logistic regression to predict a y
    double linear_predictor = 0;
    // generate a random 25% sparse datapoint
    for (size_t j = 0; j < numweights; ++j) {
      // with 25T probability generate a weight
      if (std::rand() < sparsity * RAND_MAX) {
        x.push_back(feature(j, rand_m1_p1()));
        linear_predictor += x.rbegin()->value * w[i];
      }
    }
    double py0 = 1.0 / (1 + std::exp(linear_predictor));
    double py1 = 1.0 - py0;
    // generate a 0/1Y value.  
    double yval = (std::rand() < RAND_MAX * py0) ? 0 : 1;
    // to get regression, comment the line above and uncomment the line below
    // yval = py1;

    X.push_back(x);
    Y.push_back(yval);
  }
  return w;
}

int main(int argc, char** argv) {
  if (argc > 1) iters_between_ag = atoi(argv[1]);
  if (argc > 2) num_parallel = atoi(argv[2]);
  timestep = 0;
  regularization = 0.0;//1E-5;
  stepsize = 0.5;
  size_t numweights = 1000;
  size_t is_between_ags = num_parallel;
  is_between_ags *= iters_between_ag;
  int k;
  int j;

  std::vector<size_t> firsttime;
  firsttime.resize(numweights, (size_t)(-1));
  for(j = 0; j < num_parallel; ++j){
    all_firsttime.push_back(firsttime);
  }

  // generate a little test dataset
  std::vector<std::vector<feature> > X;
  std::vector<double> Y;
  std::vector<double> true_weight = generate_dataset(X, Y, numweights, 10000, 0.2);

  std::vector<double> w;
  w.resize(numweights, 0.0);
  for(j = 0; j < num_parallel; ++j){
    all_weights.push_back(w);
  }

  // we output both L2 and 0/1 loss here.
  // but note that L2 loss is not meaningful for binary classification
  // and 0/1 loss is not meaningful for regression
  // The dataset generator above generates for binary classification,
  // but can also be modified to generate regression. See the comment
  // near the bottom of generate_dataset
  for (size_t iterations = 0; iterations < 100; ++iterations) {

    size_t loss01 = 0;
    double lossl2 = 0.0;
    for (size_t i = 0 ; i < X.size(); ++i) {

      double ret = logistic_sgd_step(X[i], Y[i], all_weights[i%num_parallel], all_firsttime[i%num_parallel]);
      lossl2 += (Y[i] - ret) * (Y[i] - ret);
      loss01 += (Y[i] != (ret >= 0.5));

      size_t m = iterations*X.size() + i;
      if(m%(is_between_ags) == (is_between_ags - 1)){
        for(k = 0; k < num_parallel; ++k){
          for(j = 0; j < (int) numweights; ++j){
            if(k == 0)
              w[j] = 0.0;
            w[j] += (all_weights[k])[j];
            if(k == (num_parallel - 1))
              w[j] /= num_parallel;
          }
        }
        all_weights.clear();
        for(k = 0; k < num_parallel; ++k){
          all_weights.push_back(w);
        }
      }
    }

    std::cout << "Iteration " << iterations << "\n";
    std::cout << "\tL2 Loss: " << std::sqrt(lossl2) << "\n";
    std::cout << "\t0/1 Loss: " << loss01 << "\n";
  }
}

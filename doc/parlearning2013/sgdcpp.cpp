#include <vector>
#include <iostream>
#include <stdint.h>
#include <cmath>
#include <cassert>
#include <cstdlib>

std::vector<double> weights;
std::vector<size_t> firsttime;
size_t timestep;
double regularization; // lambda
double stepsize; // eta

struct feature {
  size_t id;
  double value;
  feature(size_t id, double value): id(id), value(value) { }
};

/**
 * Takes a logistic gradient step using the datapoint (x,y)
 * changes the global variable weights, firsttime and timestep
 * Also returns the predicted value for the datapoint
 */
double logistic_sgd_step(const std::vector<feature>& x, double y) {
  // compute predicted value of y
  // remember to factor in regularization
  double linear_predictor = 0;
  
  // since we are doing lazy regularization, we need to compute
  for (size_t i = 0; i < x.size(); ++i) {
    // get the age of the weight 
    double age;
    size_t weight_first_access = firsttime[i];
    if (weight_first_access < timestep) {
      age = timestep - weight_first_access - 1;
    } else {
      age = 0;
      firsttime[i] = std::min(firsttime[i], timestep); // atomic
    }
    // compute the actual regularized weight based on the age of the 
    // parameter: essentially scaling it by (1 - eta * lambda)^(age)
    double regularized_weight = weights[x[i].id] 
        * std::pow(1.0 - stepsize * regularization, age);

    linear_predictor += x[i].value * regularized_weight;
  } 

  // probability that y is 0
  double py0 = 1.0 / (1 + std::exp(linear_predictor));
  double py1 = 1.0 - py0;
  // note that there is a change that we get NaNs here. If we get NaNs,
  // push down the step size or push up the regularization
  
  // ok compute the gradient
  // the gradient update is easy
  for (size_t i = 0; i < x.size(); ++i) {
    weights[x[i].id] += stepsize * (double(y) - py1) * x[i].value; // atomic
  }
  timestep++; // atomic
  return py1;
}



/**
 * Generates a simple synthetic binary classification dataset in X,Y with numdata 
 * datapointsand where each datapoint has numweights weights and average 
 * sparsity "sparsity".
 * Every weight value will be between -1 and 1.
 * Also returns the true weight vector used to generate the data.
 * (the dataset generated by this procedure is actually quite hard to learn)
 */
static double rand_m1_p1() { //return unif random in [-1.0,1.0]
  return (double(std::rand()) / RAND_MAX)*2.0 - 1.0;
}
std::vector<double> generate_dataset(std::vector<std::vector<feature> >& X,
                                     std::vector<double>& Y,
                                     int numweights,
                                     int numdata,
                                     double sparsity) {
  assert(0 < sparsity  && sparsity <= 1.0);
  // generate a random small weight vector between -1 and 1
  std::vector<double> w(numweights, 0);
  for (size_t i = 0; i < numweights; ++i) {
    w[i] = rand_m1_p1();
  }

  for (size_t i = 0; i < numdata; ++i) {
    std::vector<feature> x;  
    // use logistic regression to predict a y
    double linear_predictor = 0;
    // generate a random 25% sparse datapoint
    for (size_t j = 0; j < numweights; ++j) {
      // with 25T probability generate a weight
      if (std::rand() < sparsity * RAND_MAX) {
        x.push_back(feature(j, rand_m1_p1()));
        linear_predictor += x.rbegin()->value * w[i];
      }
    }
    double py0 = 1.0 / (1 + std::exp(linear_predictor));
    double py1 = 1.0 - py0;
    // generate a 0/1Y value. 
    double yval = (std::rand() < RAND_MAX * py0) ? 0 : 1;
    // to get regression, comment the line above and uncomment the line below
    // yval = py1;

    X.push_back(x);
    Y.push_back(yval);
  }
  return w;
}

int main(int argc, char** argv) {
  timestep = 0;
  regularization = 1E-5;
  stepsize = 0.01;
  size_t numweights = 1000;
  weights.resize(numweights, 0.0);
  firsttime.resize(numweights, (size_t)(-1));


  // generate a little test dataset
  std::vector<std::vector<feature> > X;
  std::vector<double> Y;
  std::vector<double> weights = generate_dataset(X, Y, numweights, 10000, 0.2);


  // we output both L2 and 0/1 loss here.
  // but note that L2 loss is not meaningful for binary classification
  // and 0/1 loss is not meaningful for regression
  // The dataset generator above generates for binary classification,
  // but can also be modified to generate regression. See the comment
  // near the bottom of generate_dataset
  for (size_t iterations = 0; iterations < 100; ++iterations) {
    size_t loss01 = 0;
    double lossl2 = 0.0;
    for (size_t i = 0 ; i < X.size(); ++i) {
      double ret = logistic_sgd_step(X[i], Y[i]);
      lossl2 += (Y[i] - ret) * (Y[i] - ret);
      loss01 += (Y[i] != (ret >= 0.5));
    }
    std::cout << "Iteration " << iterations << "\n";
    std::cout << "\tL2 Loss: " << std::sqrt(lossl2) << "\n";
    std::cout << "\t0/1 Loss: " << loss01 << "\n";
  }
}

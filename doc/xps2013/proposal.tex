%% NSF XPS Proposal 2013

\input{header}

\begin{center}
  \textbf{\Large XPS-CLCCA: Scalable Parallelism for Irregular and Graph Applications}\\ \vspace{5mm}
  \textbf{Luis Ceze ~~~~~ Simon Kahan ~~~~~ Mark Oskin (PI)}\\
  \textbf{University of Washington}
\end{center}

\section{Introduction}

Irregular applications generate tasks with work, dependences, and memory
accesses that are highly sensitive to input. Classic examples of irregular
applications include branch and bound optimization, SPICE circuit simulation,
contact algorithms in car crash analysis, and network flow. Important
contemporary examples include processing large graphs in the business,
national security, machine learning, data-driven science, and social network
computing domains. For these emerging applications, fast response -- given the
sheer amount of data -- requires multinode systems. The most broadly available
multinode systems are those built from x86 commodity computing nodes
interconnected via ethernet or infiniband. Our focus is to enable scalable
performance of irregular applications on these mass market systems,
addressing two key challenges:

\paragraph{Irregular applications exhibit little spatial locality.} It is not
atypical for any given task's data references to be spread randomly across the
entire memory of the system. This makes current memory hierarchy features
ineffective. Caches are of little assistance with such low data re-use and
spatial locality. Commodity prefetching hardware is effective only when
addresses are known many cycles before the data is consumed or the accesses
follow a predictable pattern, neither of which occurs in irregular
applications. As a consequence, commodity microprocessors stall often when
executing irregular applications.

\paragraph{Irregular applications frequently request small amounts of off-node
data.} On multinode systems (e.g., clusters of computers), the challenges
presented by low locality are analogous, and exacerbated by the increased
latency of going off-node. Irregular applications also present a challenge to
commodity network technology, which is designed to transfer large blocks of
data, not the word-sized references emitted by irregular application tasks.
Injection rate into the network is insufficient to utilize wire bandwidth when
blocks are below about two-kilobytes, so any straightforward communication
strategy severely under-utilizes the network.

Luckily, many of the important irregular applications naturally offer large
amounts of concurrency. This immediately suggests taking advantage of
concurrency to tolerate the latency of data movement. The fully custom Tera
MTA-2~\cite{tera:mta1} system is a classic example of supporting irregular
applications by using concurrency to hide latencies. It had a large
distributed shared memory with no caches. On every clock cycle, each processor
would execute a ready instruction chosen from one of its 128 hardware thread
contexts, a sufficient number to fully hide memory access latency. The network
was designed with a single word injection rate that matched the processor
clock frequency and sufficient bandwidth to sustain a reference from every
processor on every clock cycle. Unfortunately, while an excellent match to
extremely irregular applications, the MTA was not cost-effective on
applications that could exploit locality and had very poor single-thread
performance, making it a commercial failure. The Cray XMT approximates the
Tera MTA-2, substantially reducing its cost but not overcoming its narrow
range of applicability.

In this proposal we first describe our early
Grappa prototype, a runtime system for irregular
applications. It provides several facilities: a global shared memory
abstraction, user-level threading support, and communication and
synchronization primitives. Applications targeted for Grappa are not expected
to have \emph{any} locality (although what locality that exists is favorably
exploited), but are expected to provide many thousands of concurrent threads.

Next, we present some preliminary results. Even as an early prototype, Grappa
runs several graph-crunching applications (classic examples of irregular
behavior) efficiently on a commodity cluster. For unbalanced tree search,
Grappa is over 3X faster and shows greatly improved scalability compared to
the Cray XMT; conversely, for breadth first search Grappa is 2.5X slower --
fast compared to implementations without Grappa, but obviously there is room
for improvement. 

Given these very positive early results, this proposal plans to extend Grappa
in the following directions:

\begin{itemize}

\item \textbf{Accelerated network communication:} Our preliminary work has
revealed limitations of the current Grappa runtime system with regards to
network scaling. This proposal plans to enhance the network layer with whole
new forms of communication scheduling and aggregation, and explore new forms
of hardware/software interactions of remote direct memory access (RDMA).

\item \textbf{Language and compiler support:} Presently, Grappa is implemented
strictly as a library. Programmers must express remote operations explicitly
as function calls. This adds a burden high-level language programmers do not
usually face and obfuscates opportunities for compiler optimizations. A
significant research thrust of the proposed work is to use language and
compilation support to provide a cleaner programming model and use
statically-available knowledge to lessen the runtime overhead.

\item \textbf{Comprehensive benchmark suite:} Presently there does not exist a
comprehensive benchmark suite of irregular and graph applications. We
are also seeking funding through this proposal to put together just
such a benchmark suite. This suite will include benchmarks, reference
implementations, and input data-sets. In addition a website will be
created to distribute freely the benchmark suite and publish results.

\item \textbf{Support for \ssd\ only data structures:} The size of data being
processed by large systems today (hundreds of terabytes) dwarfs what can fit
into a single system. Multiple system nodes are required to maintain and
access all of this data efficiently. However, given the lack of locality in the application, loading this data into DRAM is wasteful, in addition to adding overheads. The
Grappa runtime is designed to make applications tolerate extremely high memory
latency, thus we will explore having program data structures reside in solid state disks only. This further takes advantage of Grappa's latency tolerance properties to deal with larger data sets and reduce system cost.

\end{itemize}

%%%%%%%%%%%%%% Tempting to delete from here....

Finally, we propose to explore how the lessons learned performing this research -- specifically knowledge of what ultimately limits Grappa's performance for irregular applications using x86 processors and commodity networks with modified communication protocols -- can be applied to the design of future architectures.  Doing so will include studying the following research questions:

\begin{itemize}
\item \textbf{What is the right system architecture support for irregular applications, including architecture, communication and storage?}
\item \textbf{What are the necessary language and compiler support for large-scale, data-intensive irregular applications?}
\item \textbf{How do we comprehensively evaluate performance of graph analytics systems?}
\end{itemize}

%%%%%%%%%%%%%%%%%  To here.... -Mark

\paragraph{Intellectual Merit:}

Efficiently executing irregular computations on commodity hardware is a
\emph{difficult} problem. Much about commodity hardware is seemingly designed
for the opposite application characteristics: caches assume applications have
locality; networks are built with low injection rates but high bandwidth,
assuming large packet sizes; even runtimes and operating systems are built
assuming relatively infrequent communication and synchronization compared to
computation. This proposal rests on the seemingly outlandish claim that this
commodity hardware / software stack can be coerced, through another layer of
software, into efficiently executing irregular applications that both lack
locality and communicate frequently with small amounts of data. A folksy way
of saying it is we think we can jam a square peg through a round hole.
Fortunately we have some preliminary evidence we won't get wedged along the
way. This is an intellectually challenging endeavor, that we believe is well
suited for the NSF XPS program. In summary, our goal is to exploit parallelism
from irregular computations to efficiently execute them at scale.

\paragraph{Broader Impact:}

Irregular computations are of increasing economic importance.  Moreover, they are key to several national security applications.  If commodity hardware can be cajoled into executing such applications efficiently it will open the door for new and rich applications.  Furthermore, irregular applications will then reap the benefits of advances in commodity processors and networks, rather than requiring custom hardware that advances only at much higher cost.  Graph queries, machine-learning and data-driven science will significantly benefit from the technology we aim to create from the ideas in this grant proposal.  In addition our work will include the construction of a comprehensive irregular benchmark suite to help guide our own and the field's research.

%% Roadmap

\section{Description of the Current Grappa System}

In the twenty years that have elapsed since the Tera MTA, commodity
microprocessors have become much faster and multicore has driven down the
absolute price of computation; commodity network price-performance has
improved as well. This shift has afforded us the opportunity to attack the
challenges posed by irregular applications by emulating in software and
inexpensive mass market hardware, the approach taken by Tera. We exploit the
increased aggregate instruction rate per socket relative to chip bandwidth,
using what would otherwise be wasted instructions to manage the multiplexing
of as many as several thousand tasks per core, thus tolerating memory latency,
reducing stalls, and making better use of available bandwidth. Ultimately, the
opportunity is to cover the spectrum of irregular to regular computation:
where tasks exhibit locality, multiplex fewer tasks and expend fewer
instructions on context switching; where locality is lacking, multiplex more
tasks at a higher rate to tolerate latency. Thus we make the best use of task
parallelism -- either to scale to more cores or to tolerate latency -- and of
caches -- either to exploit application locality or to house more task
contexts.

Grappa is a software runtime system that allows a commodity x86
distributed-memory HPC cluster to be programmed as if it were a single large
shared-memory machine and provides scalable performance for irregular
applications. Grappa is designed to smooth over some of the performance
discontinuities in commodity hardware, giving good performance when there is
little locality to be exploited while allowing the programmer to exploit it
when it is available.

Grappa leverages as much freely available and commodity infrastructure as possible. We use unmodified Linux for the operating system and an off-the-shelf user-mode infiniband device driver stack~\cite{OFED}. MPI is used for process setup and tear down. GASnet~\cite{gasnet} is used as the underlying mechanism for remote memory reads and writes using active message invocations. To this commodity hardware and software mix Grappa adds three main software components: (1) a \emph{lightweight tasking\/} layer that supports a context switch in as few as 38ns and distributed global load balancing; (2) a \emph{distributed shared memory\/} layer that supports normal access operations such as \emph{read\/} and \emph{write\/} and synchronizing operations including \emph{fetch-and-add\/}~\cite{fetchandadd} as well as user-defined atomic methods; and (3) a \emph{message aggregation\/} layer that combines short messages to mitigate the aforementioned problem that commodity networks are designed to achieve peak bandwidth only on large packet sizes, yet irregular applications tend to fetch only a handful of bytes at a time. As we will show later, Grappa can tolerate latencies way beyond that of the network. Therefore, Grappa can afford to \emph{trade latency for throughput\/}: by {\em increasing\/} latency in key components of the system we are able to increase our effective random access memory bandwidth, our synchronization bandwidth, and our ability to improve load imbalance.

\begin{figure}[t]
\begin{center}     
  \includegraphics[width=0.95\columnwidth]{figs/system-overview}
\begin{minipage}{0.95\columnwidth} 
  \caption{\label{fig:grappa} Grappa system overview}
\end{minipage}
\vspace{-3ex}
\end{center}
\end{figure}

The current Grappa prototype (Figure~\ref{fig:grappa}) has three main software components: 
\begin{description}

\item [Tasking system.] Our tasking system supports lightweight multithreading to tolerate communication latency and global distributed workstealing (i.e., tasks can be stolen from any node in the system), which provides automated load balancing.

\item[Distributed shared memory.] Our DSM system provides support for fine-grain access to data anywhere in the system. It supports synchronization operations on global data, explicit local caching of any memory in the system, and support for operation on remote data (delegating operations to home node). By tight integration with the tasking system and the communication layer, our DSM system offers high aggregate random access bandwidth for accessing remote data.

\item[Communication layer.] As discussed earlier, modern commodity networks support high bandwidth only for large messages. Since irregular applications tend to need frequent communication of small requests, the main goal of our communication layer is to aggregate small messages into large ones to better exploit what the network can offer. It is largely invisible to the application programmer.

\end{description}

\subsection{Tasks}

The basic unit of execution in Grappa is a {\em task}. Each task is represented by a function pointer and its arguments. A new task is enqueued at spawn time; when resources are free, it is allocated a stack, bound to a core, and executed.

During execution, a task yields control of its core following initiation of any long-latency operation, allowing the processor to remain busy while waiting for the operation to complete.  In addition,  a programmer can direct scheduling explicitly via the Grappa API calls. To minimize yield overhead, the Grappa scheduler operates entirely in user-space and does little more than store register state of one task and load that of another. Context switch times are as low as 38ns even when switching amongst thousands of tasks.

\subsection{Expressing parallelism}

Grappa programmers focus on expressing as much parallelism as possible without concern for where it will execute. Grappa then chooses where and when to exploit this parallelism, scheduling as much work as is necessary on each core to keep it busy in the presence of system latencies and task dependences.

Grappa provides two methods for expressing parallelism. First, when the programmer identifies work that can be done in parallel, the work may be wrapped up in a function and queued with its arguments for later execution using a \texttt{spawn}. A programmer can further specify using \texttt{spawn\_on} to spawn a task on a specific core in the system or at the home core of a particular memory location. Second, the programmer can invoke a parallel for loop, provided that the trip count is known at loop entry. The programmer specifies a function pointer along with start and end indices and an optional threshold to control parallel overhead. Grappa does {\em recursive decomposition} of iterations, similar to Cilk's cilk\_for construct~\cite {cilkforimplementation} and TBB's {\tt parallel\_for}~\cite{intel_tbb}.  It generates a logarithmically-deep tree of tasks, stopping to execute the loop body when the number of iterations is below a specified threshold.

\subsection{Memory}

Applications written for Grappa utilize two forms of memory: local and global.  Tasks executing on the same core share an address space local to that core.  The core accesses this local memory through conventional pointers, such as those generated by a standard C compiler.  We nonetheless refer to these pointers as ``local pointers'' to differentiate them from ``global pointers'' that may be used by other cores to reference the same physical memory as described below.  Applications use local memory for a number of things in Grappa: the stack associated with a task, localized global memory in caches (see below), and accesses to debugging infrastructure that is local to each system node.  Local pointers cannot access memory on other cores, and are valid only on their home core.

Large data that is expected to be shared and accessed with low locality is stored in Grappa's global memory. All global data must be accessed through Grappa's API. %, shown in Figure~\ref{fig:accessing-memory}.

\paragraph{Global memory addressing} Grappa provides two methods for storing data in the global memory. The first is a distributed heap striped across all the machines in the system in a block cyclic fashion.  Addresses to memory in the global heap use \textbf{linear addresses}.  Choosing the block size involves trading off sequential bandwidth against aggregate random access bandwidth. Smaller block sizes help spread data across all the memory controllers in the cluster, but larger block sizes allow the locality-optimized memory controllers to provide increased sequential bandwidth. The block size, which is configurable, is typically set to 64 bytes, or the size of a single hardware cache line, in order to exploit spatial locality when available. The heap metadata is stored on a single node. Currently all heap operations serialize through this node; while this has been sufficient for our benchmarks, in the future Grappa will provide parallel performance through combining~\cite{MAMA,flatcombining}.

Grappa also allows any local data on a core's stacks or heap to be exported to the global address space to be made accessible to other cores across the system. Addresses to global memory allocated in this way use \textbf{2D global addresses}.  This uses a traditional PGAS addressing model, where each address is a tuple of a rank in the job (or global process ID) and an address in that process. The lower 48 bits of the address hold a virtual address in the process. The top bit is set to indicates that the reference is a 2D address (as opposed to linear address). This leaves 15 bits for network endpoint ID, which limits our scalability to $2^{15}$ endpoints. Any node-local data can be made accessible by other nodes in the system by wrapping the address and node ID into a 2D global address. This address can then be accessed with a delegate.  At the destination the address is converted into a canonical x86 address by replacing the upper bits with the sign-extended upper bit of the virtual address. 2D addresses may refer to memory allocated from a single processes' heap or from a task's stack.

\paragraph{Global memory access} There are two general approaches Grappa applications use to {\emph access} global memory. When the programmer expects a computation on shared data to have spatial locality to exploit, explicit {\em cache} operations may be used. When there is no locality to exploit, remote {\em delegate} operations are used.

\textbf{Explicit caching.} Grappa provides an API to fetch a global pointer of any length and return a local pointer to a cached copy of the global memory.  Grappa cache operations have the usual read-only and read-write variants, along with a write-only variant used to initialize data structures. Languages for distributed shared memory systems have done optimizations to achieve a similar goal. For example, the UPC compiler coalesces struct and array accesses into remote get/put \cite{Chen:2005}, and Fortran D compiler's message vectorization hoists small messages out of a loop \cite{FortranD:1992}. 

Caching in Grappa additionally provides a mechanism for exploiting temporal locality by operating on the data locally --- even though we aim Grappa at applications with low locality, we want to be able to exploit it when available. 
Currently, the burden is on the programmer to maintain coherency by introducing explicit synchronization.  Future work will develop a software directory based coherence scheme to simplify consistent access to global data.

\textbf{Delegate operations.} When the access pattern has low-locality, it is more efficient to modify the data on its home core rather than bringing a copy to the requesting core and returning it after modification. Delegate operations provide this capability. Applications can dispatch computation to be performed on individual machine-word sized chunks of global memory to the memory system itself (e.g., \emph{fetch-and-add}).  Delegate operations, proposed in \cite{Nelson:hotpar11} and \cite{delegated:oopsla11}, are also the primary synchronization method in Grappa.

\subsection{Communication}
\label{sec:communication}

In order to mitigate the low message injection rate limits of commodity networks, Grappa's communication stack has two layers: one for user-level messages and one for network-level messages.

At the upper layer, Grappa implements asynchronous active messages \cite{vonEicken92}. Each message consists of a function pointer, an optional argument payload, and an optional data payload. When a task sends a message, the message is copied to a send queue associated with the message's destination and the task continues execution.

Grappa's lower networking layer aggregates the upper layer's messages to
improve performance. Commodity networks including
Infiniband~\cite{infinibandbandwidth} achieve their peak bisection bandwidth
\emph{only} when the packet sizes are relatively large---on the order of
multiple kilobytes. The reason for this discrepancy is the combination of
overheads associated with handling each packet (in terms of bytes that form
the actual packet, processing time at the card and processing on the CPU
within the driver stack). Our measurements confirm manufacturers published
data~\cite{infinibandbandwidth}, that with this packet size the bisection
bandwidth is only a small fraction, less than 3\% of the peak bisection
bandwidth.

In our early experiments the vast majority of requests were smaller than 44
bytes, far too small to make efficient use of the network. To make the best
use of the network, we must convert our small messages into large ones. When a
task sends a message, it is not immediately sent, but rather placed in a queue
specific to the destination. If the size in bytes of a queue is above a
threshold (e.g., 4096, typically large enough for good bandwidth), the
contents of the queue are sent immediately. Also, each queue has a wait time
threshold ($\approx${1ms}). If the oldest message in a queue has been waiting
longer than this threshold, the contents of the queue are sent immediately,
even if the queue size is lower than the message size threshold. Finally,
queues may be explicitly flushed, for situations when latency is more
important than bandwidth.

Underneath the aggregation layer, Grappa uses the \gasnet~communication
library~\cite{gasnet} to actually move data. All interprocess communication,
whether on or off a cluster node, is handled by the \gasnet~library.
\gasnet~is able to take advantage of many communication mechanisms, including
ethernet and infiniband between nodes, as well as shared memory within a node.

Some networks provide access to a remote machine's memory directly. This would seem to be a good fit for a programming model focused on global shared memory, however, we do not currently use it. In our experiments, we found that RDMA operations are subject to the same message rate limitations as all other messages on these cards, and thus using raw RDMA operations does not measurably increase achieved bandwidth (i.e., we are limited by injection rate not peak bandwidth).  At the moment, we implement remote memory operations with active messages in conjunction with aggregation, providing an order of magnitude or more increase in throughput compared to straightforward RDMA. As we describe later, however, we wish to explore richer semantics for RDMA communication that will make it integrate better with user level runtime systems such as Grappa.

\subsection{Performance}

To evaluate the performance of our current prototype with respect to the XMT,
we ran each of our three benchmarks on up to 16 nodes of each machine. Grappa
used 6 cores per node, with the best parameters chosen for each point. In some
cases, the XMT could not run the benchmark with 2 nodes, so the point is
omitted.

\begin{figure}
\begin{center}

\hspace{-1in}\begin{minipage}{0.3\textwidth}
\begin{center}
\includegraphics[width=2.5in]{figs/uts_performance.pdf}
\caption{\label{fig:uts_compare} Performance of in-memory unbalanced tree search.}
\end{center}
\end{minipage}
\hspace{1.25in}\
\begin{minipage}{0.3\textwidth}
\begin{center}
\includegraphics[width=2.5in]{figs/bfs_performance}
\caption{\label{fig:bfs-performance} Performance of breadth-first search.}
\end{center}
\end{minipage}

\end{center}
\end{figure}

\paragraph{Unbalanced tree search} We ran UTS-mem with a geometric 100M-vertex
tree (T1L). Figure~\ref{fig:uts_compare} shows the performance in terms of
number of vertices visited per second versus number of compute nodes. Grappa
is 3.2 times faster than the XMT at 16 nodes. As we will show later, the
performance advantage Grappa has over XMT increases as more nodes are added.
The main reason Grappa performs better is the software-based delegate
synchronization obviates the need for the retry-based synchronization that XMT
uses.


\paragraph{BFS} We ran BFS on a synthetic Kronecker graph with $2^{25}$ vertices and $2^{29}$ edges (25 GB of data). Figure~\ref{fig:bfs-performance} shows our performance in terms of graph edges traversed per second. The XMT is 2.5 times faster than Grappa at 16 nodes.  Performance has not peaked for Grappa, suggesting that adding more nodes will increase performance.


\section{Related work}

Our efforts are built on many existing ideas in programming languages, systems and architecture. In this section we discuss related frameworks and key enabling technologies that Grappa builds upon.

\paragraph{Comparable frameworks} Distributed graph processing frameworks like Pregel~\cite{pregel:2010} and Distributed GraphLab~\cite{distgraphlab:vldb12} share similar goals as Grappa. Pregel adopts a bulk-synchronous parallel (BSP) execution model, which makes it inefficient on workloads that could prioritize vertices. GraphLab, on the other hand, schedules vertex computations individually, allowing prioritization, which gives faster convergence in a variety of iterative algorithms.  GraphLab, however, imposes a rigid computation model where programmers must express computation as transformations on a vertex and its edge list only, with information only from adjacent vertexes. Pregel is only slightly less restrictive, as the input data can be any vertex in the graph.  Grappa also supports dynamic parallelism with asynchronous execution, but parallelism is expressed as tasks or loop iterations, which is a far more general programming model for irregular computation tasks.

\paragraph{Global memory} Grappa includes a custom implementation of a software distributed shared memory (DSM) system. Many traditional software DSM systems are page based~\cite{Treadmarks,munin} and aim to hide the fact that they are built in software from applications by exploiting the processor's paging mechanisms, therefore relying heavily on locality. Instead, Grappa, like other partitioned global address space (PGAS) models, implements its DSM at the language, rather than system level.
%%note:  Myers suggestion is not incorporated here, as this seems ok:
Languages such as Chapel~\cite{Chamberlain:2007}, X10~\cite{X10:2005}, and UPC~\cite{upc:2005} make accesses to shared structures look like normal memory references. As we describe later, Grappa chooses a middle ground, where global addresses are explicit in the API and local accesses are emitted conventionally by the compiler.  While Grappa's DSM system is conceptually similar to prior work, its implementation is tuned for irregular computations.  Past DSM work, being page-based, could exploit the RDMA capabilities of network hardware to move large page-sized blocks of data from node to node. In our experience, when these networks move small blocks of data (a few bytes), only a fraction of the available bandwidth is achieved.
%%note:  Myers suggestion is not incorporated here, as this seems ok:
In addition, the DSM system in Grappa ends up being tightly coupled to the task scheduler in order to overlap long latency memory operations with useful computation.  For these two reasons it became necessary to build a new DSM system specifically for Grappa.

\paragraph{Multithreading} Grappa uses multithreading to tolerate memory latency. This is a well known technique. Hardware implementations include the Tera MTA~\cite{tera:mta1}, Cray XMT~\cite{feo:xmt}, Simultaneous multithreading~\cite{tullsen:smt}, MIT Alewife~\cite{agarwal:alewife}, Cyclops~\cite{almasi:cyclops}, and even GPUs~\cite{gpus}. As we describe in this paper, Grappa use a lightweight user-mode task scheduler to multiplex \emph{thousands\/} of tasks on a single processing core. The large number of tasks is required because of the extremely high internode latency Grappa mitigates.  Grappa's task library employs several optimizations: an extremely fast task switch, a small task size, and judicious use of hardware prefetching to bring task state into the cache before the task is scheduled to execute.

\section{Proposed Research}

As described earlier, our work on Grappa so far has focused on building an initial prototype so we have a baseline system to build upon. This early prototype has lead us to think of many research opportunities, from new ways to improve network communication scheduling, to new data storage management ideas to compiler and language support. Below we detail our research plans in each of these thrusts. 

\subsection{Networking}

Since Grappa is designed for applications that have no or little locality, nearly \emph{every} memory reference is to a remote system node.  This makes the networking layer key to overall system performance.  In this section we outline the key directions we wish to take in enhancing the networking layer of the Grappa runtime system.

\paragraph{Virtual topology:} Currently Grappa uses a flat network topology where each processing core in the system is individually addressed. As more nodes are added to the system, a quadratic increase in the number of threads (in aggregate across all nodes) is required in order to compensate for the \emph{same} network delay.  This is because the current aggregator treats each of these end-points individually, i.e., it only aggregates messages that go to the same exact end-points. This does not scale as the number of nodes increases because the probability of messages going to the same destination goes down (assuming a somewhat uniform communication distribution). In summary, this past design choice is simple but does not scale.

We need to redesign the aggregator to build in a hierarchical virtual network layer.  The first layer of this hierarchy is to treat all processors at a given system node as a single destination. We intend to continue to use the lock-free single-threaded \gasnet~communication layer that we have been using, as we have found the lock-based one to incur too high of an overhead for our purposes.  Thus the aggregator will need to very carefully utilize lock-free data-structures and interact with \gasnet~carefully.
%%Didn't understand Myers suggestion here.

The second, and upper layers of the virtual topology are built from clustering system nodes into groups.  Instead of aggregating for all end-points, aggregation will occur for system nodes within a group.  A designated (and dedicated) node will act as a router, processing incoming and outgoing traffic to system nodes within the group and other groups.

We expect this hierarchical organization will reduce scaling requirements from \BigO{n^2} threads to \BigO{n \times log(n)}. Of course, it takes what used to be a single network hop for communication and turns it into \BigO{log(n)} hops. More threads will be required to overcome this increased latency, but we hypothesize only \BigO{log(n)} more, hence a real \BigO{n / log(n)^2} scaling benefit is achievable with a virtual hierarchical network topology.

\paragraph{Hardware support for aggregation:} If the increased latency caused by extra hops in hierarchical aggregation turns out to be detrimental, we will explore moving some of the aggregation logic to the network card, either by exploiting off-the-shelf programmable network cards or experimental systems such as NetFPGA~\cite{netfpga} (while we are planning to focus on infiniband-based networking, if we use the NetFPGA route we will be using Ethernet as a test-bed).

\paragraph{User-level RDMA:} Certain commodity networking hardware today supports remote direct memory access (RDMA). Unfortunately, \emph{how} it is implemented is so primitive and low-level that it is difficult or impossible to efficiently use in practice. Below we start with a short description of the current semantics of RDMA implementations. Next we describe the research we wish to undertake to make RDMA more useful.

Networking infrastructure supports remote read and write operations. Read operations take a source address and key (on a remote node), a destination address and key (on the local node), and a length. Cards support address translation for regions of memory that are registered with them. These translations, indexed by the supplied key, are used to perform virtual to physical address translation. The physical address of the destination address is similarly translated using the translation table at the local node. Cards also support remote write operations with a similar set of semantics, where addresses are translated on a card using the card's local translation table. Both read and write operations are asynchronous. The requesting processor initiates the operation and then can check for completion by querying a completion queue at the local card. \emph{Notably}, the processor(s) on the remote node are provided a polling interface whereby they can learn if a remote write operation occurred. No such interface exists to learn about remote read or atomic operations.

This interface suffers from multiple shortcomings. The biggest of which is that ultimately, moving data to and from a remote node constructively requires higher level semantic operations than simple reads and writes. These higher level semantics can, of course, be synthesized from the underlying read and write primitives, but it wastes round trips and synchronization time. One possible alternative solution is to enable remote nodes to dispatch high-level data-structure manipulation requests (e.g. insert this item into a queue, remove an item from a set). Past work on Active Messages~\cite{vonEicken92} provides the foundation on which to build these primitives.  The challenge here is to make those calls programmable and interoperate securely with a modern operating system, and a user-level runtime system. Additionally there are questions about \emph{who} should carry out the computation. Classic RDMA does not interrupt the host processor, but these richer programmable semantics will require either a processor in the network interface or complex RDMA tasks need a mechanism to interrupt the host processor \emph{without leaving user space}.

The second challenge is that the virtual to physical address translation is necessary for RDMA operations to be meaningful. Current systems require the OS to pin translations so cards can operate directly on physical addresses. This removes paging flexibility from the operating system and requires runtime systems to be aware that this is happening, which limits overall usability. We will explore recent advances in supporting direct memory access with address translation (e.g., AMD IOMMU) to support flexible RDMA operation that go through full physical address translation without having to interrupt a general-purpose core.
 
\subsection{SSD-only data structures}

Grappa applications are likely to use large amounts of data, 100s of terabytes to petabytes. Keeping all these data in DRAM only is unlikely to be cost effective. We will explore mechanisms that use solid-state storage as primary memory for program data, greatly expanding the directly addressable storage at low cost.

Grappa is very effective in hiding latencies of network communication.  If an access is to a chunk of the heap stored in a node's local memory, Grappa issues a prefetch for the address and context switches to other work while the prefetch is executing~\cite{Nelson:hotpar2011}. If the access is to memory stored on another node, Grappa issues a message to that node to request the data and context switches to other work until the reply arrives.  In fact, our measurements and estimates show that we might be able to tolerate even greater latency -- on the order of milliseconds per request.  This provides the tantalizing possibility that we will be able to keep data in \ssd\ drives only and service data access requests directly from them, without leaving copies in DRAM.  The natural point of integration for non-volatile memories in Grappa is to store the global distributed shared heap in non-volatile memory. All accesses to this shared heap are done through an \api, so this switch would be transparent to the programmer.

This is not straightforward, however, for three reasons: (1) \ssds\ assume spatial locality. In our benchmarks, we find the average request size to be less than 64 bytes. Today's \ssds\ are optimized for reads in the multi-kilobyte range. (2) \ssds\ have a low request rate. The fastest~\cite{fusionio} can perform on the order of 1 million random reads per second, and many are limited to less than 100K/s. Even the underlying flash chips are slow, on the order of 40K/s~\cite{micronFlash}. In contrast, our measurements suggest cores may be idle in Grappa if each node cannot perform 100 million random reads per second of the global heap. And (3) \ssds\ have high overhead. Mass market \ssds\ are usually treated as normal disks: accesses must occur through the kernel, incurring a high context-switch overhead. Grappa depends on user-mode access to the network layer to avoid this overhead; a similar approach has been applied to \ssds\ in recent research~\cite{caulfield:2012}, however.

To address this challenge we will explore building a flash array consisting of several ten's to a hundred disks \emph{per node}.  This will enable us to increase random accesses to our target 100 million references per second. But it will require several active disk controllers per system.  We do not expect any existing device driver stack will work at this scale.  Hence we will need to write a custom disk controller device driver and file system that supports such a high degree of concurrent requests across so many parallel channels.

\subsection{Compiler and language support}

Language and compiler support are instrumental in making Grappa usable to a
wide audience of application developers. We plan to use the LLVM~\cite{llvm}
infrastructure. Below we describe the specific aspects we plan to tackle.

\paragraph{Relaxed consistency:} Presently Grappa uses a consistency model that is sequentially consistent for data-race free programs.  Underlying hooks are available to build even more relaxed consistency models, although we don't expect programmers themselves will want to use them.  A compiler, however, can see a performance boost by analyzing code and exploiting alias knowledge to program to this underlying Grappa \api, potentially gaining a performance boost by exposing more memory concurrency and hiding more latency.

\paragraph{Language support for synchronization operations: } At the moment, runtime library support is provided in Grappa for basic synchronization primitives (lock, barrier, etc).  Building synchronization directly into the language has the advantage that the compiler can understand and optimize it.  Several optimizations are possible in the Grappa framework, including exploiting the scheduler and work stealer to elide certain lock operations (scheduling is co-operative on a per-CPU basis), converting synchronization primitives into remote delegate operations, and exploiting the check-in / check-out nature of global memory in Grappa to coalesce multiple synchronization operations together.

\paragraph{Delayed writes: } At the moment when we we write applications for Grappa we use a technique that we call feed-forward delegates.  Essentially what this technique does is if a loop is reading and writing disjoint sets of data, it is possible and we have found efficient, to dispatch writes asynchronously and ensure those writes are completed only by the end of the loop.  This increases performance by further overlapping computation with memory access.  A compiler aided by programmer annotations or capable of alias analysis can do this transformation automatically for the programmer.

\paragraph{Automatic delegate synthesis: } Delegates are a technique in Grappa where one thread can request a small computation to be carried out on a remote node.  For instance, an increment on a memory location is dispatched to the system node that manages that memory operation and carried out there.  Currently the decision of what is a delegate is left to the programmer.  But this is an area rich for compiler optimization.  We will explore techniques to automatically synthesize delegates from code, potentially leading to more complex delegates and more efficient code.

\paragraph{Read and write combining: } As a special case of automatic delegate synthesis, when multiple remote memory operations are to be dispatched to the same node from the same thread, it may be more efficient to combine these together into a single request.  We will develop a compiler pass to combine operations when possible.
%%Myers suggested a reference to UPC or other previous work

\paragraph{Compile for a continuation model: } Presently, whenever a task is active it is allocated a complete stack.  Since several thousand tasks are active per processor at any one time, these stacks occupy a large footprint in memory.  We wish to explore compiling tasks to a continuation model, whereby when a task dispatches a remote memory operation and is to be context-switched out, another task can utilize that stack.  Combined with the delegate synthesis ideas described above, we will also explore runtime and compiler support to convert remote memory operations into thread migrations; instead of moving the data to the system node that has the task, we will move the task to the data in cases where that would be more efficient.

\paragraph{Task-weaving: } Much of the time the same task code is executing for hundreds of thousands of tasks in the Grappa runtime system.  For example, each task may be processing a loop body for a parallel for loop.  However, tasks introduce overhead, so we already manually impose a cutoff on recursive decomposition of parallel loops.  This way, multiple loop bodies are collected into a single task and executed serially.  However, this limits the amount of parallelism available for latency tolerance. A more efficient technique is to allow the compiler to unroll and schedule all loop bodies simultaneously within the same task, including overlapping remote memory operations, thereby ``weaving'' these independent tasks together to overlap computation with communication with minimal task overhead.  Further, a set of ``woven tasks'' may be amenable to vector execution, allowing even more parallelism to be exploited.

\paragraph{Task load control:} Having too few tasks per core fails to fully tolerate remote memory latency; having too many increases overheads unnecessarily. Currently, we choose a fixed number at program instantiation.  This approach is sufficient as a first step, but isn't a workable long term option.  Our goal will be to develop as part of the runtime, auto-tuning technology that dynamically controls the number of spawned tasks.

\subsection{Graph benchmark suite}

In order to facilitate the development and evaluation of Grappa, we will develop a comprehensive benchmark suite for graph analytics applications. This will not only be useful for us, but the whole community at large, since there are few benchmarks in this application space.

This benchmark suite will consist of application kernels, reference implementations, data-sets, data-set generators, documentation, specifications for generating standardized performance results, and a hosting service (http://graphbench.org) to provide public access to the benchmark and datasets as well as aggregate and publish results by all users of the benchmark suite.  We will start by adapting and building a collection of key computational kernels from graph and irregular applications.  We will collect sample data from existing sources, and produce synthetic workload generators. The collected data provides a grounding for performance analysis on real-world problems. The workload generators provide cleaner, easier to reason about randomly generated data-sets at a programmable scale.

This benchmark suite will be a tremendous value to the research community. Not only will it enable researchers to evaluate ideas on graph processing system techniques, but it will also enable fair comparisons of ideas in a depth not currently available from other graph related benchmark suites~\cite{graph500}. The benchmark suite will be released in a completely open fashion, so anyone will be able to use it (and contribute to it).  All code will be distributed into the public domain (BSD style licensing) so companies can use it. The goal is to get something out relatively early in the hope of (i) getting others to cluster around this work rather than develop a plethora of overlapping alternatives, and (ii) help establish consistent metrics for performance results.

\section{Broader impacts}

\paragraph{Benchmark suite:} As previously described, we are seeking funding support to build and distribute a benchmark suite for irregular and graph applications.  This benchmark suite has the potential to have a broad impact on academia and industry.  We will distribute this benchmark through \texttt{graphbench.org} and will also collect and publish results from competing research groups.

\paragraph{Technology transfer:} Bill Dally during his 2010 Eckert-Mauchly award speech said (in effect), technology transfers happen when people transfer.  Having just come back from a three year stint founding a startup company (where we transferred the Deterministic Multiprocessing technology from UW), we've seen this first hand.  The students and faculty involved in the Grappa project plan to spin off a company with their work.  The exact time frame is not yet known, but everyone involved in the project is excited to see the technology underpin a startup company and make this technology available to the broader commercial world.

\paragraph{Curriculum development activities:} Concurrent with the execution of the research activities described in this grant we will seek to teach an advanced graduate class on advanced concurrency concepts.  We taught a similar class over six years ago and it is time to revive, update and teach it to the next generation of computer architecture graduate students. We will also reach out to instructors of classes that relate to large-scale irregular computation, e.g., machine learning and computer vision, in order to expose students to parallel programming concepts as well as the Grappa framework. 

\paragraph{Increasing involvement of underrepresented groups:} Computer science as a whole has a significant problem attracting women and minorities to the field.  Computer architecture has an even more extreme problem with diversity (in the PI's view).  This is a problem the PIs have taken quite seriously since entering the field, and we are quite proud of our PhD students, both female who have gone on to faculty positions in top academic institutions, and male who appreciate the gender and ethnic disparity and who work hard at their own institutions to correct it.  We continue to recruit a diverse graduate student mix and we continue to include undergraduate students in our research.  At the moment we are mentoring two women undergraduates involved in the Grappa project, and if this proposal is funded will apply for REU funds to continue their involvement.

\section{Results from Prior NSF Support}

\paragraph{Ceze:} Prof. Ceze's research is on computer architecture, programming languages and operating systems. His primary focus has been on improving programmability, reliability and energy efficiency of multiprocessor computer systems. His research is funded primarily by NSF, PNNL and industry. His CAREER award {\em Deterministic Shared Memory Multiprocessing: Vision, Architecture and Impact on Programmability} (CCF-0846004, 2008-13, \$559K) demonstrated that it is possible to provide deterministic execution of arbitrary multithreaded programs without sacrificing performance significantly. The results include innovation in architecture, compiler, and operating-systems. Two other NSF grants, {\em Code-Centric Approach to Specifying, Checking, and Discovering Shared-Memory Communication} (CCF-1064497, 2011-2014, \$901K) and {\em Precise Concurrency Exceptions: Architecture Support, Semantics and System Implications} (CCF-1016495, 2010-2013, \$500K), are leading to significant results in dealing with concurrency errors and concurrent programming language semantics. His work on large-scale graph processing is being funded by the Pacific Northwest Laboratory. His work on energy-aware programming models and approximate computing has been primarily funded by a seed NSF grant {\em Disciplined Approximate Programming for Energy-Efficient Computing} (CCF-1216611, 2012-2015, \$300K) and industry, including Microsoft, Google and Qualcomm.

\paragraph{Kahan:} Dr. Kahan's research has focused on irregular parallel computation.  Over the course of twelve years, he developed the Tera MTA's multithreaded runtime system and memory allocator still in use today in the Cray XMT, exploring applicability of the system to production codes and benchmarks in the engineering, science, and government domains.  His research has been driven by real-world application needs, resulting in new algorithms and systems to address concurrency bottlenecks.  He is presently an employee of the Pacific Northwest National Laboratory and an affiliate faculty member of the Computer Science and Engineering Department at the University of Washington.  He has no past or current NSF support.

\paragraph{Oskin:} Prof. Oskin's prior research has focused on architectures and systems for scalable computing.  With support from NSF he has carried out a variety of research activities, leading to over 50 scholarly publications, three PhD students graduated, four masters students graduated, and a company spin-off.  The past NSF-supported work (WaveScalar) heavily influences the ideas described here. In some sense, the work here is a culmination in thought processes about why WaveScalar did not achieve as much ILP as earlier limit studies in ILP suggested were possible.  NSF support for the PI's work, {\em ITR: WaveScalar, A New Approach to Scalable System Design} (CCF-0325635, 2003-07, \$1.3M), {\em CAREER: Soft-Instruction Set Computing} (CCF-0133188, 2002-05, \$443K), helped to develop WaveScalar.  He has also spent over a decade exploring architectural implications of quantum computing technologies, with support from NSF {\em NER: Computer Aided Design of Silicon-based Quantum Computers} (CCF-0210373, 2002-04, \$90K), {\em EMT: Microarchitectures for Quantum Computers} (CCF-0523359, 2006-10, \$275K), {\em EMT: Self-Correcting Fault-Tolerant Quantum Computers} (CCF-0523359, 2005-08, \$406K) DARPA {\em QuIST: Architectures and Applications for Scalable Quantum Information Systems}.  He is currently exploring runtime systems for irregular parallel applications, which is the subject of this proposal and is partially funded by the Department of Energy.

\section{Work Plan}
\label{sec:plan}

Our research plan can be divided horizontally into three core areas: programming languages/compilers; communication and storage; and benchmarking. Below we outline how we plan to advance these fronts over the next four years.

\vspace{2ex}


\noindent\textbf{Year 1:} The first year of this effort will focus on creating the benchmark suite and initial language and compiler support.  We aim to pull together 6-8 benchmarks, sample data sets, and scripts to measure performance results on the initial Grappa runtime, UPC~\cite{upc:2005} and MPI implementations, and serial reference code.  For the compiler we will have the front end done enabling easy addition of keywords to annotate data-types and structural elements (parallel for, etc).  The compiler will be generating non-optimized Grappa code by the end of the first year.

\vspace{1ex}
\noindent\textbf{Year 2:} The second year of effort will see the completion of the benchmark suite, a significant effort on compiler optimizations for Grappa, and the beginning of the networking layer research.  The benchmark suite will include 12-15 applications and/or kernels.  The website will be live and we will be conducting significant outreach efforts to drive adoption.  On the compiler front we will have implemented the key optimizations that provide the highest benefit for Grappa code.  In addition we will have a detailed understanding of what additional work remains to support other languages / language constructs.  In this year we will start work on rewriting the networking layer of the Grappa runtime, providing a virtual network topology.  We will also begin work on programmable RDMA by using FPGA-based networking gear.


\vspace{1ex} 
\noindent\textbf{Year 3:} In the third and final year of this
effort (supported by this grant), we will finish the networking layer rewrite.
We will also have constructed a test-bed system for the SSD-only data
structure research. Grappa will be ported to this new style of memory
hierarchy, which we envision will largely be a tuning and not rewriting
effort. Finally we will complete a detailed system evaluation using our
benchmark suite.

\section{Summary}

Irregular applications are increasing in importance to the computing industry.
For a long time they have underpinned computations of interest to the national
security arm of the federal government. But now, they underpin such questions
as ad placement in social networks, and analysis of complex data-sets in
medicine and science. The defining characteristic of these applications is
poor locality and massive available parallelism. Our effort is focused on
making these applications perform well and scale on commodity hardware. The
key idea that makes this work is to rely on massive concurrency to tolerate
memory latency, instead of relying on locality (which doesn't exist for these
applications). This core idea underpinned the XMT~\cite{feo:xmt} system, a
fully-custom hardware system. We plan to explore that idea in software only,
running on commodity processors. And unlike the XMT, however, we must also
contend with commodity networking hardware, and here we again rely on
threading to provide a sufficient number of requests that we can buffer them
prior to dispatching on the network, to overcome bandwidth limitations when
operating with small messages.

Our preliminary effort has shown a workable proof of concept of all this
runtime smoke and mirrors: a commodity cluster can out do the XMT at its own
game, by emulating its key features in software. What is needed now is
additional effort to improve the underlying performance of this runtime and
make it usable to a broad base of users. We will be focusing our efforts
in the next three years on the key performance component, the networking
layer, and the key programmer efficiency linchpin, high level language
support. In addition we need a useful mechanism for handling data-sets in the
petabyte range, and here we imagine building a cluster of machines that relies
primarily on SSD's and not DRAM as main memory data-set storage.

If successful, the Grappa runtime will have a transformative impact on the
computing industry. Irregular applications are hugely important, and if off
the shelf hardware can be efficient with them a new class of questions can be
asked of computing systems. Additionally our benchmark suite development will
further drive innovation among researchers in this space.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%
%%%%% Extra stuff: collaboration plan and data-management plan
%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\section*{Data Management Plan}

The proposed research involves developing experimental systems and
assessing how well they work. The kinds of data we will produce
include: characterization data of program behavior; data on system
performance and reliability; data on how programmers use new language
constructs and tools. As is our practice, we will regularly submit
papers for publication detailing our findings and design of our
systems.

As we have already been doing with all our research, for every
published paper, we make all the raw data available online. This
allows other researchers to look at the data in different ways, and it
enables them to reproduce results. We also often publish
the experimental software developed, following the rules outlined by
the University intellectual property office --- which provides ample
rights for other researchers to build upon our experimental
systems. We do not anticipate using privileged or confidential
information. 

\newpage
\section*{Collaboration Plan}
%\setcounter{page}{1} 

Because the PIs already successfully collaborate and co-advise an
active cadre of spectacular students, the necessary tools and
coordination mechanisms are already in place for this project to
proceed successfully.  We first describe the PIs' complementary
backgrounds, then describe the current organization of our (amorphous)
research group, then describe the specific technology and facilities
that enable seamless coordination, and finally describe specific PI roles
for the research in this proposal.

\subsection*{Complementary PI Experience}

PI Oskin has traditionally participated in the computer-architecture research
community, co-PI Ceze, in addition to the architecture community also has a
presence in the systems and programming languages community, and co-PI Kahan
has participated mainly in the parallel and scientific computing community.
The move toward multicore architectures has re-invigorated collaboration
across these divides and recent joint work by Oskin, Ceze, and Kahan
embodies this important trend. There are currently five graduate students for
which the last two of the PIs serve as advisors. In addition to the project
described in this proposal, they are working together on techniques for
detecting concurrency errors with machine learning and language
specifications, using disciplined approximations to save energy in computing,
and developing new operating-system abstractions for emerging non-volatile
memory devices. What has made these collaborations so fruitful in such a short
period of time is the PIs' complementary backgrounds. We now briefly describe
these backgrounds.

Oskin has explored a wide variety of architectural mechanisms for improving application performance, most notably intelligent memory~\cite{activepages} and dataflow computing~\cite{wavescalar-micro, wavescalar-isca}.  He has developed novel evaluation techniques~\cite{hls} that enable efficient design space exploration of complex superscalar processors.  His past work has included runtime systems for GPU-based computing~\cite{oskin-gpu}, application specific processor design~\cite{sherpa} and polymorphic on-chip networks~\cite{polynetworks}.  He has also explored several architectural design options for quantum computers~\cite{quantum-isca03, quantum-isca05, quantum-isca08}.  Finally he developed mechanisms to make parallel systems more deterministic and easier to program~\cite{dmp}.  His most recent work is focused on the work described in this proposal~\cite{Nelson:hotpar2011}, which is to make irregular computations efficient on commodity hardware.

Ceze has been conducting research on architecture support, compiler techniques, and programming models to improve the programmability of multiprocessor systems for seven years and has co-authored over thirty papers in these areas.  His research includes advances in architecture~\cite{tlsooo, bulk, bulksc, swbulk, cyclops, dmp, delorean} and programming models~\cite{colorama, ipot, posh} to simplify exploiting concurrency in multiprocessors. Some of his recent results especially related to this proposal include a programming model based on ordered transactions and data annotations~\cite{ipot}; improving reliability of multithreaded software with architecture support for bug detection and avoidance~\cite{atomaid, aatoppicks, bugaboo, oshajava, cs-isca10, ce-isca10}; and deterministic execution of multithreaded programs~\cite{dmp, dmptoppicks, asplos10coredet, dmpos}. PI Ceze recently co-organized an NSF-funded workshop focused specifically on deterministic multiprocessing. The workshop was very successful, bringing leaders in the multicore area from industry and academia.

Kahan has been working in the area of parallel computing for irregular applications for twenty years, largely in industry.  At Cray (previously Tera Computer) he developed the runtime system~\cite{Alverson95schedulingon,Alverson97terahardware-software} and combining memory allocator~\cite{MAMA} still in production use on Cray's XMT system in which over 50,000 threads execute concurrently sharing one global address space.  He has co-authored patents and publications for both of these systems as well as for a scalable connected components graph algorithm that bears his name~\cite{Underwood07analyzingthe}. Kahan originated and for three years has been co-PI for the Grappa project.

\subsection*{The SAMPA Research Group: Addressing the Concurrency
  Challenge Across the Execution Stack}

The PIs organize their joint research activity under the umbrella of
the ``SAMPA group'' (SAfe MultiProcessing Architectures).  Some brief
statistics help summarize the group's size, strength, and focus:
\begin{itemize}
\item There are roughly ten graduate students, most actively
  coadvised.
\item In the last five years, the group has produced over twenty papers
  published in ASPLOS, ISCA, PLDI, OOPSLA, MICRO, and OSDI as well as three
  papers in the USENIX Workshop on Hot Topics in Parallelism.
\end{itemize}
The SAMPA group is an ideal size for collaboration across the system
execution stack from computer architecture up through software
engineering.  Compared to large collaborations with several faculty,
there are no graduate students (or faculty) who specialize entirely in
particular subtopics.  The students gain expertise across
computer-science disciplines and use appropriate ideas from hardware,
software, theory, etc. to solve research problems.  

\subsection*{Specific Technology and Facilities}

The research group that will perform the research has the facilities
needed for successful collaboration.  There is a shared lab facility
where weekly meetings are held.  Students in the group share computing
infrastructure, including dedicated machines and regular use of cloud
services such as Amazon's EC2 for scalability experiments.

The PIs regularly hold joint meetings with each student that they
co-advise.  They do not split advising duties so much as duplicate
them.

\subsection*{Specific Roles and Responsibilities}

It should be clear from the preceding discussions that the PIs are
entirely comfortable jointly managing the work in the proposal and the
students involved.  Nonetheless, several specific pieces of the work
more naturally align with the expertise and interests of one of the
PIs:

\begin{itemize}
\item PI Oskin will focus on communication issues. 
\item PI Ceze will focus on compiler and programming languages work.
\item PI Kahan will focus on organizing benchmarks and performance evaluation efforts.
\item All PIs expect to solicit guidance and collaboration from
  researchers such as John Feo (PNNL), David Callahan (Microsoft, recently moved to PNNL), Jim Larus (Microsoft Research), Hans
  Boehm (HP Research), and Preston Briggs,  with whom
  they have already collaborated on other projects.
\end{itemize}


\input{footer}


\section{Evaluation}
\label{sec:evaluation}

The goal of our evaluation is to show that the core pieces of the \Grappa
runtime system, namely our tasking system and the global memory/communication
layer work as expected and together are able to efficiently run irregular
applications. We evaluate \Grappa in three basic steps:

\begin{itemize}

\item We present results that show that \Grappa can
    support large amounts of concurrency, sufficient for remote memory
    access and aggregation. The communication layer is able to
    sustain a very high rate of global memory operations. We also show the performance of a
graph kernel that stresses communication and concurrency together.
\TODO{compare to key-value stores?}

\item We characterize system behavior, including
profiling where execution time goes, how aggregation affects message size and
rates, and \checkme{the relationship between aggregation and
    concurrency.}

\item Finally, we show how some more realistic irregular workloads on \Grappa
compare to the Cray XMT and hand-tuned MPI code.

\TODO{Grappa is well-suited to applications where there is
    abundant data parallelism but unpredictable or imbalanced data
    access. BSP is efficient where there is sufficient parallel slack.
    Although Grappa loses some asynchrony by relying heavily on
    aggregation, it still provides more flexibility for sections of the
    computation to proceed more quickly. }

\TODO{how much concurrency is needed on apps. Our basic plot of
    performance vs workers (with and without aggro?)}
\TODO{relationship between user-thread-visible latency and aggregate throughput.}
\TODO{characterization of aggregation message size vs nodes}
\TODO{show our scalability plots; if they don't scale, show
    specifically why. It is okay if its because aggregation stops
    working after a point if we can discuss future scalability.
    Perhaps reference ``MPI on millions of cores'' type work}

\end{itemize}

\subsection{Basic \Grappa Performance}
\label{eval:basic}

\paragraph{User-level context switching.}

Fast context switching is at the heart of \Grappa's
latency tolerance abilities. We assess context switch overheads using a simple
microbenchmark that runs a configurable number of workers on a single core, where each worker 
increments values in a large array. 

\begin{figure}[ht]
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figs/context_switch_time.pdf}
    \end{center}
    \caption{Average context switch time with and without prefetching.}
    \label{fig:context-switch-exp}
\end{figure}

Figure~\ref{fig:context-switch-exp} shows the average context switch
time as the number of workers grow. At our standard operating point
($\approx$1K workers), context switch time is on the order of
50ns. As we add workers, the time increases slowly, but levels off:
we also ran with 500,000 workers (10~times what is shown in the
figure) and found that context switch time was around 75ns. In
comparison, for the same yield test using kernel-level Pthreads on a
single core, the switch time is 450ns for a few threads and 800ns for
1000--32000 threads.

If we calculate aggregate context switch \emph{rate\/} of all
cores in a node, we find that with prefetching, \Grappa context
switching is
limited \emph{not\/} by memory latency, as normally assumed, but rather memory bandwidth.
Specifically, we empirically found that 4~cache lines (1~for worker
struct and 3 for stack data) was sufficient to avoid cache misses in
the microbenchmark. Every context switch then requires 8~cache-line transfers. The off-chip
bandwidth of a single socket in our system is 270M cache lines per
second~\cite{porterfield:bw,Nelson:hotpar11}. This implies that, in the limit,
we can sustain at most 34M context switches per second per socket (a context-switch time of 29ns).
\TODO{based on this and GUPS, our message rate will be limited by
    context switching for small+single messages. One experiment is
    GUPs with context switching, where we vary number of reads/suspend}

In summary, our context switch engine is able to efficiently sustain very high
concurrency and, as we will show later, the amount of concurrency sustained is
sufficient for the latencies \Grappa needs to hide.

\paragraph{Global memory and communication.} We measure the performance of
\Grappa's global memory and communication layers using a faithful
implementation of the giga updates per second (GUPs) benchmark, which
measures cluster-wide random access bandwidth.  Read-modify-write
updates are dispatched at random to a large array. This benchmark
stresses the communication layer of \Grappa separately from the
scheduler, because only a single worker is used per system node.
Figure~\ref{fig:grappa-gups} shows that \Grappa is able to sustain
well over a billion updates per second with 64~nodes. Note also that
when aggregation is turned off, the update rate is nearly
flat. Clearly aggregation is instrumental for good communication
performance.

This compares very favorably to published results~\cite{gups} for other
high-end HPC systems. Though the actual computation done by GUPS is not 
useful, irregular, data-intensive applications typically must be able to
sustain a high rate of random accesses in order to, for example, visit and
mark vertices during a graph traversal. High random access rate in a
distributed setting has been a long-standing challenge in HPC.

\begin{figure}[ht]
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figs/gups.pdf}
    \end{center}
    \caption{GUPS (giga updates per second) for \Grappa as the number of nodes grows with and without message aggregation.}
    \label{fig:grappa-gups}
\end{figure}

\TODO{go into more detail about what Grappa is doing when executing
    GUPs. Specifically note what the rate per core is, what the
    aggregation and deaggregation rates are. Show on the plot the
    message size distribution (some box plot)}

\paragraph{Putting it all together with Unbalanced Tree Search in memory
(UTS).} 
Figure~\ref{fig:grappa-uts} shows the overall performance of \Grappa running
UTS. This experiment demonstrates that \Grappa's context
switching and communication layers can be used together, while
balancing workload, to run an irregular application efficiently. 

Visiting vertices in the distributed tree requires mostly remote
accesses, and because each vertex in the tree must be visited before
it can be expanded, blocking remote reads are required. In this case,
we are forced to context switch to tolerate the remote access and
continue aggregating.

We look at two classes of trees, T1 and T3, from
the original benchmark. T1 trees are very shallow and wide (i.e., significant
parallel slack \TODO{cite Valiant and say more somewhere about BSP}), while T3 trees are very deep (i.e., little parallel slack).
When the access to each vertex is a random access, the critical path to search
T3 trees is very long, hence the low performance and scalability. On such
trees, we do not expect there to be sufficient concurrency for any system,
including \Grappa, to achieve high throughput -- at the 16-node data point,
the average active tasks per core over the search is 775 for T1 and
only 13 for T3.
On the other hand, \Grappa performs and scales very well for T1
trees.


\begin{figure}[ht]
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figs/uts_scale.pdf}
    \end{center}
    \caption{Vertices per second in UTS on \Grappa as the number of nodes grows.}
    \label{fig:grappa-uts}
\end{figure}

\subsection{Comparing \Grappa to Other Systems}
\label{eval:mainperf}

In order to put \Grappa's performance into a general context, we compare it
with XMT running BFS, PageRank, IntSort, GUPS and UTS. Since XMT is a
different hardware platform, we also compare \Grappa with optimized MPI
versions of BFS and GUPS running on the same hardware. Finally, we also
compare it with UTS written for UPC. We run all experiments with 64 nodes and 16 cores per node.

% \begin{figure}[ht]
%     \begin{center}
%       \includegraphics[width=0.5\textwidth]{results/benchmarks.pdf}
%     \end{center}
%     \caption{Comparing \Grappa with XMT, hand-tuned MPI and UPC.}
%     \label{fig:grappa-comparisons}
% \end{figure}

\begin{table}[htb]
\begin{center}
\begin{tabular}{l|c|c|c|c}
         & \Grappa & XMT   & MPI  & UPC \\ \hline
GUPS     & 1       & 2.23  & 0.11 & -- \\ 
BFS      & 1       & 1.63  & 3.52 & -- \\ 
Intsort  & 1       & 3.77  & 5.88 & -- \\
UTS (T1) & 1       & 0.38  & --   & 0.09 \\ 
Pagerank & 1       & 4.35  & 4.87 & -- \\ 
\end{tabular}
\end{center}
\caption{Comparing \Grappa with XMT, optimized MPI and UPC. Numbers are presented as throughput on 64 nodes normalized to \Grappa.}
\label{tab:grappa-comparisons}
\end{table}

Table~\ref{tab:grappa-comparisons} shows the results. \Grappa is able
to provide performance approaching and sometimes exceeding that of the
other systems. This data shows that, while \Grappa is good at the
operations for which it was designed, there is a cost to providing
\Grappa's generality. In general, the benchmarks whose MPI
implementations are faster than \Grappa include a implementation of a
subset of \Grappa's functionality specialized for their particular
problem. We now discuss each benchmark in more detail.

\paragraph{UTS.}
It is perhaps not surprising that the XMT is faster than
\Grappa on many of the benchmarks; after all, the XMT is custom
hardware optimized for irregular applications. The XMT can
context-switch every cycle, while \Grappa takes $~$50 ns. The XMT can
complete 100 million random-access remote reads per second per node in
hardware, while \Grappa must spend many instructions managing and
aggregating each of these reads in software.

What is more surprising is that Grappa is able to exceed the XMT's
performance on UTS. This is a demonstration of the inflexibility of
custom hardware. Recall that the UTS benchmark searches an unbalanced
tree and spawns a data-dependent number of child tasks at each
vertex. The XMT's spawn/join semantics, whose implementation is spread
across the hardware, OS, and runtime, require a task visiting a vertex
to block until all its child tasks have completed, even though the
benchmark does not require those tasks to have completed until the end
of the tree search. Furthermore, each new task is immediately made
available to all other processors in the system, even though it is
likely that processor that spawned a task will end up executing it.

Since \Grappa's tasks are implemented entirely in software, it is easy
to support arbitrary synchronization semantics. \Grappa allows both
XMT-like per-vertex task joins as well as whole-problem joins that are
a better fit for UTS. Furthermore, \Grappa's work-stealing distributed
task queue and recursive loop decomposition encourage locality in task
execution, so that the system is optimized for the common case of a
processor executing a task it spawned.

The UPC UTS implementation includes an implementation of work-stealing
but has limited ability to overlap communication with
computation. Furthermore, it must execute heavyweight synchronization
operations when stealing. \Grappa benefits from its lightweight
context switching and support for fine-grained synchronization.

\paragraph{BFS.}
The Grappa and MPI BFS implementations are quite similar. They both
use the same graph representation, and they both depend on aggregation
for performance. The difference is that the BFS' aggregation is
integrated with the traversal code and specialized for the problem:
since the only messages required are edge traversals, the aggregation
buffers store only pairs of 8-byte vertex IDs.  In contrast, \Grappa
pays execution and space overhead in order to support aggregation of
arbitrary closures; the messages \Grappa aggregates include the two
8-byte vertex IDs along with 8 bytes of deserialization information as
well as 8 bytes of synchronization information. Furthermore, the MPI
implementation writes the edge information directly into the
aggregation buffer while \Grappa executes additional code to serialize
and deserialize the messages. \TODO{Include message counts, size, and
  rates; code size too?}.

\paragraph{GUPS.}
The MPI GUPS implementation includes an implementation of aggregation,
but it is not optimized to work when the aggregated data exceeds
cache, and it has limited support for concurrent communication with
multiple destinations. \Grappa benefits from being optimized for both
of these cases.

\paragraph{Pagerank.}
The MPI Pagerank implementation it built on top of the
highly-optimized sparse matrix support in the Trilinos~\cite{trilinos}
library. In contrast, the \Grappa implementation is fairly
straightforward. 

\paragraph{Intsort.}
\TODO{Describe Intsort}

\paragraph{Summary.}

Overall, \Grappa provides a general programming model at a moderate
performance cost. While \Grappa's core functionality performs well,
applications can specialize similar functionality for their problems
and obtain better performance than \Grappa on the same hardware. This
is, however, not the end of the story for \Grappa's performance;
\Grappa is a young library and, as discussed in the next section, is
limited by its implementation rather than the hardware on which it
runs. We believe there are opportunities for optimization that will
improve its performance.

\subsection{Characterization}

\paragraph{Where execution time goes.}


\begin{figure}[ht]
    \begin{center}
      \begin{tabular}{c|c c c c}
        Benchmark     & Comm & User & Idle & Sched \\ \hline
        GUPS          & 62.9  & 18.9    & 15.4 & 2.80 \\ 
        BFS           & 54.84 & 30.90   & 10.94 & 3.43 \\ 
        Intsort       & 34.28 & 42.00   & 21.31 & 2.47 \\ 
        UTS           & 40.57 & 56.52   &  1.21 & 1.73 \\
        Pagerank      & 76.79 & 20.71   &  0.06 & 2.50 \\
      \end{tabular}
    \end{center}
    \caption{\Grappa\ execution time profile, in percent.}
%bar chart, one set of bars per benchmark, one bar per system. runtime normalized to grappa.
    \label{fig:grappa-profile}
\end{figure}
\TODO{ shorten the execution profile section in favor of deeper
    discussion about where the message rate comes from}

Figure \ref{fig:grappa-profile} shows the breakdown of execution
time. Since \Grappa is focused on frequent communication, it is not
surprising that many of our benchmarks spend more than half of their
time doing communication. User time is primarily focused on generating
more requests; idle time occurs when we run out of parallel work or
out of workers to execute that work. Scheduler time includes both
context switch time and deciding what task to run next. Our efficient
context switching allows us to keep this to a few percent of execution
time even with thousands of threads per core.

\paragraph{Message size and latency.}

To characterize the effectiveness of aggregation, we collected
histograms of aggregated message sizes and application memory request
latency for BFS and UTS (Figure \ref{fig:grappa-message-size}).


%%% I couldn't make this paragraph useful in the limited time we have.
%%% I think it's ok without. -Mark
\grappacomment{Recall that global memory can be accessed in two ways. For requests of
locations that are expected to shared, delegate operations are
used. For requests of non-shared locations where locality can be
exploited, explicit cache operations are used. We see this reflected
in the fraction of requests that are handled remotely: less than one
percent of delegate operations in both BFS and UTS access local
data. Since explicit cache operations are intended to exploit
locality; they more often access data on the local node. Only 20\%
of UTS explicit cache operations are remote, whereas 44\%
of BFS explict cache operations are remote.}

In UTS, we obtained the best throughput by flushing frequently: on the
order of 100$\mu$s. This, combined with the fewer number of remote
accesses, means that there are a limited number of requests generated
per flush interval, leading to packets primarily smaller than~10KB.

In BFS, we see a trimodal distribution. There are two reasons for
this. First, in BFS, we obtained the best throughput by flushing at
least 500$\mu$s apart, and also flushing whenever a core is
idle. Since this process ignores whether another flush has occurred
recently, it leads to a large number of small messages. This leads to
the first and smallest mode of the distribution.  The other two modes
are due to a locality optimization in the \Grappa version of BFS. To
minimize unnecessary remote communication, each core visits vertices
whose edge lists are stored locally. Since we are traversing power-law graphs, a small fraction of vertices have a very high number of edges. This means that occasionally a core visits a high-degree vertex which allows it to very quickly generate a large batch of requests. This interacts well with aggregation; when we find high degree vertices, we are able to aggregate packets on the order of 40KB. In flush intervals without high-degree vertices, we see packets on the order of 8KB.

Figure \ref{fig:grappa-latency} shows the latency distribution of
remote delegate operations. This distribution follows the aggregated
message size distribution: when we form larger packets, the system
takes longer to process them.


\begin{figure}[ht]
    \begin{center}
      \includegraphics[width=0.5\textwidth]{results/histograms/latency_cmb.pdf}
    \end{center}
    \caption{Round-trip latency of delegate operations.}
%bar chart, one set of bars per benchmark, one bar per system. runtime normalized to grappa.
    \label{fig:grappa-latency}
\end{figure}

\begin{figure}[ht]
    \begin{center}
      \includegraphics[width=0.5\textwidth]{results/histograms/rdma_bytes_sent_histogram_cmb.pdf}
    \end{center}
    \caption{Distribution of aggregated message sizes.}
%bar chart, one set of bars per benchmark, one bar per system. runtime normalized to grappa.
    \label{fig:grappa-message-size}
\end{figure}



















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% OLD %%%%%%%%%%%%%%%%%%

\grappacomment{


Our evaluation begins with presentation of microbenchmark results, establishing
the intrinsic potential of \Grappa to provide random access bandwidth and latency tolerance. 
Next, we present application results, both for \Grappa and for other paradigms, as well
as comparing against the Cray XMT.  Finally, we present the impact of increased aggregation delay
on \Grappa results, thus exploring robustness to network scale.

\subsection{Microbenchmark Results}

\subsubsection{Random Access}
\TODO{Random access feed forward results on \Grappa.  optional: Results we measured for XMT.  Cite MPI results}
\subsubsection{Latency Tolerance}
\TODO{Simple ping test results -- eg, MPI ping, not the full blown aggregation ping test.  Random access blocking results on \Grappa.}
\subsubsection{Scheduling and Robustness}

\begin{figure}[ht]
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figs/context_switch_time.pdf}
    \end{center}
    \caption{Average context switch time with 1 and 6 active cores,
        with and without prefetching.}
    \label{fig:context-switch-time}
\end{figure}

\begin{figure}[ht]
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figs/context_switch_bw.pdf}
    \end{center}
    \caption{Context switch rate with prefetching. Once the total
        size of contexts sufficiently exceeds last-level cache, most
    prefetches go to main memory and the rate becomes limited to the
    off-chip bandwidth.}
    \label{fig:context-switch-time}
\end{figure}

\TODO{Summary of what to write please expand: Reference above plots. Bandwidth of single socket is
    270Mcacheline/s. Each context is 4 cachelines: 1 for worker struct
    and 3 stack cachelines. We must read and write every context, so 8
    cacheline transfers per context switch. This asymptotically
    approaches 33.75Mcontexts/s as you increase the number of contexts
    (fewer and fewer in L3). Note that only 4+ cores reaches full rate
    because these westmere chips are balanced to not achieve full off-chip bandwidth until 4
    cores--bdmyers}

\TODO{Yield test results:  latency \& bandwidth.  Discussion of implications of zillions of contexts on robustness of latency tolerance preshadowing ~\ref{sec:scaling}}

\subsection{Application Results}
To evaluate \Grappa's performance with respect to the XMT, we ran each
of our three benchmarks on up to 16 nodes of each machine. \Grappa used
6 cores per node, with the best parameters chosen for each point. In
some cases, the XMT could not run the benchmark with 2 nodes, so the
point is omitted. \TODO{rewrite this!}
\subsubsection{Unbalanced Tree Search}\TODO{rewrite with new results}
%% UTS: performance comparison
\begin{figure}[ht]
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figs/uts_scale.pdf}
    \end{center}
    \caption{Performance of in-memory unbalanced tree search.}
    \label{fig:uts_compare}
\end{figure}

We ran UTS-mem with a geometric 100M-vertex tree
(T1L). Figure~\ref{fig:uts_compare} shows the performance in terms of
number of vertices visited per second versus number of compute
nodes. \Grappa is 3.2 times faster than the XMT at 16 nodes.  As we will show later, the performance advantage \Grappa has over XMT increases as more nodes are added.  The main reason \Grappa performs better is the software-based delegate synchronization obviates the need for the retry-based synchronization that XMT uses.

\subsubsection{Breadth First Search}\TODO{rewrite with new results}
\begin{figure}[tH]
\begin{center}
  \includegraphics[width=0.95\columnwidth]{figs/bfs_performance}
\begin{minipage}{0.95\columnwidth}
  \caption{\label{fig:bfs-performance} BFS performance}
\end{minipage}
\vspace{-3ex}
\end{center}
\end{figure}

We ran BFS on a synthetic Kronecker graph with $2^{25}$ vertices and
$2^{29}$ edges (25 GB of data). Figure~\ref{fig:bfs-performance} shows
our performance in terms of graph edges traversed per second. The XMT
is 2.5 times faster than \Grappa at 16 nodes.  Performance does scale at a constant rate for \Grappa, suggesting that adding more nodes will increase performance.

\subsubsection{Approximate Betweenness Centrality}\TODO{rewrite with new results}
\begin{figure}[tH]
\begin{center}
  \includegraphics[width=0.95\columnwidth]{figs/centrality_performance}
\begin{minipage}{0.95\columnwidth}
  \caption{\label{fig:centrality-performance} Centrality performance}
\end{minipage}
\vspace{-3ex}
\end{center}
\end{figure}

We ran Betweenness Centrality on the same scale 25 Kronecker graph as
we did for BFS. Figure~\ref{fig:centrality-performance} shows our
performance in terms of graph edges traversed per second. At 16 XMT
processors/cluster nodes, the XMT is 1.75 times faster than \Grappa.

% Got rid of this discussion of UTS by itself, tried to work the highlights in below
%\paragraph{UTS-Mem}
%We ran UTS-Mem on \Grappa and the XMT with a geometric 1.6B-vertex tree
%(T1XL) and a geometric 4.2B-vertex tree (T1XXL), using up to 128
%nodes -- the maximum we had available for each. \Grappa results are for 5 cores per node. \Grappa with 20 machines is faster than the entire XMT of 128 processors.

%\Grappa achieves \checkme{188Mvert/s} with 128 nodes and the XMT
%achieves only 50Mvert/s, plateauing at 60 nodes. Beyond 90 nodes, \Grappa adds 1.4 Mvert/s/node.
%The XMT scales at 850 Kvert/s/node, until it plateaus. \Grappa keeps
%scaling up through 128 nodes, although scaling
%declines because of the unscalability of our aggregation mechanism as
%number of network endpoints increases. 
%
%Despite our efforts to tune the UTS implementation specific to the 
%XMT, performance does not scale well with increasing processor count,
%flattening out around 60 processors.  When we increase the size of
%the tree from 100M to 4.2B, we find that performance does not improve,
%suggesting that performance is not limited by task parallelism.
%Cray's performance tools show an increasing number of memory
%retry operations for failed synchronization operations generated by
%the runtime, which create network contention.
%
To determine how \Grappa's performance scales compared to the performance of the entire XMT, we ran a set of experiments up to all 128 XMT processors and 128 cluster nodes. For the XMT, the number of allowed processors was varied up to the entire machine, with some minor tuning of stream parameters needed to get optimal performance. For \Grappa, parameters such as cores per node, aggregator timeouts, and parallel threshold were tuned to get the best performance for each node count. All of the benchmarks continue to improve out to 128 nodes for \Grappa. UTS continues to fare better than the XMT with large node counts, with the XMT appearing to plateau at 60 processors due to contention from synchronization retries, while \Grappa handles this by suspending tasks until messages return. For BFS and Centrality, the XMT scales approximately a constant factor better than \Grappa. We attribute this to a limitation in the current aggregator design and network stack that \Grappa uses.  This limits the practical number of cores we can use to 6 per node (adding more cores per node \emph{decreases\/} performance).  Ironically, this limitation makes \Grappa applications compute-bound instead of network-bound.  Work is ongoing to rework the Infiniband driver stack and aggregation interface to remove this limitation and improve aggregation addressing using local routing.

\begin{figure}[ht]
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figs/scaling_cropped.pdf}
    \end{center}
    \caption{Scaling number of nodes: \Grappa continues to perform significantly better than XMT for UTS but scales a constant factor slower than XMT for BFS (4x slower) and Centrality (2x slower). }
    \label{fig:uts_threshold}
\end{figure}

\subsection{Scaling}\label{sec:scaling} \TODO{rewrite with new results}

\subsubsection{Network Aggregation Performance and Robustness}

\begin{figure}[htb]
\begin{center}
  \includegraphics[width=0.95\columnwidth]{figs/aggregator_ping}
\begin{minipage}{0.95\columnwidth}
  \caption{\label{fig:aggregator-ping} Bandwidth versus message size
    unidirectional ping test for \Grappa with aggregation, \Grappa with
    raw GASNet messages, and MPI. Aggregation provides an 11x
    bandwidth benefit at our common operating point.}
\end{minipage}
\vspace{-3ex}
\end{center}
\end{figure}

To evaluate the benefits of network aggregation, we ran two experiments.
First, we ran a simple unidirectional ping test to see the maximum
benefit the aggregator can provide in terms of improved network
efficiency. Second, we ran BFS with the aggregator disabled in order to
measure its benefit on an application.

To implement the ping test, we wrote a simple \Grappa application where
the cores of one node send messages as fast as possible to the cores
of another node. We vary the size of the payload up the maximum
payload size supported by the aggregator (nearly 4KB). Each core has a
single task sending to a single destination, so this is a best case
scenario for the aggregator. To see the benefit of the aggregator, we
added a bypass that lets us send messages directly through GASNet. We
also compare against the OSU \texttt{osu\_mbw\_mr} benchmark
\cite{osu:mpi}  compiled against OpenMPI 1.5.3; this
benchmark has the same pattern of communication but doesn't have the
overhead of \Grappa's context switching.

The results are shown in Figure~\ref{fig:aggregator-ping}. There are
two key observations.

First, small message performance against the existing libraries is, as expected, poor. The MPI application test shows us that peak per node
bandwidth supported by our infiniband card is 3.4GB/s. This is
achievable only with large messages; we must send 16KB packets to get
within 5 percent of peak bandwidth. But in our benchmarks, we saw
average message between 32 and 64 bytes. At 32 bytes, the MPI test is
using less than 7 percent of its peak bandwidth. \Grappa sending
messages directly through GASNet uses less than 3 percent of the peak
bandwidth.

Second, aggregation has the potential to improve this situation by an
order of magnitude. With aggregation, \Grappa is able to send 32-byte
messages over 12 times faster than using GASNet directly. This is a
more respectable 32 percent of peak bandwidth. Due to expedient design
decisions, \Grappa's aggregator limits its aggregation to 4KB; this
limits its peak achievable bandwidth to 75 percent of the actual
peak.

This comparison is the best possible case for the aggregator. In
order to verify that the aggregator still has value on actual
applications at scale, we ran a small (100M node tree) UTS-Mem
with the aggregator disabled, on 16~nodes.
Figure~\ref{fig:no-aggregation-uts}. At this configuration, the aggregator
improves our application performance by 10x. 

\begin{figure}[htb]
\begin{center}
  \includegraphics[width=0.95\columnwidth]{figs/no_aggregation_uts.pdf}
\begin{minipage}{0.95\columnwidth}
  \caption{\label{fig:no-aggregation-uts} Performance of UTS on 16
      nodes with and without \Grappa's aggregation.}
\end{minipage}
\vspace{-3ex}
\end{center}
\end{figure}

\subsection{Scaling}
% Got rid of this discussion of UTS by itself, tried to work the highlights in below
%\paragraph{UTS-Mem}
%We ran UTS-Mem on \Grappa and the XMT with a geometric 1.6B-vertex tree
%(T1XL) and a geometric 4.2B-vertex tree (T1XXL), using up to 128
%nodes -- the maximum we had available for each. \Grappa results are for 5 cores per node. \Grappa with 20 machines is faster than the entire XMT of 128 processors.
%\Grappa achieves \checkme{188Mvert/s} with 128 nodes and the XMT
%achieves only 50Mvert/s, plateauing at 60 nodes. Beyond 90 nodes, \Grappa adds 1.4 Mvert/s/node.
%The XMT scales at 850 Kvert/s/node, until it plateaus. \Grappa keeps
%scaling up through 128 nodes, although scaling
%declines because of the unscalability of our aggregation mechanism as
%number of network endpoints increases. 
%
%Despite our efforts to tune the UTS implementation specific to the 
%XMT, performance does not scale well with increasing processor count,
%flattening out around 60 processors.  When we increase the size of
%the tree from 100M to 4.2B, we find that performance does not improve,
%suggesting that performance is not limited by task parallelism.
%Cray's performance tools show an increasing number of memory
%retry operations for failed synchronization operations generated by
%the runtime, which create network contention.
%
To determine how \Grappa's performance scales compared to the performance of the entire XMT, we ran a set of experiments up to all 128 XMT processors and 128 cluster nodes. For the XMT, the number of allowed processors was varied up to the entire machine, with some minor tuning of stream parameters needed to get optimal performance. For \Grappa, parameters such as cores per node, aggregator timeouts, and parallel threshold were tuned to get the best performance for each node count. All of the benchmarks continue to improve out to 128 nodes for \Grappa. UTS continues to fare better than the XMT with large node counts, with the XMT appearing to plateau at 60 processors due to contention from synchronization retries, while \Grappa handles this by suspending tasks until messages return. For BFS and Centrality, the XMT scales approximately a constant factor better than \Grappa. We attribute this to a limitation in the current aggregator design and network stack that \Grappa uses.  This limits the practical number of cores we can use to 6 per node (adding more cores per node \emph{decreases\/} performance).  Ironically, this limitation makes \Grappa applications compute-bound instead of network-bound.  Work is ongoing to rework the Infiniband driver stack and aggregation interface to remove this limitation and improve aggregation addressing using local routing.

\begin{figure}[ht]
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figs/scaling_cropped.pdf}
    \end{center}
    \caption{Scaling number of nodes: \Grappa continues to perform significantly better than XMT for UTS but scales a constant factor slower than XMT for BFS (4x slower) and Centrality (2x slower). }
    \label{fig:uts_threshold}
\end{figure}


\subsection{Sensitivity}

\paragraph{Aggregator timeout}

\begin{figure}[htb]
\begin{center}
  \includegraphics[width=0.95\columnwidth]{figs/flushticks_sweep}
\begin{minipage}{0.95\columnwidth}
  \caption{\label{fig:bfs-sweep-flushticks} Sensitivity to aggregation delay}
\end{minipage}
\vspace{-3ex}
\end{center}
\end{figure}


One of the key parameters of the aggregator is the message
timeout. All messages that are queued must eventually be sent in order
to ensure progress. In the best case, we are able to aggregate enough
messages to fill an aggregation buffer and cause it to be sent, but as
we scale up, the average rate of messages heading to a common
destination decreases, and this gets harder. To bound the problem, the
aggregator includes a timeout. Any packet waiting this long is sent
the next time the communications layer is serviced.

Figure~\ref{fig:bfs-sweep-flushticks} shows a sweep of this parameter
for UTS, BFS, and Betweenness Centrality on 16 nodes, using the
datasets described previously. The maximum number of workers is fixed
at 2048. All the benchmarks show a performance peak with a 2
millisecond timeout; at this point we are delaying long enough to
aggregate the largest packets we can; setting the parameter higher
causes tasks to wait longer for responses, but few new requests are
being generated.


\begin{figure}[htb]
\begin{center}
  \includegraphics[width=0.95\columnwidth]{figs/worker_sweep}
\begin{minipage}{0.95\columnwidth} 
  \caption{\label{fig:bfs-sweep-workers} Sensitivity to maximum active tasks}
\end{minipage}
\vspace{-3ex}
\end{center}
\end{figure}

\subsubsection{Number of active tasks} \TODO{possibly eliminate}

When a task issues a request that requires a response, it blocks to
allow other tasks to utilize its core. These tasks may also block. To
support the many milliseconds of latency aggregation adds, we need to
support many thousands of blocked tasks. One of the key parameters of
the runtime is the number of blocked tasks allowed; we need enough to
cover the network and aggregation latency, but too many running tasks
can add extra latency as they all must be multiplexed onto the same core.

Figure~\ref{fig:bfs-sweep-workers} shows a sweep of the maximum number
of active tasks (workers) per core for each of our three benchmarks on
16 nodes. The aggregator timeout is set at 1 ms for UTS and 2 ms for
BFS and Betweenness Centrality. The performance peak shifts in this
case, with UTS peaking at 1536 workers, BFS peaking at 2048 workers,
and Betweenness Centrality peaking at 3072 workers. This is the point
where we have enough workers to cover the latency of aggreation. The
different values reflect the different amounts of work done by a task
in each benchmark; UTS does the least, while Betweenness Centrality does the most.

%\subsubsection{Work stealing parameters}
%
%\paragraph{Chunk size}
%
%It is important to steal multiple tasks at a time to both amortize the
%cost of stealing over the network and to spread out work quickly in a
%large system. Figure~\ref{fig:ut_chunksize} shows performance and
%stealing statistics for UTS on \checkme{30} nodes as we increase the stealing chunk size. Recall
%that a thief will take a number of tasks equal to the minimum of half
%the available work or the chunk size; steals fail only when the victim
%has fewer than 2 available tasks. As the scheduler is allowed to
%steal more work beyond 1 task, we see that performance increases up to 6x. This
%shows that the heuristic of stealing the oldest task from victims is
%insufficient alone when a tree-structured computation is imbalanced,
%as observed in \cite{UTS}. By observing sampled state in the execution
%trace, we find that a chunk size as low as 1 allows stealing to
%spread the load evenly across the cluster but cores spend much time
%underutilized as multiple workers wait for steal replies that
%utlimately return little new work.
%
%Performance plateaus before maximum steal amount is limited by the
%size of the victim's task queue. This indicates that artificially limiting steals
%to \checkme{128} tasks does not limit performance. Although a lower
%chunk size limits how quickly work spreads, for sufficient chunk size,
%the heuristic of stealing the oldest tasks from victims in tree-based computations allows for
%stolen work to expand quickly.


%% UTS: chunk size
%\begin{figure}[ht]
%    \begin{center}
%      \includegraphics[width=0.5\textwidth]{figs/uts_chunksize.pdf}
%    \end{center}
%    \caption{Performance of UTS-Mem with varying maximum chunk size of
%    steals, run with 30 nodes, 6 cores per node, 4000 workers,
%    \checkme{6M flush ticks}}
%    \label{fig:uts_chunksize}
%\end{figure}


%\TODO{(difference with BFS)}



\subsubsection{Parallel loop threshold}

%\begin{figure}[htb]
%\begin{center}
%  \includegraphics[width=0.95\columnwidth]{figs/bfs_sweep_threshold}
%\begin{minipage}{0.95\columnwidth}
%  \caption{\label{fig:bfs-sweep-threshold} Sensitivity to parallel loop threshold. Note the log scale.}
%\end{minipage}
%\vspace{-3ex}
%\end{center}
%\end{figure}

Parallel overhead -- in the form of context switches, task spawns, and
synchronization -- can reduce the performance benefit of parallelism.
\Grappa sees a benefit to limiting the amount of parallelism created by
a recursive loop decomposition. The parallel loop threshold (``parallel
granularity'') parameter tells the runtime when to stop creating new tasks and just execute iterations
sequentially. This allows us to amortize the overhead of task
creation. In addition, assigning sequential iterations to a single
task provides the potential to exploit locality when data for adjacent iterations is
also adjacent in memory. The ability to exploit this locality that
exists in the application is an important advantage. We found that in UTS and BFS, increasing the
threshold from 1 up to 8 or 16, respectively, increases performance by
more than 60\%.


% uts threshold
%\begin{figure}[ht]
%    \begin{center}
%      \includegraphics[width=0.5\textwidth]{figs/uts_threshold.pdf}
%    \end{center}
%    \caption{Performance of UTS-Mem with varying parallel loop
%        threshold, run with 30 nodes, 6 cores per node, 4000 workers,
%    \checkme{6M flush ticks}}
%    \label{fig:uts_threshold}
%\end{figure}

\subsection{Summary}\TODO{this is a placeholder: do we need this section?}

\paragraph{Context-switch overhead.} Should we also compare with other packages? (Maybe Capriccio, QThreads?, or even real OS threads?)

\paragraph{Latency.} Measure remote data access latency with and without aggregation turned on.

\paragraph{Aggregated message sizes.} Characterization of the resulting message sizes with aggregation. Right now we only have message size vs. bandwidth.

\paragraph{Utilization.} CPU utilization, Memory, Network. It would be great to answer the question of where is our bottleneck right now. Amount of concurrency with and without aggregation. 

\paragraph{Memory accesses.} Rate of accesses to remote data. Rate of delegate ops. Show limit with GUPs.



%Retries are performed by the memory controller when remote synchronization operations fail to find the full-bit associated with each memory location in the unavailable state.  Retries are issued at low priority relative to new memory operations issued by the processor by other contexts, so they consume what would otherwise be unused injection bandwidth.  On a full-bandwidth system such as the MTA-2, retries have no impact on the progress of tasks other than their own.  On a Cray XMT, network bandwidth is limited, so retries create congestion.  In comparison, \Grappa performs synchronization without retries, delaying responses at the receiving end until ready to notify the sender to proceed.  This saves bandwidth and permits scaling of tasks performing synchronization even on low injection rate networks.


%\begin{figure*}[ht]
%    \begin{minipage}{0.3\linewidth}
%        \centering
%        \includegraphics[width=\textwidth]{figs/chunksize-uts.pdf}
%        \caption{chunksize caption}
%        \label{fig:chunksize-uts}
%    \end{minipage}
%    \begin{minipage}{0.3\linewidth}
%        \centering
%        \includegraphics[width=\textwidth]{figs/workers-uts.pdf}
%        \caption{workers caption}
%        \label{fig:workers-uts}
%    \end{minipage}
%    \begin{minipage}{0.3\linewidth}
%        \centering
%        \includegraphics[width=\textwidth]{figs/thresh-uts.pdf}
%        \caption{threshold caption}
%        \label{fig:thresh-uts}
%    \end{minipage}
%\end{figure*}

}

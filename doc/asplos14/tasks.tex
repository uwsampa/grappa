\section{Tasking System}

Below we discuss the implementation of our task management support and then
describe how applications expose parallelism to the \Grappa runtime.

\subsection{Task Support Implementation}

The basic unit of execution in \Grappa is a {\em task}. When tasks are ready to
execute, they are mapped to a {\em worker,} which is akin to a user-level
thread. Each hardware core has a single operating system thread pinned to it.

\paragraph{Tasks} 
Tasks are specified by a closure (or ``function object'' in C++ parlance) that holds both code to execute and initial state. The functor can be specified with a function pointer and explicit arguments, a C++ struct that overloads the parentheses operator, or a C++11 lambda construct. These objects, typically very small (on the order of 64 bytes), hold read-only values such as an iteration index and pointers to common data or synchronization objects. Task functors can be serialized and transported around the system, and eventually executed by a worker, as described next.

\paragraph{Workers} Workers execute application and system (e.g.,
communication) tasks. A worker is simply a collection of status bits and a
stack, allocated at a particular core. When a task is ready to execute it
is assigned to a worker, which executes the task functor on its own stack. 
Once a task is mapped to a worker it stays with that worker until it finishes.

\paragraph{Scheduling} During execution, a worker yields control of its core
whenever performing a long-latency operation, allowing the processor to
remain busy while waiting for the operation to complete. In addition, a
programmer can direct scheduling explicitly.
% via the \Grappa API calls shown in
% Figure~\ref{fig:scheduling}. 
To minimize context-switch overhead, the \Grappa scheduler
operates entirely in user-space and does little more than store state of one
worker and load that of another. When a tasks encounters a long-latency operation, its worker is suspended and subsequently woken when the operation completes. 

%% Maybe the sentence above is enough for the TODOs below.
%
% \TODO{mention how we block on response (``scheduler activations'' or whatever) not busy wait}
% 
% \TODO{either explain the usage of yield vs suspend in talking about
%     short and longer latency tolerance or eliminate the
%     distinction}



%% \begin{figure}[htbp]
%%   \begin{center}
%%   \begin{minipage}{0.95\columnwidth}  
%%   \begin{description}
%%     \item[\texttt{yield() }] \hfill \\
%%       Yields core to scheduler, enqueuing caller to be scheduled again soon
%%     \item[\texttt{wake( Worker * $w$ ) }] \hfill \\
%%       Enqueues some other worker $w$ to be scheduled again soon
%%     \item[\texttt{suspend() } ] \hfill \\
%%       Yields core to scheduler, enqueuing caller only once another task calls wake
%% 	\end{description}
%%   \end{minipage}
%%     \begin{minipage}{0.95\columnwidth}
%%       \caption{\label{fig:scheduling} \Grappa scheduling API
%%       \TODO{Drop this? Doesn't seem non-obvious or important.}}
%%     \end{minipage}
%%   \end{center}
%% \end{figure}

Each core in a \Grappa system has its own independent scheduler. The scheduler
has a collection of active workers ready to execute called the {\it ready
worker queue}. Each scheduler also has three queues of tasks waiting to be
assigned a worker:

\begin{description}

\item[deadline task queue] a priority queue of tasks that are executed according to task-specific deadline constraints;

\item[private task queue] a queue of tasks that must run on
this core and is therefore not subject to stealing;

\item[public task queue] a queue of tasks that are
  waiting to be matched with workers. It is a local partition of a shared
  task pool.

\end{description}
Whenever a task yields, the scheduler makes a decision about what
to do next. First, any task in the deadline task queue whose deadline
is imminent is chosen for execution. This queue manages high priority
system tasks, such as periodically servicing communication requests. Second,
the scheduler determines if any workers with running tasks are ready to
execute; if so, one is scheduled. Finally, if no workers are ready to
run, but tasks are waiting to be matched with workers, an idle worker is
woken (or a new worker is spawned), matched with a task, and scheduled.

\paragraph{Context switching} 
\Grappa context switches between workers non-preemptively. As with other
cooperative multithreading systems, we treat context switches as function
calls, saving and restoring only the callee-saved state as specified in the
x86-64 ABI~\cite{amd64:abi:2012}. This involves saving six general-purpose
64-bit registers and the stack pointer, as well as the 16-bit x87 floating
point control word and the SSE context/status register. Thus, the minimum
amount of state a cooperative context switch routine must save, according to
the ABI, is 62~bytes.

Since \Grappa keeps a very large number of active workers, their context data
will not fit in cache. By oversubscribing on the number of workers
beyond what is required for local DRAM latency tolerance, the scheduler can
ensure there is always some number of context pointers in the ready
queue. This allows the scheduler to prefetch contexts into
cache using software prefetch instructions; the size of the L1 cache is
sufficient to hold enough contexts to tolerate the latency to main
memory. Empirically we find that prefetching the fourth worker in the scheduling order is
sufficient. This prefetching constrains the types of task scheduling
decisions that can be made but makes context switching effectively
free of cache misses, even to hundreds
of thousands of workers. We provide an analysis of our context switch
performance in Section~\ref{eval:basic}.


\paragraph{Work stealing} 
When the scheduler finds no work to assign to its workers, it commences to
steal tasks from other cores using an asynchronous \texttt{call\_on} active
message. It chooses a victim at random until it finds one with a non-zero
amount of work in its public task queue. The scheduler steals half of the
tasks it finds at the victim. Work stealing is particularly interesting in
\Grappa since performance depends on having many active worker threads on each
core. Even if there are many active threads, if they are all suspended on
long-latency operations, then the core is underutilized. 
The
stealing policy 
must predict whether local tasks will likely
generate enough new work soon; a similar problem is addressed in
\cite{vanNieuwpoort:2001}.

\subsection{Expressing Parallelism}

% \TODO{addressing reviewer2: Grappa provides mechanisms for many
%     programming idioms. Also, may parallelism to
%     give flexibility to the implementation.}

\Grappa programmers focus on expressing as much parallelism as possible
without concern for where it will execute. \Grappa then chooses where and when
to exploit this parallelism, scheduling as much work as is necessary on each
core to keep it busy in the presence of system latencies and task dependences.

\Grappa provides three methods for expressing parallelism, shown in
Figure~\ref{fig:expressing-parallelism}. First, when the programmer identifies
work that can be done in parallel, the work may be wrapped up in a function
and queued with its arguments for later execution using a \texttt{spawn}.
Second,
the programmer can invoke a parallel for loop with \texttt{parallel\_for}, provided that the trip count is
known at loop entry. The programmer specifies a function pointer along with
start and end indices and an optional threshold to control parallel overhead.
\Grappa does {\em recursive decomposition} of iterations, similar to Cilk's
cilk\_for construct~\cite {cilkforimplementation}, and TBB's {\tt
parallel\_for}~\cite{intel_tbb}. It generates a logarithmically-deep tree of
tasks, stopping to execute the loop body when the number of iterations is
below the required threshold. Third, a programmer may want to execute an active message~\cite{vonEicken92}; that is, to run a
small non-blocking piece of code on a particular core in the system without waiting for
 a free worker to become available. For example, custom synchronization primitives are implemented as an active message executed on the core where the data
lives. \Grappa provides the \texttt{call\_on} routine for this purpose.

\begin{figure}[htbp]
  \begin{center}
	\begin{description}
    \item[\texttt{spawn( Functor f )}] \hfill \\
      Creates a new stealable task
    \item[\texttt{parallel\_for( start, end, Functor iteration )}] \hfill \\
      Executes iterations of a loop as stealable tasks that
      take the iteration index as an argument
    \item[\texttt{call\_on( core, Functor f )}] \hfill \\
      Runs a limited function on a specific core without
      consuming a stack or context switching
	\end{description}
    \begin{minipage}{0.95\columnwidth}
      \caption{\label{fig:expressing-parallelism} \Grappa API: expressing parallelism
      } % \vspace{-4ex}}
    \end{minipage}
    %\vspace{-3ex}
  \end{center}
\end{figure}

Figure~\ref{fig:sample} shows sample code using \Grappa for a parallel tree
search. The important aspect to note is that the code looks very similar to
what would be written for single shared-memory system, without any concern about data locality or communication.

\begin{figure}[htbp]
\begin{center}
\begin{scriptsize}
\begin{lstlisting}[style=grappa]
class node_t {
  key_t   key
  int64_t numChildren;
  GlobalAddress<node_t> children;
};

void search(GlobalAddress<node_t> node, key_t key, GlobalAddress<node_t> result) {
  // deref of key is expected cache miss
  if (node->key == key) {
    delegate::write(result, node);
  } else {
    parallel_for(0, node->numChildren,
    [=](int index) {
      // first deref of children is likely miss
      search(node->children[index], key); 
    });
  }
}
\end{lstlisting}
\end{scriptsize}

    \begin{minipage}{0.95\columnwidth}
      \caption{\label{fig:sample} Sample \Grappa code illustrating a
          parallel tree search similar to the unbalanced tree search
          benchmark we describe later. The labeled cache misses occur
          if we assume the tree is not layed out in a regular fashion, which
      models more general graphs.}
    \end{minipage}

\end{center}
\end{figure}


<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <title>The Combinatorial BLAS Release Notes</title>
  <meta name="Generator" content="Cocoa HTML Writer">
  <meta name="CocoaVersion" content="1187.34">
  <style type="text/css">
    p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Arial}
    p.p3 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Arial; min-height: 14.0px}
    p.p4 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Arial; color: #232323; min-height: 14.0px}
    p.p5 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Arial; color: #232323}
    p.p6 {margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; color: #232323; min-height: 15.0px}
    p.p7 {margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Arial; color: #232323}
    p.p8 {margin: 0.0px 0.0px 0.0px 0.0px; font: 14.0px Times; min-height: 18.0px}
    span.s1 {text-decoration: underline ; color: #042eee}
    span.s2 {font: 12.0px Courier}
    span.s3 {color: #000000}
    span.s4 {color: #232323}
    span.s5 {color: #3d578c}
    span.s6 {text-decoration: underline ; color: #0000ee}
    span.s7 {font: 13.0px Arial}
    span.s8 {color: #000000; background-color: #fbfcfd}
    span.s9 {font: 12.0px Courier; color: #000000; background-color: #fbfcfd}
    span.s10 {font: 12.0px Courier; color: #4665a2}
    span.s11 {background-color: #fffecc}
  </style>
</head>
<body>
<h1 style="margin: 0.0px 0.0px 16.0px 0.0px; font: 24.0px Times"><b>Frequently Asked Questions about Combinatorial BLAS</b></h1>
<p class="p2">Go <a href="http://gauss.cs.ucsb.edu/~aydin/CombBLAS/html/index.html"><span class="s1">back</span></a> to the the Combinatorial BLAS home page.</p>
<p class="p3"><br></p>
<p class="p4"><br></p>
<p class="p5"><b>Q1: </b>I would like to use your Combinatorial BLAS code for some of my experiments which involve sparse matrix multiplication. However, it is not clear how to write an output of <span class="s2">PSpGEMM(…)</span> function to a file. I've tried to use "put" function of SpParMat, but it outputs part of the matrix that corresponds to particular process and not the whole matrix. Is there a way to do it using your code?</p>
<p class="p3"><br></p>
<p class="p5"><span class="s3"><b>A1:</b> Yes, </span><span class="s2">SpParMat::SaveGathered(…)</span> will create a single file output (albeit slow) sorted with increasing row id's when called like<span class="s2"> A.SaveGathered("product.txt")</span>. The caveat is that<span class="Apple-converted-space">  </span>"gathered" I/O in human readable form is quite slow due to serialization on large processor counts. It should only be used for debugging, ideally.<span class="Apple-converted-space"> </span></p>
<p class="p6"><br></p>
<p class="p7">----</p>
<p class="p6"><br></p>
<p class="p5"><b>Q2</b>: Does Combinatorial BLAS support in-node multithreading?<span class="Apple-converted-space"> </span></p>
<p class="p4"><br></p>
<p class="p2"><span class="s4"><b>A2:</b> </span>Some primitives (Sparse SpMV, EWiseMult, Apply, Set) are hybrid multithreaded within a socket. Read this <a href="http://gauss.cs.ucsb.edu/~aydin/CombBLAS_FILES/CombBLASv1.2_threading.txt"><span class="s5">short tutorial</span><span class="s6">.</span></a> We are working to extend multithreading support to all primitives.<span class="Apple-converted-space"> </span></p>
<p class="p6"><br></p>
<p class="p7">---</p>
<p class="p6"><br></p>
<p class="p5"><b>Q3:</b> Reading/writing text files is really slow for my purposes, what can I do?</p>
<p class="p4"><br></p>
<p class="p2"><span class="s4"><b>A3:</b> </span>Combinatorial BLAS supports a binary format for fast parallel I/O. Read the manual <a href="http://gauss.cs.ucsb.edu/~aydin/CombBLAS_FILES/Input_File_Formats.pdf"><span class="s5">here</span></a>.<span class="s4"><span class="Apple-converted-space"> </span></span></p>
<p class="p6"><br></p>
<p class="p7">---</p>
<p class="p6"><br></p>
<p class="p5"><b>Q4:</b> Is there a preferred way to prune elements from a SpParMat according to a predicate?</p>
<p class="p4"><br></p>
<p class="p5"><b>A4:</b> Yes,<span class="s7"> </span><span class="s2">SpParMat::Prune(…)</span> will do it according to a predicate. An overloaded version of the same function, <span class="s2">SpParMat::Prune(ri,ci)</span> will <span class="s8">prune all entries whose row indices are in </span><span class="s9">ri </span><span class="s8">and column indices are in </span><span class="s9">ci</span></p>
<p class="p6"><br></p>
<p class="p7">---</p>
<p class="p6"><br></p>
<p class="p5"><b>Q5:</b> I am trying to run CombBLAS on Windows but the MS MPI does not seem to support MPI C++ bindings.</p>
<p class="p4"><br></p>
<p class="p2"><b>A5: </b>Combinatorial BLAS recently (version 1.3.0) switched to C-API for all its internal MPI calls. After that, we've also compiled CombLAS on a windows machine without problems. However, we recommend using an open source MPI for windows too, such as MPICH-2.</p>
<p class="p3"><br></p>
<p class="p2">---</p>
<p class="p3"><br></p>
<p class="p5"><b>Q6:</b> I would like to use Combinatorial BLAS for some parallel sparse matrix-matrix multiplications. This works quite well, however when I try to assign a m x 1 sparse matrix (actually a vector) to the first column of an existing sparse matrix with SpAsgn I get an error saying: "Matrix is too small to be splitted". Is this because it's not possible to use SpAsgn on vector-like matrices?</p>
<p class="p4"><br></p>
<p class="p5"><b>A6: </b>SpAsgn internally uses a memory efficient <span class="s2">Mult_AnXBn_DoubleBuff</span> as opposed to <span class="s2">Mult_AnXBn_Synch</span>). You might probably go into <a href="http://gauss.cs.ucsb.edu/~aydin/CombBLAS/html/class_sp_par_mat.html#ae510b0084843dc7d7c7d5c6572f5ef12"><span class="s10">SpParMat&lt;IT,NT,DER&gt;::SpAsgn</span></a><span class="s2">(...)</span> and change occuranges of <span class="s2">Mult_AnXBn_DoubleBuff</span> to <span class="s2">Mult_AnXBn_Synch</span>. However, this will likely only solve your problem for the serial case. because ComBBLAS can not effectively 2D decompose an m x 1 matrix: each dimension should ideally be at least sqrt(p). It is much better to represent that vector as a <span class="s2">FullyDistSpVec</span>.</p>
<p class="p6"><br></p>
<p class="p7">---</p>
<p class="p6"><br></p>
<p class="p5"><b>Q7: </b>Starting from a general sparse matrix Z, I want to construct the symmetric matrix M: [[X'X; X'Z];[Z'X; Z'Z]], where X is a vector of 1's. Thus the element at position (1,1) is simply the number of columns of Z, and the vector Z'X contains the sums per column of Z. For now, I have a working code, but it is quite sloppy because I do not find a function for which I can easily increase the dimension of a sparse matrix or even set an element to a specific value. Is there any function in Combinatorial BLAS which can do this?<span class="Apple-converted-space"> </span></p>
<p class="p4"><br></p>
<p class="p5"><b>A7:</b> Not out of the box. You don't want an SpAsgn or any variant of it because it can't grow the matrix. You want some sort of matrix append. How about using Find(…) and Sparse(…)?  The Matlab care of what you want to do is:</p>
<p class="p4"><br></p>
<p class="p5">X = ones(size(Z,2),1) </p>
<p class="p5">M = [X' * X, X' * Z; Z'* X, Z' * Z]</p>
<p class="p4"><br></p>
<p class="p5">Supporting such a general concatenation efficiently might be hard to add at this point. Instead,<span class="Apple-converted-space">  </span>there is a Concatenate(…) function for vectors. Armed with Concatenate(…), find(), and the sparse-like constructor, one can solve your problem.  Check out the working example in ReleaseTests/FindSparse.cpp</p>
<p class="p6"><br></p>
<p class="p7">---</p>
<p class="p6"><br></p>
<p class="p5"><b>Q8:</b> Does CombBLAS include the API to perform a symmetric permutation on a matrix, as explained in your <a href="http://gauss.cs.ucsb.edu/~aydin/spgemm_sisc12.pdf"><span class="s6">SISC paper</span></a>? </p>
<p class="p4"><br></p>
<p class="p5"><b>A8:</b> Yes it does. Check out the ReleaseTests/IndexingTiming.cpp for an example.</p>
<p class="p6"><br></p>
<p class="p7">---<span class="Apple-converted-space"> </span></p>
<p class="p6"><br></p>
<p class="p5"><b>Q9:</b> How can I use small test case to see whether the operation on matrix is correct? In other words, how do I print all the information of a matrix with each value in matrix? </p>
<p class="p5">I can use PrintInfo to print basic information, but it only gives me number of rows and columns and nnz</p>
<p class="p4"><br></p>
<p class="p2"><span class="s4"><b>A9:</b> Our recommendation is </span>to use <span class="s2">SaveGathered(…) </span>to dump the whole matrix into a file in triples (matrix market) format. There is also an equivalent vector version of it:<span class="Apple-converted-space">  </span><span class="s2">FullyDistVec::SaveGathered(…)</span></p>
<p class="p6"><br></p>
<p class="p7">---</p>
<p class="p6"><br></p>
<p class="p5"><span class="s3"><b>Q10:</b> </span>Does CombBLAS code run on any graph size or there is some limitation on the dimension of the matrix A. I mean should it be a multiple of sqrt(p) where p is total number of processors. </p>
<p class="p4"><br></p>
<p class="p5"><b>A10:</b> No, the matrix dimension does not have to be a multiple of sqrt(p) but it should be bigger than sqrt(p). In other words you can have a 5x5 matrix on 4 processors but not on 36 processors. We don't really see the point of using more than |V|^2 processors.</p>
<p class="p4"><br></p>
<p class="p5">---</p>
<p class="p4"><br></p>
<p class="p5"><b>Q11:</b> My comparison results on real graph inputs revealed something weird. In input loc-gowalla, how can 16 processors time(called time_16) and </p>
<p class="p5">64 processors time(called time_64) which time_64*4&lt;time_16  which is more than linear scale? </p>
<p class="p4"><br></p>
<p class="p5"><b>A11:</b> The complexity of the parallel algorithm drops as sub-matrices owned by each processor gets sparser. In particular, it is proportional to O(flops x log(ni)) where ni is the size of the intersection of the set of nonzero columns of Aik and nonzero rows of Bkj for A*B. What might happen as p increases is that there is a phase transition that makes ni drop significantly for your input (for p=64, each sub-matrix will have only ~1.2 nonzeros per row or column). More details are in the <a href="http://gauss.cs.ucsb.edu/~aydin/spgemm_sisc12.pdf"><span class="s6">SISC paper</span></a> and the references therein. Hope this makes sense. This is why I don't suggest people use CombBLAS for small p (&lt; 40) because it is not on the top of its game for small number of processors. </p>
<p class="p4"><br></p>
<p class="p5">---<span class="Apple-converted-space"> </span></p>
<p class="p4"><br></p>
<p class="p5"><b>Q12:</b> Should the input file have nodes numbered from 1 or it is fine if the nodes are numbered from 0?</p>
<p class="p4"><br></p>
<p class="p5"><b>A12:</b> If you're using the human readable matrix market format as your input, then it should be 1-indexed.<span class="Apple-converted-space"> </span></p>
<p class="p6"><br></p>
<p class="p7">---</p>
<p class="p6"><br></p>
<p class="p5"><b>Q13: </b>I'm wondering for breadth-first-search, under the hood does the matrix-vector multiplication method change based on the sparsity of the frontier vector, or does the underlying matrix-vector multiplication assume the frontier is always sparse?</p>
<p class="p4"><br></p>
<p class="p5"><b>A13:</b> Depending on your definition of sparseness, the frontier is almost always sparse. We use the pragmatic definition of "sparse" in the sense that a vector is sparse if it is worth taking advantage of the sparsity in there. I'd guess, for a dense vector assumption to be competitive, it would have to have at least 1/3 of its potential locations nonzero. However, I might be wrong (and you're welcome to prove me wrong). To answer your question more directly, CombBLAS supports both dense and sparse right hand side vectors, but the specific BFS implementation does not adapt. </p>
<p class="p6"><br></p>
<p class="p7">---</p>
<p class="p4"><br></p>
<p class="p5"><b>Q14: </b>Could you briefly explain the difference in your implementations of matrix-sparse vector and matrix-dense vector multiply? For example, is the sparse vector case a write-based approach: Every element updates all of its neighbors (from a graph-theoretic standpoint) locations in the output vector; and the dense vector case a read-based approach: Every element reads some value from each of its neighbors and updates its own entry in the resulting vector?</p>
<p class="p4"><br></p>
<p class="p5"><b>A14:</b> Sparse matrix-sparse vector is "right hand side vector structure" driven. In y = A*x, for each nonzero x_i, we scale the column A(:,i) with that and merge the scaled sparse columns results into y. The computation boils down into merging sparse columns into one. <span class="s11">Combinatorial</span> <span class="s11">BLAS</span> is a matrix-vector based library, so thinking in terms of updates on single entries is probably not the right abstraction.</p>
<p class="p4"><br></p>
<p class="p5">Sparse matrix-dense vector is slightly different in the sense that it is driven by the matrix structure; you basically stream the matrix. The correctness of both operations are handled by a SPA-like or heap-like data structure that merges multiple intermediate values contributing to the same output location; no atomics are used.</p>
<p class="p6"><br></p>
<p class="p7">---<span class="Apple-converted-space"> </span></p>
<p class="p3"><br></p>
<p class="p5"><span class="s3"><b>Q15: </b>I </span>would like to get your opinion on how sparse-matrix based implementations compare with more native implementations</p>
<p class="p4"><br></p>
<p class="p5"><span class="s3"><b>A15: </b></span>Sparse matrix abstraction, like any abstraction, will leave some performance on the table. In particular it is prone to performing extra passes over data or creating extra temporaries (if you've ever programmed in Matlab; this is similar). On the other hand, sparse matrix abstraction gives you "primitives" to implement graph "algorithms" as opposed to the algorithms themselves. For instance, CombBLAS has sparse matrix x sparse vector over a semiring as opposed to BFS, because now using the same primitive one can implement MIS (maximal independent set) too, only by changing the semiring. Or one can perform run time filtering on edges based on the attributes, similarly by changing the semiring functions (therefore extending functionality to semantic graphs). Indeed this is what we've done in our upcoming IPDPS'13 paper.</p>
<p class="p4"><br></p>
<p class="p2">---</p>
<p class="p3"><br></p>
<p class="p2"><b>Q16: </b>Is there an effort to incorporate the bottom-up BFS of Scott Beamer into CombBLAS?</p>
<p class="p3"><br></p>
<p class="p2"><b>A16: </b>Yes, it is already done but the code is not yet public. Check back in the next release. In the meantime, use this website for info: <a href="http://www.eecs.berkeley.edu/~sbeamer/dobfs.html"><span class="s6">http://www.eecs.berkeley.edu/~sbeamer/dobfs.html</span></a></p>
<p class="p4"><br></p>
<p class="p7">---</p>
<p class="p6"><br></p>
<p class="p5"><b>Q17: </b>My serial code is faster than CombBLAS on a single core.</p>
<p class="p4"><br></p>
<p class="p5"><b>A17: </b>I believe that. CombBLAS targets "scalability", not optimizing the single core performance.</p>
<p class="p5"> </p>
<p class="p5">Examples:</p>
<p class="p5">- think about the 2D BFS. CombBLAS does not use a CSR like data structure because that is not memory scalable due to problems of <a href="http://gauss.cs.ucsb.edu/publication/hypersparse-ipdps08.pdf"><span class="s6">hypersparsity</span></a> in large concurrencies. Instead CombBLAS opts to use a slower (about 2x around 1000 cores) but memory scalable format called DCSC.  </p>
<p class="p5">- think about betweenness centrality which uses sparse matrix-matrix multiply. CombBLAS doesn't use the fastest serial algorithm as its subroutine because it doesn't  scale to thousands of cores. Instead it uses a outer-product algorithm that is significantly slower for p=1, but scales indefinitely.</p>
<p class="p6"><br></p>
<p class="p7">---<span class="Apple-converted-space"> </span></p>
<p class="p6"><br></p>
<p class="p2"><b>Q18:</b> Looking at the output of your Graph500 application, I noticed a large number of self-edges removed. That’s very interesting.</p>
<p class="p3"><br></p>
<p class="p2"><b>A18: </b>The duplicate edges problem is inherent to the R-MAT generator on large scale, unless some special kind of noise is added. Check here for a great analysis of this phenomenon: <a href="http://arxiv.org/abs/1102.5046"><span class="s6">http://arxiv.org/abs/1102.5046</span></a></p>
<p class="p3"><br></p>
<p class="p2">---</p>
<p class="p3"><br></p>
<p class="p2"><b>Q19:</b> How are you counting the number of edges traversed in Graph500? Is this still using the original verify.c file provided with the reference version of the Graph500 benchmark and passing in the parent tree?</p>
<p class="p3"><br></p>
<p class="p5"><span class="s3"><b>A19: </b></span>It is calculated by summing the degrees of the discovered vertices using <span class="s2">EWiseMult(…)</span> followed by a <span class="s2">Reduce(…)</span>. Degrees are pre-symmetrization (original edges), so we're not over-counting. However, we count self-loops and duplicates as mentioned in the benchmark specs.</p>
<p class="p8"><br></p>
<p class="p2">Go <a href="http://gauss.cs.ucsb.edu/~aydin/CombBLAS/html/index.html"><span class="s6">back</span></a> to the the Combinatorial BLAS home page.</p>
</body>
</html>
